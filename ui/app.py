"""Gradio Webç•Œé¢ - Tab + Sidebaræ¶æ„ v0.4.0"""
import gradio as gr
import sys
import time
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
src_dir = project_root / "src"
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from main import ResearchAssistant
from tools.reading_guide import ReadingGuide
from tools.pdf import PaperProcessor


def create_app():
    """åˆ›å»ºGradioåº”ç”¨"""
    assistant = ResearchAssistant()
    pdf_processor = PaperProcessor()

    def show_paper_details(paper_json: str) -> str:
        """æ˜¾ç¤ºè®ºæ–‡è¯¦æƒ…åˆ°ä¾§è¾¹æ """
        if not paper_json or paper_json == "{}":
            return "## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\nç‚¹å‡»è®ºæ–‡æ ‡é¢˜æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯"

        import json
        try:
            paper = json.loads(paper_json)
        except:
            return "## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\næ•°æ®è§£æå¤±è´¥"

        output = "## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\n"

        # æ ‡é¢˜
        title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
        output += f"### {title}\n\n"

        # ä¸­æ–‡æ ‡é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
        title_cn = paper.get('title_cn', '')
        if title_cn:
            output += f"*{title_cn}*\n\n"

        # ä½œè€…
        authors = paper.get('authors', [])
        if authors:
            output += f"**ğŸ‘¥ ä½œè€…**: {', '.join(authors[:5])}\n\n"

        # å¹´ä»½ã€å¼•ç”¨æ•°
        year = paper.get('year', 'N/A')
        output += f"**ğŸ“… å¹´ä»½**: {year}\n\n"

        citation_count = paper.get('citation_count')
        if citation_count:
            output += f"**ğŸ“Š å¼•ç”¨æ•°**: {citation_count}\n\n"

        # æ¥æº
        source = paper.get('source', '').upper()
        if source:
            output += f"**ğŸ”– æ¥æº**: {source}\n\n"

        # æ‘˜è¦
        abstract = paper.get('abstract', '')
        summary = paper.get('summary', '')  # LLMç”Ÿæˆçš„ä¸­æ–‡æ‘˜è¦

        if summary:
            output += f"**ğŸ“ æ‘˜è¦** (AIç”Ÿæˆ):\n\n{summary}\n\n"

        if abstract:
            output += f"**ğŸ“„ åŸæ–‡æ‘˜è¦**:\n\n{abstract}\n\n"

        # URL
        url = paper.get('url', '')
        if url:
            output += f"**ğŸ”— é“¾æ¥**: [{url}]({url})\n\n"

        # arXiv IDï¼ˆå¦‚æœæ˜¯arXivè®ºæ–‡ï¼Œæ˜¾ç¤ºè·å–å…¨æ–‡æç¤ºï¼‰
        if source == 'ARXIV' and url:
            import re
            match = re.search(r'(\d{4}\.\d{4,5})', url)
            if match:
                arxiv_id = match.group(1)
                output += f"\n---\n\nğŸ’¡ **æç¤º**: è¿™æ˜¯ arXiv è®ºæ–‡ (ID: `{arxiv_id}`)ï¼Œç‚¹å‡»ä¸‹æ–¹ã€Œè·å–å…¨æ–‡ã€æŒ‰é’®å¯ä¸‹è½½ PDF å¹¶æå–å…¨æ–‡ã€‚\n\n"
        else:
            output += f"\n---\n\nâš ï¸ é arXiv è®ºæ–‡ï¼Œæš‚ä¸æ”¯æŒå…¨æ–‡è·å–ã€‚\n\n"

        return output

    def jump_to_citation(cite_num, report_sources_json, papers_json):
        """æ ¹æ®å¼•ç”¨ç¼–å·è·³è½¬åˆ°å¯¹åº”è®ºæ–‡

        Args:
            cite_num: å¼•ç”¨ç¼–å·ï¼ˆ1, 2, 3...ï¼‰
            report_sources_json: æŠ¥å‘Šæ¥æºåˆ—è¡¨ JSONï¼ˆä¸å¼•ç”¨ç¼–å·å¯¹åº”ï¼‰
            papers_json: è®ºæ–‡åˆ—è¡¨ JSON
        """
        import json

        if cite_num is None or cite_num < 1:
            return (
                "## âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„å¼•ç”¨ç¼–å·\n\nå¼•ç”¨ç¼–å·ä» 1 å¼€å§‹",
                gr.update(value=None),
                "{}"
            )

        try:
            # å°è¯•ä» report_sources è·å–ï¼ˆä¸æŠ¥å‘Šå¼•ç”¨ç¼–å·ä¸€è‡´ï¼‰
            report_sources = json.loads(report_sources_json) if report_sources_json else []
            papers = json.loads(papers_json) if papers_json else []

            # å¼•ç”¨ç¼–å·æ˜¯ 1-indexedï¼Œè½¬æ¢ä¸º 0-indexed
            idx = int(cite_num) - 1

            # ä¼˜å…ˆä½¿ç”¨ report_sourcesï¼ˆä¸æŠ¥å‘Šå¼•ç”¨ä¸€è‡´ï¼‰
            source_list = report_sources if report_sources else papers

            if idx < 0 or idx >= len(source_list):
                return (
                    f"## âš ï¸ å¼•ç”¨ [{int(cite_num)}] ä¸å­˜åœ¨\n\nå½“å‰å…±æœ‰ {len(source_list)} ä¸ªæ¥æº",
                    gr.update(value=None),
                    "{}"
                )

            paper = source_list[idx]
            paper_json = json.dumps(paper, ensure_ascii=False)

            # è¿”å›è®ºæ–‡è¯¦æƒ…å’Œæ›´æ–°å½“å‰è®ºæ–‡çŠ¶æ€
            return (
                show_paper_details(paper_json),
                gr.update(value=idx),  # æ›´æ–°ä¸‹æ‹‰èœå•é€‰ä¸­é¡¹
                paper_json
            )

        except Exception as e:
            return (
                f"## âŒ è·³è½¬å¤±è´¥\n\né”™è¯¯: {str(e)}",
                gr.update(value=None),
                "{}"
            )

    def fetch_paper_fulltext(current_paper_json):
        """è·å–è®ºæ–‡å…¨æ–‡

        Args:
            current_paper_json: å½“å‰é€‰ä¸­è®ºæ–‡çš„ JSON
        """
        import json
        import re

        if not current_paper_json or current_paper_json == "{}":
            return "è¯·å…ˆé€‰æ‹©ä¸€ç¯‡è®ºæ–‡"

        try:
            paper = json.loads(current_paper_json)
        except:
            return "è®ºæ–‡æ•°æ®è§£æå¤±è´¥"

        url = paper.get('url', '')
        source = paper.get('source', '').lower()
        title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')

        # åªæ”¯æŒ arXiv è®ºæ–‡
        if source != 'arxiv':
            return f"âš ï¸ æš‚ä¸æ”¯æŒé arXiv è®ºæ–‡çš„å…¨æ–‡è·å–\n\nè®ºæ–‡æ¥æº: {source.upper()}"

        # æå– arXiv ID
        match = re.search(r'(\d{4}\.\d{4,5})', url)
        if not match:
            return f"âš ï¸ æ— æ³•ä» URL æå– arXiv ID\n\nURL: {url}"

        arxiv_id = match.group(1)

        try:
            # ä½¿ç”¨ PaperProcessor ä¸‹è½½å¹¶è§£æ PDF
            result = pdf_processor.process(arxiv_id)

            if not result.success:
                return f"âŒ è·å–å…¨æ–‡å¤±è´¥\n\n{result.error}"

            # è¿”å›å…¨æ–‡
            full_text = result.full_text
            if not full_text:
                return "âŒ PDF è§£ææˆåŠŸä½†æœªæå–åˆ°æ–‡æœ¬"

            # æ·»åŠ å…ƒä¿¡æ¯å¤´
            header = f"ğŸ“„ **{title}**\n"
            header += f"arXiv ID: {arxiv_id} | é¡µæ•°: {result.total_pages}\n"
            header += f"å­—ç¬¦æ•°: {len(full_text):,}\n"
            header += "=" * 50 + "\n\n"

            return header + full_text

        except Exception as e:
            return f"âŒ è·å–å…¨æ–‡æ—¶å‡ºé”™\n\né”™è¯¯: {str(e)}"

    def format_paper(paper: dict, index: int, show_source: bool = False) -> str:
        """æ ¼å¼åŒ–å•ç¯‡è®ºæ–‡"""
        title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
        title_cn = paper.get('title_cn', '')
        source_tag = f" [{paper.get('source', '')}]" if show_source else ""

        if title_cn:
            output = f"**[{index}]{source_tag} {title}**\n\n"
            output += f"ğŸ“– *{title_cn}*\n\n"
        else:
            output = f"**[{index}]{source_tag} {title}**\n\n"

        authors = paper.get('authors', [])
        if authors:
            output += f"- ä½œè€…: {', '.join(authors[:3])}\n"
        output += f"- å¹´ä»½: {paper.get('year', 'N/A')}\n"
        if paper.get("citation_count"):
            output += f"- å¼•ç”¨: {paper['citation_count']}\n"

        summary = paper.get('summary', '')
        if summary:
            output += f"- ğŸ“ **æ‘˜è¦**: {summary}\n"
        elif paper.get("abstract"):
            abstract = paper['abstract']
            if len(abstract) > 200:
                abstract = abstract[:200] + "..."
            output += f"- æ‘˜è¦: {abstract}\n"

        if paper.get('url'):
            output += f"- [ğŸ”— æŸ¥çœ‹è®ºæ–‡]({paper['url']})\n\n"
        output += "---\n\n"
        return output

    def format_reading_guide(guide: dict) -> str:
        """æ ¼å¼åŒ–é˜…è¯»å¯¼èˆª"""
        if not guide or guide.get("error"):
            return ""

        lines = ["## ğŸ“š é˜…è¯»å»ºè®®\n"]

        if guide.get("summary"):
            lines.append(f"{guide['summary']}\n")

        if guide.get("entry_point"):
            ep = guide["entry_point"]
            title = ep.get('title', '')[:60]
            lines.append(f"### ğŸŒŸ å…¥é—¨å¿…è¯»")
            lines.append(f"**[{ep.get('index', '')}] {title}...**")
            lines.append(f"> {ep.get('reason', '')}\n")

        if guide.get("core_papers"):
            lines.append(f"### ğŸ“Œ æ ¸å¿ƒè®ºæ–‡")
            for cp in guide["core_papers"]:
                title = cp.get('title', '')[:60]
                lines.append(f"- **[{cp.get('index', '')}] {title}...** â†’ {cp.get('reason', '')}")
            lines.append("")

        if guide.get("latest"):
            lt = guide["latest"]
            title = lt.get('title', '')[:60]
            lines.append(f"### ğŸ†• æœ€æ–°è¿›å±•")
            lines.append(f"**[{lt.get('index', '')}] {title}...**")
            lines.append(f"> {lt.get('reason', '')}\n")

        if guide.get("reading_order"):
            order = guide["reading_order"][:5]
            lines.append(f"### ğŸ“– æ¨èé˜…è¯»é¡ºåº")
            lines.append(f"**{' â†’ '.join(map(str, order))}**\n")

        if guide.get("categories"):
            lines.append(f"### ğŸ“‚ è®ºæ–‡åˆ†ç±»\n")
            for cat in guide["categories"]:
                name = cat.get("name", "å…¶ä»–")
                desc = cat.get("description", "")
                papers = cat.get("papers", [])
                paper_nums = [str(p.get("index", "")) for p in papers]
                cat_text = f"#### {name} ({len(papers)}ç¯‡)\n"
                cat_text += f"- è®ºæ–‡ç¼–å·: `{', '.join(paper_nums)}`\n"
                if desc:
                    cat_text += f"- è¯´æ˜: {desc}\n"
                lines.append(cat_text)

        return "\n".join(lines)

    def search_papers_stream(query: str, mode: str = "auto", use_fulltext: bool = False, use_v2: bool = False):
        """æµå¼æœç´¢è®ºæ–‡ - å®æ—¶æ˜¾ç¤ºè¿›åº¦

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            mode: æœç´¢æ¨¡å¼ ("auto", "simple", "deep_research")
            use_fulltext: æ˜¯å¦ä½¿ç”¨å…¨æ–‡ç ”ç©¶ï¼ˆä»…æ·±åº¦ç ”ç©¶æ¨¡å¼æœ‰æ•ˆï¼‰
            use_v2: æ˜¯å¦ä½¿ç”¨ V2 æ¶æ„ï¼ˆSupervisor å¾ªç¯ï¼‰
        """
        if not query.strip():
            yield "è¯·è¾“å…¥ç ”ç©¶é—®é¢˜", "", "", "", "", "", "{}"
            return

        start_time = datetime.now()
        actual_mode = "auto" if mode == "æ™ºèƒ½åˆ¤æ–­" else ("simple" if mode == "å¿«é€Ÿæœç´¢" else "deep_research")

        # é˜¶æ®µ1: æ˜¾ç¤ºå¼€å§‹çŠ¶æ€
        start_time_str = start_time.strftime("%H:%M:%S")
        header = f"## ğŸ” æŸ¥è¯¢åˆ†æ\n\n"
        header += f"**æŸ¥è¯¢**: {query}\n\n"

        # æ˜¾ç¤ºæ¨¡å¼ï¼ˆåŒ…æ‹¬å…¨æ–‡ç ”ç©¶çŠ¶æ€å’ŒV2æ¶æ„ï¼‰
        if actual_mode == 'deep_research' or (actual_mode == 'auto' and len(query) > 20):
            mode_display = "ğŸš€ æ·±åº¦ç ”ç©¶"
            if use_v2:
                mode_display += " (V2 Supervisor å¾ªç¯)"
            if use_fulltext:
                mode_display += " (ğŸ“„ å…¨æ–‡æ¨¡å¼)"
        else:
            mode_display = "âš¡ å¿«é€Ÿæœç´¢"

        header += f"**æ¨¡å¼**: {mode_display}\n\n"
        header += f"**çŠ¶æ€**: â³ æ­£åœ¨åˆ†ææŸ¥è¯¢... (å¼€å§‹äº {start_time_str})\n"

        yield header, f"â³ æ­£åœ¨åˆ†æé—®é¢˜ï¼Œè¯·ç¨å€™...\n\n> å¼€å§‹æ—¶é—´: {start_time_str}ï¼Œå¯ç‚¹å‡»ã€Œåœæ­¢ã€æŒ‰é’®å–æ¶ˆ", "*ç­‰å¾…ç ”ç©¶å®Œæˆ...*", "", "*ğŸ”„ æœç´¢ä¸­...*", "{}", "[]"

        # é˜¶æ®µ2: æ‰§è¡Œæœç´¢ï¼ˆä¼ å…¥ use_fulltext å’Œ use_v2 å‚æ•°ï¼‰
        try:
            result = assistant.process_query(query, mode=actual_mode, use_fulltext=use_fulltext, use_v2=use_v2)
        except Exception as e:
            elapsed = (datetime.now() - start_time).total_seconds()
            error_msg = f"## âŒ æœç´¢å‡ºé”™\n\nè€—æ—¶: {elapsed:.1f}ç§’\n\né”™è¯¯: {str(e)}"
            yield header.replace("â³ æ­£åœ¨åˆ†ææŸ¥è¯¢...", f"âŒ å‡ºé”™ ({elapsed:.1f}s)"), error_msg, "", "", "", "{}", "[]"
            return

        elapsed = (datetime.now() - start_time).total_seconds()

        # æ›´æ–° header
        header = f"## ğŸ” æŸ¥è¯¢åˆ†æ\n\n"
        header += f"**æ„å›¾**: {result.get('intent', 'æœªè¯†åˆ«')}\n\n"
        keywords = result.get('keywords', [])
        if keywords:
            header += f"**æœç´¢å…³é”®è¯**: `{'`, `'.join(keywords)}`\n\n"
        mode_display = "ğŸš€ æ·±åº¦ç ”ç©¶" if result['mode'] in ("deep_research", "deep_research_v2") else "âš¡ å¿«é€Ÿæœç´¢"
        header += f"**æ¨¡å¼**: {mode_display} | "
        header += f"**æœç´¢æº**: {', '.join(result.get('sources', [])) or 'æ— '} | "
        header += f"**æ‰¾åˆ°**: {result.get('total_found', len(result.get('papers', [])))} ç¯‡\n\n"
        header += f"**æ€»è€—æ—¶**: âœ… {elapsed:.1f}ç§’\n"

        # æ·±åº¦ç ”ç©¶æ¨¡å¼ï¼šæ˜¾ç¤ºç ”ç©¶æŠ¥å‘Š
        report_output = ""
        thinking_output = ""  # åˆ†ç¦»çš„æ€è€ƒè¿‡ç¨‹è¾“å‡º

        if result['mode'] in ('deep_research', 'deep_research_v2') and result.get('report'):
            # V2 æ¨¡å¼æ˜¾ç¤ºæ€è€ƒå†å²ï¼ˆåˆ†ç¦»åˆ° thinking_outputï¼‰
            if result['mode'] == 'deep_research_v2':
                thinking_history = result.get('thinking_history', [])
                if thinking_history:
                    thinking_output = "### ç ”ç©¶æ€è€ƒè¿‡ç¨‹\n\n"
                    for record in thinking_history:
                        # æ”¯æŒæ–°æ ¼å¼ï¼ˆå¸¦è½®æ¬¡ï¼‰å’Œæ—§æ ¼å¼ï¼ˆçº¯å­—ç¬¦ä¸²ï¼‰
                        if isinstance(record, dict):
                            round_num = record.get('round', '?')
                            thought = record.get('thought', '')
                        else:
                            round_num = '?'
                            thought = str(record)

                        # é•¿æ€è€ƒå†…å®¹ä½¿ç”¨ HTML details å®ç°å±•å¼€/æ”¶èµ·
                        if len(thought) > 80:
                            thought_preview = thought[:80].replace('\n', ' ')
                            thought_full = thought.replace('\n', '<br>')
                            thinking_output += f"**ç¬¬ {round_num} è½®ï¼š** {thought_preview}... "
                            thinking_output += f"<details><summary>ğŸ“– å±•å¼€å…¨éƒ¨</summary>\n\n{thought_full}\n\n</details>\n\n"
                        else:
                            thinking_output += f"**ç¬¬ {round_num} è½®ï¼š** {thought}\n\n"
            else:
                # V1 æ¨¡å¼æ˜¾ç¤ºå­é—®é¢˜åˆ†è§£ï¼ˆä¹Ÿæ”¾åˆ° thinking_outputï¼‰
                decomposition = result.get('decomposition', {})
                if decomposition:
                    thinking_output = "### é—®é¢˜åˆ†è§£\n\n"
                    thinking_output += f"**é—®é¢˜ç±»å‹**: {decomposition.get('query_type', 'N/A')}\n\n"
                    thinking_output += f"**ç ”ç©¶ç­–ç•¥**: {decomposition.get('strategy', 'N/A')}\n\n"
                    sub_questions = decomposition.get('sub_questions', [])
                    if sub_questions:
                        thinking_output += "**å­é—®é¢˜**:\n"
                        for i, sq in enumerate(sub_questions, 1):
                            thinking_output += f"{i}. {sq.get('question', '')} *(ç›®çš„: {sq.get('purpose', '')})*\n"

            # æ˜¾ç¤ºç ”ç©¶æŠ¥å‘Šï¼ˆä¸åŒ…å«æ€è€ƒè¿‡ç¨‹ï¼‰
            report_output += result['report']

            # æ˜¾ç¤ºå…ƒæ•°æ®å’Œå„é˜¶æ®µè€—æ—¶
            metadata = result.get('metadata', {})
            if metadata:
                report_output += "\n\n---\n"
                report_output += f"**æ€»è€—æ—¶: {metadata.get('duration_seconds', 0):.1f}ç§’** | "

                if result['mode'] == 'deep_research_v2':
                    # V2 å…ƒæ•°æ®
                    total_searched = metadata.get('total_searched', 0)
                    total_selected = metadata.get('total_selected', 0)
                    report_output += f"ç ”ç©¶è½®æ•°: {metadata.get('total_rounds', 0)}è½® | "
                    report_output += f"è®ºæ–‡: æœç´¢ {total_searched} ç¯‡ â†’ ç­›é€‰ {total_selected} ç¯‡\n\n"
                    report_output += f"**å®ŒæˆåŸå› **: {metadata.get('completion_reason', 'N/A')}\n"
                else:
                    # V1 å…ƒæ•°æ®
                    report_output += f"å­é—®é¢˜: {metadata.get('sub_questions_count', 0)}ä¸ª | "
                    report_output += f"è®ºæ–‡: {metadata.get('total_papers', 0)}ç¯‡\n\n"

                    # æ˜¾ç¤ºå„é˜¶æ®µè€—æ—¶è¯¦æƒ…
                    stage_times = metadata.get('stage_times', {})
                    if stage_times:
                        report_output += "**â±ï¸ å„é˜¶æ®µè€—æ—¶:**\n"
                        for stage, time_sec in stage_times.items():
                            stage_name = stage.split('_', 1)[1] if '_' in stage else stage
                            report_output += f"- {stage_name}: {time_sec:.1f}ç§’\n"

        # å¦‚æœæ²¡æœ‰æ€è€ƒè¿‡ç¨‹ï¼Œæ˜¾ç¤ºæç¤º
        if not thinking_output:
            thinking_output = "*æ— æ€è€ƒè®°å½•ï¼ˆä»…æ·±åº¦ç ”ç©¶æ¨¡å¼æœ‰æ•ˆï¼‰*"

        # é˜…è¯»å¯¼èˆªï¼ˆå¿«é€Ÿæœç´¢æ¨¡å¼ï¼‰
        reading_guide = result.get('reading_guide', {})
        guide_output = format_reading_guide(reading_guide) if result['mode'] == 'simple' else ""

        # è®ºæ–‡åˆ—è¡¨ï¼ˆåˆå¹¶ä¸ºå•ä¸€åˆ—è¡¨ï¼Œå¸¦æ¥æºæ ‡ç­¾ï¼‰
        import json
        arxiv_papers = result.get('arxiv_papers', [])
        openalex_papers = result.get('openalex_papers', [])
        papers_list = arxiv_papers + openalex_papers

        # ç»Ÿä¸€ç”Ÿæˆè®ºæ–‡åˆ—è¡¨ï¼ˆå¸¦æ¥æºæ ‡ç­¾ï¼‰
        arxiv_count = len(arxiv_papers)
        openalex_count = len(openalex_papers)
        total_count = arxiv_count + openalex_count

        papers_output = f"### ğŸ“š ç›¸å…³è®ºæ–‡ ({total_count}ç¯‡)\n\n"
        papers_output += f"> arXiv: {arxiv_count}ç¯‡ | OpenAlex: {openalex_count}ç¯‡\n\n"

        if result['mode'] in ('deep_research', 'deep_research_v2'):
            papers_output += "> â„¹ï¸ *ä»¥ä¸‹ç¼–å·ä¸æŠ¥å‘Šå¼•ç”¨ç¼–å·æ— å…³*\n\n"

        for i, paper in enumerate(papers_list, 1):
            papers_output += format_paper(paper, i, show_source=True)

        if not papers_list:
            papers_output += "*æš‚æ— ç»“æœ*\n"

        # è¿”å›è®ºæ–‡åˆ—è¡¨JSONä¾›ä¾§è¾¹æ ä½¿ç”¨
        papers_json = json.dumps(papers_list, ensure_ascii=False)

        # è¿”å›æŠ¥å‘Šæ¥æºJSONï¼ˆä¸æŠ¥å‘Šå¼•ç”¨ç¼–å·ä¸€è‡´ï¼‰
        report_sources = result.get('report_sources', [])
        report_sources_json = json.dumps(report_sources, ensure_ascii=False) if report_sources else "[]"

        yield header, report_output, thinking_output, guide_output, papers_output, papers_json, report_sources_json

    # === è®ºæ–‡åº“åŠŸèƒ½å‡½æ•° ===

    # å…¨æ–‡åˆ†é¡µå‚æ•°
    CHARS_PER_PAGE = 3000

    def analyze_pdf(pdf_file):
        """åˆ†æä¸Šä¼ çš„ PDF æ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        empty_result = (
            "è¯·å…ˆä¸Šä¼  PDF æ–‡ä»¶", "",  # header, abstract
            "", "ç¬¬ 1 / 1 é¡µ", gr.update(maximum=1, value=1),  # fulltext, page_info, page_slider
            "", gr.update(maximum=1, value=1), "", "",  # chunks_info, chunk_selector, chunk_content, translation
            {}  # pdf_state
        )

        if pdf_file is None:
            return empty_result

        try:
            pdf_path = pdf_file.name if hasattr(pdf_file, 'name') else str(pdf_file)
            result = pdf_processor.process_local_pdf(pdf_path)

            if not result.success:
                return (
                    f"## âŒ åˆ†æå¤±è´¥\n\n{result.error}", "", "", "ç¬¬ 1 / 1 é¡µ",
                    gr.update(maximum=1, value=1), "", gr.update(maximum=1, value=1), "", "", {}
                )

            # åŸºæœ¬ä¿¡æ¯
            header = f"## âœ… åˆ†æå®Œæˆ\n\n"
            header += f"**ğŸ“„ æ ‡é¢˜**: {result.title}\n\n"
            header += f"**ğŸ“– é¡µæ•°**: {result.total_pages} é¡µ\n\n"
            header += f"**ğŸ“ å…¨æ–‡é•¿åº¦**: {len(result.full_text):,} å­—ç¬¦\n\n"
            header += f"**ğŸ§© åˆ‡ç‰‡æ•°**: {len(result.chunks)} ä¸ª\n"

            # æ‘˜è¦
            abstract = result.abstract if result.abstract else "*æœªèƒ½è‡ªåŠ¨æå–æ‘˜è¦ï¼ˆå¯å°è¯• AI æ€»ç»“ï¼‰*"

            # å…¨æ–‡åˆ†é¡µ
            full_text = result.full_text or ""
            total_pages = max(1, (len(full_text) + CHARS_PER_PAGE - 1) // CHARS_PER_PAGE)
            first_page_text = full_text[:CHARS_PER_PAGE] if full_text else "æ— æ³•æå–å…¨æ–‡"
            page_info = f"ç¬¬ 1 / {total_pages} é¡µï¼ˆæ¯é¡µ {CHARS_PER_PAGE} å­—ï¼‰"

            # åˆ‡ç‰‡ç»Ÿè®¡
            chunks_info = ""
            if result.chunks:
                total_tokens = sum(c.token_count for c in result.chunks)
                avg_tokens = total_tokens / len(result.chunks)
                chunks_info = f"**åˆ‡ç‰‡æ•°é‡**: {len(result.chunks)} | **æ€» Token**: {total_tokens:,} | **å¹³å‡**: {avg_tokens:.0f} tokens/ç‰‡"
            else:
                chunks_info = "*æ— åˆ‡ç‰‡ä¿¡æ¯*"

            # ç¬¬ä¸€ä¸ªåˆ‡ç‰‡å†…å®¹
            first_chunk = result.chunks[0].text if result.chunks else ""

            # å­˜å‚¨çŠ¶æ€ï¼ˆç”¨äºåç»­æ“ä½œï¼‰
            pdf_state = {
                "title": result.title,
                "abstract": result.abstract,
                "full_text": full_text,
                "total_pages": total_pages,
                "chunks": [{"text": c.text, "pages": c.pages, "tokens": c.token_count} for c in result.chunks]
            }

            return (
                header, abstract,
                first_page_text, page_info, gr.update(maximum=total_pages, value=1),
                chunks_info, gr.update(maximum=max(1, len(result.chunks)), value=1),
                first_chunk, "",
                pdf_state
            )

        except Exception as e:
            return (
                f"## âŒ å¤„ç†å‡ºé”™\n\n{str(e)}", "", "", "ç¬¬ 1 / 1 é¡µ",
                gr.update(maximum=1, value=1), "", gr.update(maximum=1, value=1), "", "", {}
            )

    def update_fulltext_page(page_num: int, pdf_state: dict):
        """æ›´æ–°å…¨æ–‡æ˜¾ç¤ºé¡µç """
        if not pdf_state or "full_text" not in pdf_state:
            return "", "ç¬¬ 1 / 1 é¡µ"

        full_text = pdf_state["full_text"]
        total_pages = pdf_state.get("total_pages", 1)
        page_num = max(1, min(int(page_num), total_pages))

        start = (page_num - 1) * CHARS_PER_PAGE
        end = start + CHARS_PER_PAGE
        page_text = full_text[start:end]

        page_info = f"ç¬¬ {page_num} / {total_pages} é¡µï¼ˆæ¯é¡µ {CHARS_PER_PAGE} å­—ï¼‰"
        return page_text, page_info

    def update_chunk_content(chunk_idx: int, pdf_state: dict):
        """æ›´æ–°åˆ‡ç‰‡å†…å®¹æ˜¾ç¤º"""
        if not pdf_state or "chunks" not in pdf_state:
            return "", ""

        chunks = pdf_state["chunks"]
        idx = max(0, min(int(chunk_idx) - 1, len(chunks) - 1))

        if idx < len(chunks):
            chunk = chunks[idx]
            return chunk["text"], ""  # æ¸…ç©ºç¿»è¯‘ç»“æœ
        return "", ""

    def translate_chunk(chunk_idx: int, pdf_state: dict):
        """ç¿»è¯‘é€‰ä¸­çš„åˆ‡ç‰‡"""
        if not pdf_state or "chunks" not in pdf_state:
            return "*è¯·å…ˆä¸Šä¼ å¹¶åˆ†æ PDF*"

        chunks = pdf_state["chunks"]
        idx = max(0, min(int(chunk_idx) - 1, len(chunks) - 1))

        if idx >= len(chunks):
            return "*åˆ‡ç‰‡ä¸å­˜åœ¨*"

        chunk_text = chunks[idx]["text"]

        # è°ƒç”¨ LLM ç¿»è¯‘
        try:
            from utils.llm_client import QwenClient
            from utils.config import config

            translator_config = config.get_translator_config()
            api_key = translator_config.get("qwen_api_key")

            if not api_key:
                return "*æœªé…ç½® QWEN_API_KEYï¼Œæ— æ³•ç¿»è¯‘*"

            client = QwenClient(api_key=api_key)
            prompt = f"è¯·å°†ä»¥ä¸‹å­¦æœ¯è®ºæ–‡æ®µè½ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒä¸“ä¸šæœ¯è¯­å‡†ç¡®ï¼š\n\n{chunk_text}"

            translation = client.chat(prompt, task_type="translation", max_tokens=2000)
            return f"### ğŸŒ ç¿»è¯‘ç»“æœ\n\n{translation}"

        except Exception as e:
            return f"*ç¿»è¯‘å¤±è´¥: {str(e)}*"

    def summarize_pdf(pdf_state: dict):
        """AI æ€»ç»“å…¨æ–‡"""
        if not pdf_state or "full_text" not in pdf_state:
            return "*è¯·å…ˆä¸Šä¼ å¹¶åˆ†æ PDF*"

        title = pdf_state.get("title", "æœªçŸ¥æ ‡é¢˜")
        abstract = pdf_state.get("abstract", "")
        full_text = pdf_state.get("full_text", "")

        # å–å‰ 8000 å­—ç”¨äºæ€»ç»“ï¼ˆé¿å…è¶…å‡º token é™åˆ¶ï¼‰
        text_for_summary = full_text[:8000] if len(full_text) > 8000 else full_text

        try:
            from utils.llm_client import QwenClient
            from utils.config import config

            translator_config = config.get_translator_config()
            api_key = translator_config.get("qwen_api_key")

            if not api_key:
                return "*æœªé…ç½® QWEN_API_KEYï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“*"

            client = QwenClient(api_key=api_key)
            prompt = f"""è¯·å¯¹ä»¥ä¸‹å­¦æœ¯è®ºæ–‡è¿›è¡Œç»“æ„åŒ–æ€»ç»“ï¼š

**è®ºæ–‡æ ‡é¢˜**: {title}

**æ‘˜è¦**: {abstract if abstract else 'ï¼ˆæ— æ‘˜è¦ï¼‰'}

**æ­£æ–‡å†…å®¹**:
{text_for_summary}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºæ€»ç»“ï¼š

## ç ”ç©¶èƒŒæ™¯ä¸é—®é¢˜
ï¼ˆä¸€å¥è¯æ¦‚æ‹¬ç ”ç©¶èƒŒæ™¯å’Œè¦è§£å†³çš„é—®é¢˜ï¼‰

## ä¸»è¦æ–¹æ³•
ï¼ˆæ¦‚æ‹¬è®ºæ–‡é‡‡ç”¨çš„æ ¸å¿ƒæ–¹æ³•æˆ–æŠ€æœ¯ï¼‰

## å…³é”®å‘ç°
- å‘ç°1
- å‘ç°2
- å‘ç°3

## ä¸»è¦è´¡çŒ®
ï¼ˆæ¦‚æ‹¬è®ºæ–‡çš„åˆ›æ–°ç‚¹å’Œè´¡çŒ®ï¼‰

## å±€é™ä¸å±•æœ›
ï¼ˆå¦‚æœ‰ï¼‰
"""

            summary = client.chat(prompt, task_type="summary", max_tokens=1500)
            return summary

        except Exception as e:
            return f"*æ€»ç»“å¤±è´¥: {str(e)}*"

    def export_markdown(report_content: str, papers_json: str) -> str:
        """å¯¼å‡º Markdown æŠ¥å‘Š"""
        import tempfile
        import json
        from datetime import datetime

        if not report_content or report_content.startswith("â³"):
            return None

        # ç”Ÿæˆå®Œæ•´çš„ Markdown æ–‡ä»¶
        # æ£€æŸ¥æŠ¥å‘Šæ˜¯å¦å·²æœ‰æ ‡é¢˜ï¼ˆä»¥ # å¼€å¤´ï¼‰
        has_title = report_content.strip().startswith('#')

        if has_title:
            # æŠ¥å‘Šå·²æœ‰æ ‡é¢˜ï¼Œåªæ·»åŠ å…ƒæ•°æ®
            md_content = f"*å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n\n"
            md_content += "---\n\n"
            md_content += report_content
        else:
            # æŠ¥å‘Šæ— æ ‡é¢˜ï¼Œæ·»åŠ é»˜è®¤æ ‡é¢˜
            md_content = f"# ç ”ç©¶æŠ¥å‘Š\n\n"
            md_content += f"*å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n\n"
            md_content += "---\n\n"
            md_content += report_content

        # æ£€æŸ¥æŠ¥å‘Šæ˜¯å¦å·²åŒ…å«å‚è€ƒæ¥æºï¼ˆæ·±åº¦ç ”ç©¶æ¨¡å¼ä¼šè‡ªåŠ¨æ·»åŠ ï¼‰
        has_references = "## å‚è€ƒæ¥æº" in report_content or "å‚è€ƒæ¥æº" in report_content

        # åªæœ‰åœ¨æŠ¥å‘ŠæœªåŒ…å«å‚è€ƒæ¥æºæ—¶æ‰æ·»åŠ è®ºæ–‡åˆ—è¡¨
        if not has_references:
            md_content += "\n\n---\n\n## å‚è€ƒè®ºæ–‡\n\n"
            try:
                papers = json.loads(papers_json) if papers_json else []
                for i, p in enumerate(papers, 1):
                    title = p.get('title', 'æœªçŸ¥æ ‡é¢˜')
                    authors = ', '.join(p.get('authors', [])[:3])
                    year = p.get('year', 'N/A')
                    url = p.get('url', '')
                    md_content += f"{i}. **{title}** ({year})\n"
                    if authors:
                        md_content += f"   - ä½œè€…: {authors}\n"
                    if url:
                        md_content += f"   - é“¾æ¥: [{url}]({url})\n"
                    md_content += "\n"
            except:
                pass

        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as f:
            f.write(md_content)
            return f.name

    def export_bibtex(papers_json: str) -> str:
        """å¯¼å‡º BibTeX æ ¼å¼"""
        import tempfile
        import json
        import re

        if not papers_json:
            return None

        try:
            papers = json.loads(papers_json)
        except:
            return None

        if not papers:
            return None

        bib_entries = []
        for i, p in enumerate(papers, 1):
            title = p.get('title', 'Unknown')
            authors = p.get('authors', [])
            year = p.get('year', '')
            url = p.get('url', '')
            source = p.get('source', 'misc')

            # ç”Ÿæˆ cite key
            first_author = authors[0].split()[-1] if authors else 'unknown'
            first_word = re.sub(r'[^a-zA-Z]', '', title.split()[0]) if title else 'paper'
            cite_key = f"{first_author.lower()}{year}{first_word.lower()}"

            # æ ¼å¼åŒ–ä½œè€…
            author_str = ' and '.join(authors) if authors else 'Unknown'

            # æ„å»º BibTeX æ¡ç›®
            entry_type = 'article' if source == 'openalex' else 'misc'
            entry = f"@{entry_type}{{{cite_key},\n"
            entry += f"  title = {{{title}}},\n"
            entry += f"  author = {{{author_str}}},\n"
            if year:
                entry += f"  year = {{{year}}},\n"
            if url:
                entry += f"  url = {{{url}}},\n"
            if source == 'arxiv':
                entry += f"  note = {{arXiv preprint}},\n"
            entry += "}\n"
            bib_entries.append(entry)

        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.bib', delete=False, encoding='utf-8') as f:
            f.write('\n'.join(bib_entries))
            return f.name

    # åˆ›å»ºç•Œé¢
    with gr.Blocks(title="ç§‘ç ”åŠ©æ‰‹ v0.4.0", theme=gr.themes.Soft()) as app:
        gr.Markdown(
            """
            # ğŸ”¬ ç§‘ç ”åŠ©æ‰‹ v0.4.0

            **v0.4.0 æ–°åŠŸèƒ½**:
            - ğŸ—ï¸ **Tab + Sidebar æ¶æ„**: æ›´æ¸…æ™°çš„å¸ƒå±€
            - ğŸ“„ **PDF å…¨æ–‡ç ”ç©¶**: ä¸‹è½½arXivè®ºæ–‡å¹¶ä½¿ç”¨å…¨æ–‡åˆ†æ
            - ğŸ¯ **æ™ºèƒ½ç­›é€‰**: LLM è¯„ä¼°è®ºæ–‡ç›¸å…³æ€§
            """
        )

        # ä¸»å¸ƒå±€ï¼šå·¦ä¾§å†…å®¹åŒº + å³ä¾§ä¾§è¾¹æ 
        with gr.Row():
            # å·¦ä¾§ï¼šTabå¯¼èˆª + å†…å®¹åŒº
            with gr.Column(scale=7):
                with gr.Tabs() as tabs:
                    # Tab 1: æœç´¢ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
                    with gr.Tab("æœç´¢", id="search"):
                        gr.Markdown("### âš¡ å¿«é€Ÿæœç´¢æ¨¡å¼\n\nå¿«é€Ÿæœç´¢ç›¸å…³è®ºæ–‡å¹¶æä¾›é˜…è¯»å»ºè®®")

                        with gr.Row():
                            search_query_input = gr.Textbox(
                                label="ç ”ç©¶é—®é¢˜",
                                placeholder="ä¾‹å¦‚ï¼šTransformeræ³¨æ„åŠ›æœºåˆ¶",
                                lines=2,
                                scale=4
                            )

                        with gr.Row():
                            search_btn = gr.Button("ğŸ” æœç´¢è®ºæ–‡", variant="primary", scale=4)
                            search_stop_btn = gr.Button("â¹ï¸ åœæ­¢", variant="stop", scale=1)

                        search_header = gr.Markdown(label="æŸ¥è¯¢åˆ†æ")
                        search_guide = gr.Markdown(label="é˜…è¯»å»ºè®®")

                        # åˆå¹¶çš„è®ºæ–‡åˆ—è¡¨ï¼ˆå¸¦æ¥æºæ ‡ç­¾ï¼‰
                        search_papers = gr.Markdown(label="ç›¸å…³è®ºæ–‡")

                        gr.Examples(
                            examples=[
                                ["Transformeræ³¨æ„åŠ›æœºåˆ¶"],
                                ["RAGæ–‡æ¡£è§£æ"],
                                ["å¤§æ¨¡å‹æœ€æ–°è¿›å±•"],
                            ],
                            inputs=search_query_input,
                        )

                    # Tab 2: æ·±åº¦ç ”ç©¶
                    with gr.Tab("æ·±åº¦ç ”ç©¶", id="deep_research"):
                        gr.Markdown("### ğŸš€ æ·±åº¦ç ”ç©¶æ¨¡å¼\n\nå­é—®é¢˜åˆ†è§£ + å¹¶è¡Œæœç´¢ + ç ”ç©¶æŠ¥å‘Šç”Ÿæˆ")

                        with gr.Row():
                            dr_query_input = gr.Textbox(
                                label="ç ”ç©¶é—®é¢˜",
                                placeholder="ä¾‹å¦‚ï¼šå¯¹æ¯” Transformer å’Œ RNN çš„ä¼˜åŠ£",
                                lines=2,
                                scale=4
                            )

                        # v0.4.0: å…¨æ–‡ç ”ç©¶é€‰é¡¹
                        with gr.Row():
                            use_fulltext_checkbox = gr.Checkbox(
                                label="ğŸ“„ ä½¿ç”¨å…¨æ–‡ç ”ç©¶ (ä¸‹è½½arXiv PDFï¼Œéœ€è¦æ›´å¤šæ—¶é—´)",
                                value=False,
                                info="å¯ç”¨åä¼šç­›é€‰ç›¸å…³è®ºæ–‡å¹¶ä¸‹è½½PDFè¿›è¡Œæ·±å…¥åˆ†æ"
                            )

                        # v0.4.5: V2æ¶æ„é€‰é¡¹
                        with gr.Row():
                            use_v2_checkbox = gr.Checkbox(
                                label="ğŸ”„ ä½¿ç”¨ V2 æ¶æ„ (Supervisor å¾ªç¯ï¼ŒåŠ¨æ€å†³ç­–ç ”ç©¶è½®æ•°)",
                                value=False,
                                info="å¯ç”¨ V2 æ¶æ„ï¼šåŠ¨æ€ç ”ç©¶å¾ªç¯ + æ˜¾å¼åæ€è¿‡ç¨‹"
                            )

                        with gr.Row():
                            dr_search_btn = gr.Button("ğŸš€ å¼€å§‹ç ”ç©¶", variant="primary", scale=3)
                            dr_fullscreen_btn = gr.Button("ğŸ“– å…¨å±æŸ¥çœ‹", variant="secondary", scale=1)
                            dr_stop_btn = gr.Button("â¹ï¸ åœæ­¢", variant="stop", scale=1)

                        dr_header = gr.Markdown(label="æŸ¥è¯¢åˆ†æ")

                        # æ€è€ƒè¿‡ç¨‹ï¼ˆæŠ˜å æ˜¾ç¤ºï¼‰
                        with gr.Accordion("ğŸ§  æ€è€ƒè¿‡ç¨‹", open=False) as dr_thinking_accordion:
                            dr_thinking = gr.Markdown(value="*ç ”ç©¶å®Œæˆåæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹*")

                        dr_report = gr.Markdown(label="ç ”ç©¶æŠ¥å‘Š")

                        # å¯¼å‡ºæŒ‰é’®è¡Œ
                        with gr.Row():
                            export_md_btn = gr.Button("ğŸ“„ å¯¼å‡º Markdown", scale=1)
                            export_bib_btn = gr.Button("ğŸ“š å¯¼å‡º BibTeX", scale=1)

                        # ä¸‹è½½æ–‡ä»¶æ˜¾ç¤ºåŒºï¼ˆå¯¼å‡ºåå¯è§ï¼‰
                        export_file = gr.File(label="ğŸ“¥ ç‚¹å‡»ä¸‹è½½", visible=True, interactive=False)

                        # åˆå¹¶çš„è®ºæ–‡åˆ—è¡¨ï¼ˆå¸¦æ¥æºæ ‡ç­¾ï¼‰
                        dr_papers = gr.Markdown(label="ç›¸å…³è®ºæ–‡")

                        gr.Examples(
                            examples=[
                                ["å¯¹æ¯” Transformer å’Œ RNN çš„ä¼˜åŠ£"],
                                ["RAGåœ¨æ–‡æ¡£è§£æä»»åŠ¡ä¸­çš„ä½œç”¨"],
                                ["å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å‘å±•è¶‹åŠ¿"],
                            ],
                            inputs=dr_query_input,
                        )

                    # Tab 3: æŠ¥å‘ŠæŸ¥çœ‹ï¼ˆå…¨å±ï¼‰
                    with gr.Tab("ğŸ“– æŠ¥å‘Š", id="report_view"):
                        gr.Markdown(
                            """
                            ### ğŸ“– ç ”ç©¶æŠ¥å‘Šå…¨æ–‡

                            åœ¨æ·±åº¦ç ”ç©¶ Tab å®Œæˆç ”ç©¶åï¼Œç‚¹å‡»ã€Œå…¨å±æŸ¥çœ‹ã€æŒ‰é’®å°†æŠ¥å‘Šæ˜¾ç¤ºåœ¨æ­¤å¤„ã€‚
                            """
                        )
                        report_fullscreen = gr.Markdown(
                            value="*ç­‰å¾…ç ”ç©¶å®ŒæˆåæŸ¥çœ‹æŠ¥å‘Š...*",
                            elem_id="fullscreen-report"
                        )

                    # Tab 4: è®ºæ–‡åº“ï¼ˆPDF ä¸Šä¼ åˆ†æï¼‰
                    with gr.Tab("è®ºæ–‡åº“", id="papers"):
                        gr.Markdown(
                            """
                            ### ğŸ“š æœ¬åœ°è®ºæ–‡åº“

                            ä¸Šä¼  PDF æ–‡ä»¶è¿›è¡Œæ·±åº¦è§£æã€‚æ”¯æŒï¼š
                            - ğŸ“„ æ™ºèƒ½æå–æ ‡é¢˜å’Œæ‘˜è¦
                            - ğŸ“– å…¨æ–‡åˆ†é¡µæµè§ˆ
                            - ğŸ§© åˆ‡ç‰‡å¤åˆ¶ä¸ç¿»è¯‘
                            - ğŸ“ AI ä¸€é”®æ€»ç»“
                            """
                        )

                        # ä¸Šä¼ åŒºåŸŸ
                        with gr.Row():
                            pdf_upload = gr.File(
                                label="ä¸Šä¼  PDF æ–‡ä»¶",
                                file_types=[".pdf"],
                                file_count="single"
                            )

                        with gr.Row():
                            analyze_btn = gr.Button("ğŸ“„ åˆ†æ PDF", variant="primary", scale=2)
                            summarize_btn = gr.Button("ğŸ“ AI æ€»ç»“", variant="secondary", scale=1)

                        # åŸºæœ¬ä¿¡æ¯
                        pdf_result_header = gr.Markdown(label="åˆ†æç»“æœ")

                        # æ‘˜è¦ï¼ˆå¯æŠ˜å ï¼‰
                        with gr.Accordion("ğŸ“ æ‘˜è¦", open=True):
                            pdf_abstract = gr.Markdown(label="æ‘˜è¦")

                        # å…¨æ–‡æµè§ˆï¼ˆåˆ†é¡µï¼‰
                        with gr.Accordion("ğŸ“– å…¨æ–‡æµè§ˆ", open=False):
                            with gr.Row():
                                pdf_page_slider = gr.Slider(
                                    minimum=1, maximum=1, step=1, value=1,
                                    label="é¡µç ", info="æ‹–åŠ¨åˆ‡æ¢é¡µé¢"
                                )
                                pdf_page_info = gr.Markdown("ç¬¬ 1 / 1 é¡µ")
                            pdf_fulltext = gr.Textbox(
                                label="å…¨æ–‡å†…å®¹",
                                lines=20,
                                max_lines=25,
                                interactive=False,
                                show_copy_button=True
                            )

                        # åˆ‡ç‰‡æµè§ˆ
                        with gr.Accordion("ğŸ§© æ–‡æ¡£åˆ‡ç‰‡", open=False):
                            pdf_chunks_info = gr.Markdown(label="åˆ‡ç‰‡ç»Ÿè®¡")
                            with gr.Row():
                                chunk_selector = gr.Slider(
                                    minimum=1, maximum=1, step=1, value=1,
                                    label="é€‰æ‹©åˆ‡ç‰‡", scale=3
                                )
                                translate_chunk_btn = gr.Button("ğŸŒ ç¿»è¯‘æ­¤åˆ‡ç‰‡", scale=1)
                            chunk_content = gr.Textbox(
                                label="åˆ‡ç‰‡å†…å®¹",
                                lines=8,
                                interactive=False,
                                show_copy_button=True
                            )
                            chunk_translation = gr.Markdown(label="ç¿»è¯‘ç»“æœ")

                        # AI æ€»ç»“ç»“æœ
                        with gr.Accordion("ğŸ“Š AI æ€»ç»“", open=False):
                            pdf_summary = gr.Markdown(value="*ç‚¹å‡»ã€ŒAI æ€»ç»“ã€æŒ‰é’®ç”Ÿæˆ*")

                        # éšè—çŠ¶æ€ï¼šå­˜å‚¨ PDF è§£æç»“æœ
                        pdf_state = gr.State(value={})

            # å³ä¾§ï¼šç»Ÿä¸€ä¾§è¾¹æ 
            with gr.Column(scale=3):
                gr.Markdown("### ğŸ“„ è®ºæ–‡è¯¦æƒ…")

                # å¼•ç”¨ç¼–å·è¾“å…¥ï¼ˆç”¨äºæŠ¥å‘Šå¼•ç”¨è·³è½¬ï¼‰
                with gr.Row():
                    cite_number_input = gr.Number(
                        label="ğŸ” æŒ‰å¼•ç”¨ç¼–å·æŸ¥æ‰¾",
                        value=None,
                        precision=0,
                        minimum=1,
                        info="è¾“å…¥æŠ¥å‘Šä¸­çš„å¼•ç”¨ç¼–å· [1][2]...",
                        scale=2
                    )
                    cite_jump_btn = gr.Button("è·³è½¬", scale=1, size="sm")

                # è®ºæ–‡é€‰æ‹©å™¨ï¼ˆä¸‹æ‹‰èœå•ï¼‰
                paper_selector = gr.Dropdown(
                    label="é€‰æ‹©è®ºæ–‡",
                    choices=[],
                    interactive=True,
                    info="ä»æœç´¢ç»“æœä¸­é€‰æ‹©è®ºæ–‡æŸ¥çœ‹è¯¦æƒ…"
                )

                # ä¾§è¾¹æ å†…å®¹ï¼ˆæ˜¾ç¤ºé€‰ä¸­è®ºæ–‡çš„è¯¦æƒ…ï¼‰
                sidebar_content = gr.Markdown(
                    value="## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\nç‚¹å‡»ä¸Šæ–¹ä¸‹æ‹‰èœå•é€‰æ‹©è®ºæ–‡æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯\n\nğŸ’¡ **æç¤º**: è¾“å…¥æŠ¥å‘Šä¸­çš„å¼•ç”¨ç¼–å· [1][2] å¯å¿«é€Ÿè·³è½¬åˆ°å¯¹åº”è®ºæ–‡",
                    label="è¯¦ç»†ä¿¡æ¯"
                )

                # è·å–å…¨æ–‡æŒ‰é’®ï¼ˆä»… arXiv è®ºæ–‡å¯ç”¨ï¼‰
                with gr.Row():
                    fetch_fulltext_btn = gr.Button("ğŸ“„ è·å–å…¨æ–‡", variant="secondary", visible=True)

                # å…¨æ–‡æ˜¾ç¤ºåŒºåŸŸï¼ˆå¯æŠ˜å ï¼‰
                with gr.Accordion("ğŸ“– è®ºæ–‡å…¨æ–‡", open=False, visible=True) as fulltext_accordion:
                    paper_fulltext = gr.Textbox(
                        label="å…¨æ–‡å†…å®¹",
                        lines=15,
                        max_lines=20,
                        interactive=False,
                        show_copy_button=True,
                        value=""
                    )

                # éšè—çš„çŠ¶æ€ï¼šå­˜å‚¨æ‰€æœ‰è®ºæ–‡æ•°æ®JSON
                papers_state = gr.State(value="[]")

                # éšè—çš„çŠ¶æ€ï¼šæŠ¥å‘Šæ¥æºåˆ—è¡¨ï¼ˆä¸å¼•ç”¨ç¼–å·å¯¹åº”ï¼‰
                report_sources_state = gr.State(value="[]")

                # éšè—çš„çŠ¶æ€ï¼šå½“å‰é€‰ä¸­è®ºæ–‡çš„ä¿¡æ¯
                current_paper_state = gr.State(value="{}")

        # äº‹ä»¶ç»‘å®š

        # æœç´¢Tab - å¿«é€Ÿæœç´¢
        search_report_dummy = gr.State(value="")  # å ä½ï¼Œå¿«é€Ÿæ¨¡å¼ä¸éœ€è¦report
        search_thinking_dummy = gr.State(value="")  # å ä½ï¼Œå¿«é€Ÿæ¨¡å¼ä¸éœ€è¦thinking
        search_sources_dummy = gr.State(value="[]")  # å ä½ï¼Œå¿«é€Ÿæ¨¡å¼ä¸éœ€è¦report_sources

        search_event = search_btn.click(
            fn=search_papers_stream,
            inputs=[search_query_input, gr.State(value="å¿«é€Ÿæœç´¢"), gr.State(value=False)],
            outputs=[search_header, search_report_dummy, search_thinking_dummy, search_guide, search_papers, papers_state, search_sources_dummy]
        )

        submit_search_event = search_query_input.submit(
            fn=search_papers_stream,
            inputs=[search_query_input, gr.State(value="å¿«é€Ÿæœç´¢"), gr.State(value=False)],
            outputs=[search_header, search_report_dummy, search_thinking_dummy, search_guide, search_papers, papers_state, search_sources_dummy]
        )

        search_stop_btn.click(fn=None, inputs=None, outputs=None, cancels=[search_event, submit_search_event])

        # æ·±åº¦ç ”ç©¶Tab
        dr_guide_dummy = gr.State(value="")  # å ä½ï¼Œæ·±åº¦æ¨¡å¼ä¸éœ€è¦guide

        dr_event = dr_search_btn.click(
            fn=search_papers_stream,
            inputs=[dr_query_input, gr.State(value="æ·±åº¦ç ”ç©¶"), use_fulltext_checkbox, use_v2_checkbox],
            outputs=[dr_header, dr_report, dr_thinking, dr_guide_dummy, dr_papers, papers_state, report_sources_state]
        )

        submit_dr_event = dr_query_input.submit(
            fn=search_papers_stream,
            inputs=[dr_query_input, gr.State(value="æ·±åº¦ç ”ç©¶"), use_fulltext_checkbox, use_v2_checkbox],
            outputs=[dr_header, dr_report, dr_thinking, dr_guide_dummy, dr_papers, papers_state, report_sources_state]
        )

        dr_stop_btn.click(fn=None, inputs=None, outputs=None, cancels=[dr_event, submit_dr_event])

        # å¯¼å‡ºæŒ‰é’®äº‹ä»¶
        export_md_btn.click(
            fn=export_markdown,
            inputs=[dr_report, papers_state],
            outputs=[export_file]
        )

        export_bib_btn.click(
            fn=export_bibtex,
            inputs=[papers_state],
            outputs=[export_file]
        )

        # å…¨å±æŸ¥çœ‹æŒ‰é’® - å¤åˆ¶æŠ¥å‘Šåˆ°æŠ¥å‘Š Tab
        def copy_report_to_fullscreen(report_content):
            """å¤åˆ¶æŠ¥å‘Šå†…å®¹åˆ°å…¨å± Tab"""
            if not report_content or report_content.startswith("â³"):
                return "## âš ï¸ æš‚æ— æŠ¥å‘Š\n\nè¯·å…ˆåœ¨ã€Œæ·±åº¦ç ”ç©¶ã€Tab ä¸­å®Œæˆç ”ç©¶ã€‚"
            return f"## ğŸ“– ç ”ç©¶æŠ¥å‘Š\n\n{report_content}"

        dr_fullscreen_btn.click(
            fn=copy_report_to_fullscreen,
            inputs=[dr_report],
            outputs=[report_fullscreen]
        ).then(
            fn=None,
            inputs=None,
            outputs=None,
            js="() => { document.querySelector('[data-tab-id=\"report_view\"]')?.click(); }"
        )

        # è®ºæ–‡åº“Tab - PDF åˆ†æåŠäº¤äº’
        analyze_btn.click(
            fn=analyze_pdf,
            inputs=[pdf_upload],
            outputs=[
                pdf_result_header, pdf_abstract,
                pdf_fulltext, pdf_page_info, pdf_page_slider,
                pdf_chunks_info, chunk_selector,
                chunk_content, chunk_translation,
                pdf_state
            ]
        )

        # å…¨æ–‡åˆ†é¡µåˆ‡æ¢
        pdf_page_slider.change(
            fn=update_fulltext_page,
            inputs=[pdf_page_slider, pdf_state],
            outputs=[pdf_fulltext, pdf_page_info]
        )

        # åˆ‡ç‰‡é€‰æ‹©åˆ‡æ¢
        chunk_selector.change(
            fn=update_chunk_content,
            inputs=[chunk_selector, pdf_state],
            outputs=[chunk_content, chunk_translation]
        )

        # ç¿»è¯‘åˆ‡ç‰‡
        translate_chunk_btn.click(
            fn=translate_chunk,
            inputs=[chunk_selector, pdf_state],
            outputs=[chunk_translation]
        )

        # AI æ€»ç»“
        summarize_btn.click(
            fn=summarize_pdf,
            inputs=[pdf_state],
            outputs=[pdf_summary]
        )

        # ä¾§è¾¹æ ï¼šæ›´æ–°è®ºæ–‡é€‰æ‹©å™¨ï¼ˆå½“æœç´¢å®Œæˆåï¼‰
        def update_paper_selector(papers_json: str):
            """æ›´æ–°è®ºæ–‡ä¸‹æ‹‰èœå•"""
            import json
            try:
                papers = json.loads(papers_json) if papers_json else []
                if not papers:
                    return gr.Dropdown(choices=[], value=None)

                choices = []
                for i, p in enumerate(papers, 1):
                    title = p.get('title', 'æœªçŸ¥æ ‡é¢˜')[:60]
                    choices.append((f"[{i}] {title}...", i-1))  # (æ˜¾ç¤ºæ–‡æœ¬, å€¼)

                return gr.Dropdown(choices=choices, value=None)
            except:
                return gr.Dropdown(choices=[], value=None)

        # å½“æœç´¢å®Œæˆæ—¶æ›´æ–°é€‰æ‹©å™¨
        papers_state.change(
            fn=update_paper_selector,
            inputs=[papers_state],
            outputs=[paper_selector]
        )

        # ä¾§è¾¹æ ï¼šå½“é€‰æ‹©è®ºæ–‡æ—¶æ˜¾ç¤ºè¯¦æƒ…
        def show_selected_paper(paper_index, papers_json):
            """æ˜¾ç¤ºé€‰ä¸­çš„è®ºæ–‡è¯¦æƒ…ï¼ŒåŒæ—¶æ›´æ–° current_paper_state"""
            import json
            if paper_index is None or not papers_json:
                return (
                    "## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\nè¯·å…ˆæœç´¢è®ºæ–‡ï¼Œç„¶åä»ä¸Šæ–¹ä¸‹æ‹‰èœå•é€‰æ‹©",
                    "{}"
                )

            try:
                papers = json.loads(papers_json)
                if 0 <= paper_index < len(papers):
                    paper = papers[paper_index]
                    paper_json = json.dumps(paper, ensure_ascii=False)
                    return (
                        show_paper_details(paper_json),
                        paper_json
                    )
                else:
                    return (
                        "## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\nè®ºæ–‡ç´¢å¼•æ— æ•ˆ",
                        "{}"
                    )
            except Exception as e:
                return (
                    f"## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\nè§£æå‡ºé”™: {str(e)}",
                    "{}"
                )

        paper_selector.change(
            fn=show_selected_paper,
            inputs=[paper_selector, papers_state],
            outputs=[sidebar_content, current_paper_state]
        )

        # å¼•ç”¨ç¼–å·è·³è½¬äº‹ä»¶
        cite_jump_btn.click(
            fn=jump_to_citation,
            inputs=[cite_number_input, report_sources_state, papers_state],
            outputs=[sidebar_content, paper_selector, current_paper_state]
        )

        # è¾“å…¥æ¡†å›è½¦ä¹Ÿè§¦å‘è·³è½¬
        cite_number_input.submit(
            fn=jump_to_citation,
            inputs=[cite_number_input, report_sources_state, papers_state],
            outputs=[sidebar_content, paper_selector, current_paper_state]
        )

        # è·å–å…¨æ–‡æŒ‰é’®äº‹ä»¶
        fetch_fulltext_btn.click(
            fn=fetch_paper_fulltext,
            inputs=[current_paper_state],
            outputs=[paper_fulltext]
        )

    return app


if __name__ == "__main__":
    app = create_app()
    app.launch(share=False)
