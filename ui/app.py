"""Gradio Webç•Œé¢ - Tab + Sidebaræ¶æ„ v0.4.0"""
import gradio as gr
import sys
import time
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
src_dir = project_root / "src"
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from main import ResearchAssistant
from tools.reading_guide import ReadingGuide
from tools.pdf import PaperProcessor


def create_app():
    """åˆ›å»ºGradioåº”ç”¨"""
    assistant = ResearchAssistant()
    pdf_processor = PaperProcessor()

    def show_paper_details(paper_json: str) -> str:
        """æ˜¾ç¤ºè®ºæ–‡è¯¦æƒ…åˆ°ä¾§è¾¹æ """
        if not paper_json or paper_json == "{}":
            return "## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\nç‚¹å‡»è®ºæ–‡æ ‡é¢˜æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯"

        import json
        try:
            paper = json.loads(paper_json)
        except:
            return "## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\næ•°æ®è§£æå¤±è´¥"

        output = "## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\n"

        # æ ‡é¢˜
        title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
        output += f"### {title}\n\n"

        # ä¸­æ–‡æ ‡é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
        title_cn = paper.get('title_cn', '')
        if title_cn:
            output += f"*{title_cn}*\n\n"

        # ä½œè€…
        authors = paper.get('authors', [])
        if authors:
            output += f"**ğŸ‘¥ ä½œè€…**: {', '.join(authors[:5])}\n\n"

        # å¹´ä»½ã€å¼•ç”¨æ•°
        year = paper.get('year', 'N/A')
        output += f"**ğŸ“… å¹´ä»½**: {year}\n\n"

        citation_count = paper.get('citation_count')
        if citation_count:
            output += f"**ğŸ“Š å¼•ç”¨æ•°**: {citation_count}\n\n"

        # æ¥æº
        source = paper.get('source', '').upper()
        if source:
            output += f"**ğŸ”– æ¥æº**: {source}\n\n"

        # æ‘˜è¦
        abstract = paper.get('abstract', '')
        summary = paper.get('summary', '')  # LLMç”Ÿæˆçš„ä¸­æ–‡æ‘˜è¦

        if summary:
            output += f"**ğŸ“ æ‘˜è¦** (AIç”Ÿæˆ):\n\n{summary}\n\n"

        if abstract:
            output += f"**ğŸ“„ åŸæ–‡æ‘˜è¦**:\n\n{abstract}\n\n"

        # URL
        url = paper.get('url', '')
        if url:
            output += f"**ğŸ”— é“¾æ¥**: [{url}]({url})\n\n"

        # arXiv IDï¼ˆå¦‚æœæ˜¯arXivè®ºæ–‡ï¼Œæ˜¾ç¤ºè·å–å…¨æ–‡æŒ‰é’®æç¤ºï¼‰
        if source == 'ARXIV' and url:
            import re
            match = re.search(r'(\d{4}\.\d{4,5})', url)
            if match:
                arxiv_id = match.group(1)
                output += f"\n---\n\nğŸ’¡ **æç¤º**: è¿™æ˜¯ arXiv è®ºæ–‡ï¼Œå¯åœ¨æ·±åº¦ç ”ç©¶æ¨¡å¼ä¸­å‹¾é€‰ã€Œä½¿ç”¨å…¨æ–‡ç ”ç©¶ã€æ¥è·å– PDF å…¨æ–‡ã€‚\n\n"

        return output

    def format_paper(paper: dict, index: int, show_source: bool = False) -> str:
        """æ ¼å¼åŒ–å•ç¯‡è®ºæ–‡"""
        title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
        title_cn = paper.get('title_cn', '')
        source_tag = f" [{paper.get('source', '')}]" if show_source else ""

        if title_cn:
            output = f"**[{index}]{source_tag} {title}**\n\n"
            output += f"ğŸ“– *{title_cn}*\n\n"
        else:
            output = f"**[{index}]{source_tag} {title}**\n\n"

        authors = paper.get('authors', [])
        if authors:
            output += f"- ä½œè€…: {', '.join(authors[:3])}\n"
        output += f"- å¹´ä»½: {paper.get('year', 'N/A')}\n"
        if paper.get("citation_count"):
            output += f"- å¼•ç”¨: {paper['citation_count']}\n"

        summary = paper.get('summary', '')
        if summary:
            output += f"- ğŸ“ **æ‘˜è¦**: {summary}\n"
        elif paper.get("abstract"):
            abstract = paper['abstract']
            if len(abstract) > 200:
                abstract = abstract[:200] + "..."
            output += f"- æ‘˜è¦: {abstract}\n"

        if paper.get('url'):
            output += f"- [ğŸ”— æŸ¥çœ‹è®ºæ–‡]({paper['url']})\n\n"
        output += "---\n\n"
        return output

    def format_reading_guide(guide: dict) -> str:
        """æ ¼å¼åŒ–é˜…è¯»å¯¼èˆª"""
        if not guide or guide.get("error"):
            return ""

        lines = ["## ğŸ“š é˜…è¯»å»ºè®®\n"]

        if guide.get("summary"):
            lines.append(f"{guide['summary']}\n")

        if guide.get("entry_point"):
            ep = guide["entry_point"]
            title = ep.get('title', '')[:60]
            lines.append(f"### ğŸŒŸ å…¥é—¨å¿…è¯»")
            lines.append(f"**[{ep.get('index', '')}] {title}...**")
            lines.append(f"> {ep.get('reason', '')}\n")

        if guide.get("core_papers"):
            lines.append(f"### ğŸ“Œ æ ¸å¿ƒè®ºæ–‡")
            for cp in guide["core_papers"]:
                title = cp.get('title', '')[:60]
                lines.append(f"- **[{cp.get('index', '')}] {title}...** â†’ {cp.get('reason', '')}")
            lines.append("")

        if guide.get("latest"):
            lt = guide["latest"]
            title = lt.get('title', '')[:60]
            lines.append(f"### ğŸ†• æœ€æ–°è¿›å±•")
            lines.append(f"**[{lt.get('index', '')}] {title}...**")
            lines.append(f"> {lt.get('reason', '')}\n")

        if guide.get("reading_order"):
            order = guide["reading_order"][:5]
            lines.append(f"### ğŸ“– æ¨èé˜…è¯»é¡ºåº")
            lines.append(f"**{' â†’ '.join(map(str, order))}**\n")

        if guide.get("categories"):
            lines.append(f"### ğŸ“‚ è®ºæ–‡åˆ†ç±»\n")
            for cat in guide["categories"]:
                name = cat.get("name", "å…¶ä»–")
                desc = cat.get("description", "")
                papers = cat.get("papers", [])
                paper_nums = [str(p.get("index", "")) for p in papers]
                cat_text = f"#### {name} ({len(papers)}ç¯‡)\n"
                cat_text += f"- è®ºæ–‡ç¼–å·: `{', '.join(paper_nums)}`\n"
                if desc:
                    cat_text += f"- è¯´æ˜: {desc}\n"
                lines.append(cat_text)

        return "\n".join(lines)

    def search_papers_stream(query: str, mode: str = "auto", use_fulltext: bool = False):
        """æµå¼æœç´¢è®ºæ–‡ - å®æ—¶æ˜¾ç¤ºè¿›åº¦

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            mode: æœç´¢æ¨¡å¼ ("auto", "simple", "deep_research")
            use_fulltext: æ˜¯å¦ä½¿ç”¨å…¨æ–‡ç ”ç©¶ï¼ˆä»…æ·±åº¦ç ”ç©¶æ¨¡å¼æœ‰æ•ˆï¼‰
        """
        if not query.strip():
            yield "è¯·è¾“å…¥ç ”ç©¶é—®é¢˜", "", "", "", "", "{}"
            return

        start_time = datetime.now()
        actual_mode = "auto" if mode == "æ™ºèƒ½åˆ¤æ–­" else ("simple" if mode == "å¿«é€Ÿæœç´¢" else "deep_research")

        # é˜¶æ®µ1: æ˜¾ç¤ºå¼€å§‹çŠ¶æ€
        start_time_str = start_time.strftime("%H:%M:%S")
        header = f"## ğŸ” æŸ¥è¯¢åˆ†æ\n\n"
        header += f"**æŸ¥è¯¢**: {query}\n\n"

        # æ˜¾ç¤ºæ¨¡å¼ï¼ˆåŒ…æ‹¬å…¨æ–‡ç ”ç©¶çŠ¶æ€ï¼‰
        if actual_mode == 'deep_research' or (actual_mode == 'auto' and len(query) > 20):
            mode_display = "ğŸš€ æ·±åº¦ç ”ç©¶"
            if use_fulltext:
                mode_display += " (ğŸ“„ å…¨æ–‡æ¨¡å¼)"
        else:
            mode_display = "âš¡ å¿«é€Ÿæœç´¢"

        header += f"**æ¨¡å¼**: {mode_display}\n\n"
        header += f"**çŠ¶æ€**: â³ æ­£åœ¨åˆ†ææŸ¥è¯¢... (å¼€å§‹äº {start_time_str})\n"

        yield header, f"â³ æ­£åœ¨åˆ†æé—®é¢˜ï¼Œè¯·ç¨å€™...\n\n> å¼€å§‹æ—¶é—´: {start_time_str}ï¼Œå¯ç‚¹å‡»ã€Œåœæ­¢ã€æŒ‰é’®å–æ¶ˆ", "", "*ğŸ”„ æœç´¢ä¸­...*", "*ğŸ”„ æœç´¢ä¸­...*", "{}"

        # é˜¶æ®µ2: æ‰§è¡Œæœç´¢ï¼ˆä¼ å…¥ use_fulltext å‚æ•°ï¼‰
        try:
            result = assistant.process_query(query, mode=actual_mode, use_fulltext=use_fulltext)
        except Exception as e:
            elapsed = (datetime.now() - start_time).total_seconds()
            error_msg = f"## âŒ æœç´¢å‡ºé”™\n\nè€—æ—¶: {elapsed:.1f}ç§’\n\né”™è¯¯: {str(e)}"
            yield header.replace("â³ æ­£åœ¨åˆ†ææŸ¥è¯¢...", f"âŒ å‡ºé”™ ({elapsed:.1f}s)"), error_msg, "", "", "", "{}"
            return

        elapsed = (datetime.now() - start_time).total_seconds()

        # æ›´æ–° header
        header = f"## ğŸ” æŸ¥è¯¢åˆ†æ\n\n"
        header += f"**æ„å›¾**: {result.get('intent', 'æœªè¯†åˆ«')}\n\n"
        keywords = result.get('keywords', [])
        if keywords:
            header += f"**æœç´¢å…³é”®è¯**: `{'`, `'.join(keywords)}`\n\n"
        mode_display = "ğŸš€ æ·±åº¦ç ”ç©¶" if result['mode'] == "deep_research" else "âš¡ å¿«é€Ÿæœç´¢"
        header += f"**æ¨¡å¼**: {mode_display} | "
        header += f"**æœç´¢æº**: {', '.join(result.get('sources', [])) or 'æ— '} | "
        header += f"**æ‰¾åˆ°**: {result.get('total_found', len(result.get('papers', [])))} ç¯‡\n\n"
        header += f"**æ€»è€—æ—¶**: âœ… {elapsed:.1f}ç§’\n"

        # æ·±åº¦ç ”ç©¶æ¨¡å¼ï¼šæ˜¾ç¤ºç ”ç©¶æŠ¥å‘Š
        report_output = ""
        if result['mode'] == 'deep_research' and result.get('report'):
            # æ˜¾ç¤ºå­é—®é¢˜åˆ†è§£
            decomposition = result.get('decomposition', {})
            if decomposition:
                report_output += "## ğŸ“‹ é—®é¢˜åˆ†è§£\n\n"
                report_output += f"**é—®é¢˜ç±»å‹**: {decomposition.get('query_type', 'N/A')}\n\n"
                report_output += f"**ç ”ç©¶ç­–ç•¥**: {decomposition.get('strategy', 'N/A')}\n\n"
                sub_questions = decomposition.get('sub_questions', [])
                if sub_questions:
                    report_output += "**å­é—®é¢˜**:\n"
                    for i, sq in enumerate(sub_questions, 1):
                        report_output += f"{i}. {sq.get('question', '')} *(ç›®çš„: {sq.get('purpose', '')})*\n"
                    report_output += "\n"

            # æ˜¾ç¤ºç ”ç©¶æŠ¥å‘Š
            report_output += "---\n\n"
            report_output += result['report']

            # æ˜¾ç¤ºå…ƒæ•°æ®å’Œå„é˜¶æ®µè€—æ—¶
            metadata = result.get('metadata', {})
            if metadata:
                report_output += "\n\n---\n"
                report_output += f"**æ€»è€—æ—¶: {metadata.get('duration_seconds', 0):.1f}ç§’** | "
                report_output += f"å­é—®é¢˜: {metadata.get('sub_questions_count', 0)}ä¸ª | "
                report_output += f"è®ºæ–‡: {metadata.get('total_papers', 0)}ç¯‡\n\n"

                # æ˜¾ç¤ºå„é˜¶æ®µè€—æ—¶è¯¦æƒ…
                stage_times = metadata.get('stage_times', {})
                if stage_times:
                    report_output += "**â±ï¸ å„é˜¶æ®µè€—æ—¶:**\n"
                    for stage, time_sec in stage_times.items():
                        stage_name = stage.split('_', 1)[1] if '_' in stage else stage
                        report_output += f"- {stage_name}: {time_sec:.1f}ç§’\n"

        # é˜…è¯»å¯¼èˆªï¼ˆå¿«é€Ÿæœç´¢æ¨¡å¼ï¼‰
        reading_guide = result.get('reading_guide', {})
        guide_output = format_reading_guide(reading_guide) if result['mode'] == 'simple' else ""

        # è®ºæ–‡åˆ—è¡¨ï¼ˆæ·±åº¦ç ”ç©¶æ¨¡å¼ä½¿ç”¨åŸå§‹æœç´¢ç»“æœï¼Œç‹¬ç«‹äºæŠ¥å‘Šå¼•ç”¨ï¼‰
        # åŒæ—¶å‡†å¤‡ papers_list_json ä¾›ä¾§è¾¹æ é€‰æ‹©
        import json
        papers_list = []

        if result['mode'] == 'deep_research':
            arxiv_papers = result.get('arxiv_papers', [])
            openalex_papers = result.get('openalex_papers', [])
            papers_list = arxiv_papers + openalex_papers  # åˆå¹¶ç”¨äºä¾§è¾¹æ 

            # arXiv è®ºæ–‡ç‹¬ç«‹ç¼–å·ä»1å¼€å§‹
            arxiv_output = f"### arXiv æœ€æ–°è®ºæ–‡ ({len(arxiv_papers)}ç¯‡)\n\n"
            arxiv_output += "> â„¹ï¸ *ä»¥ä¸‹ç¼–å·ä¸æŠ¥å‘Šå¼•ç”¨ç¼–å·æ— å…³*\n\n"
            for i, paper in enumerate(arxiv_papers, 1):
                arxiv_output += format_paper(paper, i)
            if not arxiv_papers:
                arxiv_output += "*æš‚æ— ç»“æœ*\n"

            # OpenAlex è®ºæ–‡ç‹¬ç«‹ç¼–å·ä»1å¼€å§‹
            openalex_output = f"### OpenAlex ç»å…¸è®ºæ–‡ ({len(openalex_papers)}ç¯‡)\n\n"
            openalex_output += "> â„¹ï¸ *ä»¥ä¸‹ç¼–å·ä¸æŠ¥å‘Šå¼•ç”¨ç¼–å·æ— å…³*\n\n"
            for i, paper in enumerate(openalex_papers, 1):
                openalex_output += format_paper(paper, i)
            if not openalex_papers:
                openalex_output += "*æš‚æ— ç»“æœ*\n"
        else:
            arxiv_papers = result.get('arxiv_papers', [])
            openalex_papers = result.get('openalex_papers', [])
            papers_list = arxiv_papers + openalex_papers

            arxiv_output = f"### arXiv æœ€æ–°è®ºæ–‡ ({len(arxiv_papers)}ç¯‡)\n\n"
            for i, paper in enumerate(arxiv_papers, 1):
                arxiv_output += format_paper(paper, i)
            if not arxiv_papers:
                arxiv_output += "*æš‚æ— ç»“æœ*\n"

            start_index = len(arxiv_papers) + 1
            openalex_output = f"### OpenAlex ç»å…¸è®ºæ–‡ ({len(openalex_papers)}ç¯‡)\n\n"
            for i, paper in enumerate(openalex_papers, start_index):
                openalex_output += format_paper(paper, i)
            if not openalex_papers:
                openalex_output += "*æš‚æ— ç»“æœ*\n"

        # è¿”å›è®ºæ–‡åˆ—è¡¨JSONä¾›ä¾§è¾¹æ ä½¿ç”¨
        papers_json = json.dumps(papers_list, ensure_ascii=False)

        yield header, report_output, guide_output, arxiv_output, openalex_output, papers_json

    def analyze_pdf(pdf_file):
        """åˆ†æä¸Šä¼ çš„ PDF æ–‡ä»¶"""
        if pdf_file is None:
            return "è¯·å…ˆä¸Šä¼  PDF æ–‡ä»¶", "", "", ""

        try:
            # è·å–æ–‡ä»¶è·¯å¾„
            pdf_path = pdf_file.name if hasattr(pdf_file, 'name') else str(pdf_file)

            # å¤„ç† PDF
            result = pdf_processor.process_local_pdf(pdf_path)

            if not result.success:
                return f"## âŒ åˆ†æå¤±è´¥\n\n{result.error}", "", "", ""

            # æ„å»ºç»“æœ
            header = f"## âœ… åˆ†æå®Œæˆ\n\n"
            header += f"**æ ‡é¢˜**: {result.title}\n\n"
            header += f"**é¡µæ•°**: {result.total_pages}\n\n"
            header += f"**å…¨æ–‡é•¿åº¦**: {len(result.full_text)} å­—ç¬¦\n\n"
            header += f"**åˆ‡ç‰‡æ•°**: {len(result.chunks)}\n"

            # æ‘˜è¦
            abstract = ""
            if result.abstract:
                abstract = f"### ğŸ“ æ‘˜è¦\n\n{result.abstract}"
            else:
                abstract = "*æœªèƒ½è‡ªåŠ¨æå–æ‘˜è¦*"

            # å…¨æ–‡é¢„è§ˆ
            fulltext_preview = result.full_text[:3000] if result.full_text else "æ— æ³•æå–å…¨æ–‡"

            # åˆ‡ç‰‡ä¿¡æ¯
            chunks_info = "### ğŸ“Š åˆ‡ç‰‡ç»Ÿè®¡\n\n"
            if result.chunks:
                total_tokens = sum(c.token_count for c in result.chunks)
                avg_tokens = total_tokens / len(result.chunks)
                chunks_info += f"- åˆ‡ç‰‡æ•°é‡: {len(result.chunks)}\n"
                chunks_info += f"- æ€» Token æ•°: {total_tokens}\n"
                chunks_info += f"- å¹³å‡æ¯ç‰‡: {avg_tokens:.0f} tokens\n\n"

                # æ˜¾ç¤ºå‰ 5 ä¸ªåˆ‡ç‰‡é¢„è§ˆ
                chunks_info += "**å‰ 5 ä¸ªåˆ‡ç‰‡é¢„è§ˆ:**\n\n"
                for i, chunk in enumerate(result.chunks[:5], 1):
                    preview = chunk.text[:100].replace("\n", " ")
                    chunks_info += f"{i}. (ç¬¬{chunk.pages}é¡µ, {chunk.token_count}tokens) `{preview}...`\n\n"
            else:
                chunks_info += "*æ— åˆ‡ç‰‡ä¿¡æ¯*"

            return header, abstract, fulltext_preview, chunks_info

        except Exception as e:
            return f"## âŒ å¤„ç†å‡ºé”™\n\n{str(e)}", "", "", ""

    # åˆ›å»ºç•Œé¢
    with gr.Blocks(title="ç§‘ç ”åŠ©æ‰‹ v0.4.0", theme=gr.themes.Soft()) as app:
        gr.Markdown(
            """
            # ğŸ”¬ ç§‘ç ”åŠ©æ‰‹ v0.4.0

            **v0.4.0 æ–°åŠŸèƒ½**:
            - ğŸ—ï¸ **Tab + Sidebar æ¶æ„**: æ›´æ¸…æ™°çš„å¸ƒå±€
            - ğŸ“„ **PDF å…¨æ–‡ç ”ç©¶**: ä¸‹è½½arXivè®ºæ–‡å¹¶ä½¿ç”¨å…¨æ–‡åˆ†æ
            - ğŸ¯ **æ™ºèƒ½ç­›é€‰**: LLM è¯„ä¼°è®ºæ–‡ç›¸å…³æ€§
            """
        )

        # ä¸»å¸ƒå±€ï¼šå·¦ä¾§å†…å®¹åŒº + å³ä¾§ä¾§è¾¹æ 
        with gr.Row():
            # å·¦ä¾§ï¼šTabå¯¼èˆª + å†…å®¹åŒº
            with gr.Column(scale=7):
                with gr.Tabs() as tabs:
                    # Tab 1: æœç´¢ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
                    with gr.Tab("æœç´¢", id="search"):
                        gr.Markdown("### âš¡ å¿«é€Ÿæœç´¢æ¨¡å¼\n\nå¿«é€Ÿæœç´¢ç›¸å…³è®ºæ–‡å¹¶æä¾›é˜…è¯»å»ºè®®")

                        with gr.Row():
                            search_query_input = gr.Textbox(
                                label="ç ”ç©¶é—®é¢˜",
                                placeholder="ä¾‹å¦‚ï¼šTransformeræ³¨æ„åŠ›æœºåˆ¶",
                                lines=2,
                                scale=4
                            )

                        with gr.Row():
                            search_btn = gr.Button("ğŸ” æœç´¢è®ºæ–‡", variant="primary", scale=4)
                            search_stop_btn = gr.Button("â¹ï¸ åœæ­¢", variant="stop", scale=1)

                        search_header = gr.Markdown(label="æŸ¥è¯¢åˆ†æ")
                        search_guide = gr.Markdown(label="é˜…è¯»å»ºè®®")

                        with gr.Row():
                            with gr.Column():
                                search_arxiv = gr.Markdown(label="arXivè®ºæ–‡")
                            with gr.Column():
                                search_openalex = gr.Markdown(label="OpenAlexè®ºæ–‡")

                        gr.Examples(
                            examples=[
                                ["Transformeræ³¨æ„åŠ›æœºåˆ¶"],
                                ["RAGæ–‡æ¡£è§£æ"],
                                ["å¤§æ¨¡å‹æœ€æ–°è¿›å±•"],
                            ],
                            inputs=search_query_input,
                        )

                    # Tab 2: æ·±åº¦ç ”ç©¶
                    with gr.Tab("æ·±åº¦ç ”ç©¶", id="deep_research"):
                        gr.Markdown("### ğŸš€ æ·±åº¦ç ”ç©¶æ¨¡å¼\n\nå­é—®é¢˜åˆ†è§£ + å¹¶è¡Œæœç´¢ + ç ”ç©¶æŠ¥å‘Šç”Ÿæˆ")

                        with gr.Row():
                            dr_query_input = gr.Textbox(
                                label="ç ”ç©¶é—®é¢˜",
                                placeholder="ä¾‹å¦‚ï¼šå¯¹æ¯” Transformer å’Œ RNN çš„ä¼˜åŠ£",
                                lines=2,
                                scale=4
                            )

                        # v0.4.0: å…¨æ–‡ç ”ç©¶é€‰é¡¹
                        with gr.Row():
                            use_fulltext_checkbox = gr.Checkbox(
                                label="ğŸ“„ ä½¿ç”¨å…¨æ–‡ç ”ç©¶ (ä¸‹è½½arXiv PDFï¼Œéœ€è¦æ›´å¤šæ—¶é—´)",
                                value=False,
                                info="å¯ç”¨åä¼šç­›é€‰ç›¸å…³è®ºæ–‡å¹¶ä¸‹è½½PDFè¿›è¡Œæ·±å…¥åˆ†æ"
                            )

                        with gr.Row():
                            dr_search_btn = gr.Button("ğŸš€ å¼€å§‹ç ”ç©¶", variant="primary", scale=4)
                            dr_stop_btn = gr.Button("â¹ï¸ åœæ­¢", variant="stop", scale=1)

                        dr_header = gr.Markdown(label="æŸ¥è¯¢åˆ†æ")
                        dr_report = gr.Markdown(label="ç ”ç©¶æŠ¥å‘Š")

                        with gr.Row():
                            with gr.Column():
                                dr_arxiv = gr.Markdown(label="arXivè®ºæ–‡")
                            with gr.Column():
                                dr_openalex = gr.Markdown(label="OpenAlexè®ºæ–‡")

                        gr.Examples(
                            examples=[
                                ["å¯¹æ¯” Transformer å’Œ RNN çš„ä¼˜åŠ£"],
                                ["RAGåœ¨æ–‡æ¡£è§£æä»»åŠ¡ä¸­çš„ä½œç”¨"],
                                ["å¤šæ¨¡æ€å¤§æ¨¡å‹çš„å‘å±•è¶‹åŠ¿"],
                            ],
                            inputs=dr_query_input,
                        )

                    # Tab 3: è®ºæ–‡åº“ï¼ˆPDF ä¸Šä¼ åˆ†æï¼‰
                    with gr.Tab("è®ºæ–‡åº“", id="papers"):
                        gr.Markdown(
                            """
                            ### ğŸ“š æœ¬åœ°è®ºæ–‡åº“

                            ä¸Šä¼  PDF æ–‡ä»¶è¿›è¡Œè§£æå’Œåˆ†æã€‚æ”¯æŒï¼š
                            - ğŸ“¤ å•ä¸ª PDF ä¸Šä¼ åˆ†æ
                            - ğŸ“„ æå–å…¨æ–‡å’Œæ‘˜è¦
                            - ğŸ”¢ æ–‡æ¡£åˆ‡ç‰‡ç»Ÿè®¡
                            """
                        )

                        with gr.Row():
                            pdf_upload = gr.File(
                                label="ä¸Šä¼  PDF æ–‡ä»¶",
                                file_types=[".pdf"],
                                file_count="single"
                            )

                        with gr.Row():
                            analyze_btn = gr.Button("ğŸ“„ åˆ†æ PDF", variant="primary")

                        # åˆ†æç»“æœæ˜¾ç¤º
                        pdf_result_header = gr.Markdown(label="åˆ†æç»“æœ")
                        pdf_abstract = gr.Markdown(label="æ‘˜è¦")
                        pdf_fulltext = gr.Textbox(
                            label="å…¨æ–‡é¢„è§ˆï¼ˆå‰3000å­—ï¼‰",
                            lines=15,
                            max_lines=20,
                            interactive=False
                        )
                        pdf_chunks_info = gr.Markdown(label="åˆ‡ç‰‡ä¿¡æ¯")

            # å³ä¾§ï¼šç»Ÿä¸€ä¾§è¾¹æ 
            with gr.Column(scale=3):
                gr.Markdown("### ğŸ“„ è®ºæ–‡è¯¦æƒ…")

                # è®ºæ–‡é€‰æ‹©å™¨ï¼ˆä¸‹æ‹‰èœå•ï¼‰
                paper_selector = gr.Dropdown(
                    label="é€‰æ‹©è®ºæ–‡",
                    choices=[],
                    interactive=True,
                    info="ä»æœç´¢ç»“æœä¸­é€‰æ‹©è®ºæ–‡æŸ¥çœ‹è¯¦æƒ…"
                )

                # ä¾§è¾¹æ å†…å®¹ï¼ˆæ˜¾ç¤ºé€‰ä¸­è®ºæ–‡çš„è¯¦æƒ…ï¼‰
                sidebar_content = gr.Markdown(
                    value="## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\nç‚¹å‡»ä¸Šæ–¹ä¸‹æ‹‰èœå•é€‰æ‹©è®ºæ–‡æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯",
                    label="è¯¦ç»†ä¿¡æ¯"
                )

                # éšè—çš„çŠ¶æ€ï¼šå­˜å‚¨æ‰€æœ‰è®ºæ–‡æ•°æ®JSON
                papers_state = gr.State(value="[]")

        # äº‹ä»¶ç»‘å®š

        # æœç´¢Tab - å¿«é€Ÿæœç´¢
        search_report_dummy = gr.State(value="")  # å ä½ï¼Œå¿«é€Ÿæ¨¡å¼ä¸éœ€è¦report

        search_event = search_btn.click(
            fn=search_papers_stream,
            inputs=[search_query_input, gr.State(value="å¿«é€Ÿæœç´¢"), gr.State(value=False)],
            outputs=[search_header, search_report_dummy, search_guide, search_arxiv, search_openalex, papers_state]
        )

        submit_search_event = search_query_input.submit(
            fn=search_papers_stream,
            inputs=[search_query_input, gr.State(value="å¿«é€Ÿæœç´¢"), gr.State(value=False)],
            outputs=[search_header, search_report_dummy, search_guide, search_arxiv, search_openalex, papers_state]
        )

        search_stop_btn.click(fn=None, inputs=None, outputs=None, cancels=[search_event, submit_search_event])

        # æ·±åº¦ç ”ç©¶Tab
        dr_guide_dummy = gr.State(value="")  # å ä½ï¼Œæ·±åº¦æ¨¡å¼ä¸éœ€è¦guide

        dr_event = dr_search_btn.click(
            fn=search_papers_stream,
            inputs=[dr_query_input, gr.State(value="æ·±åº¦ç ”ç©¶"), use_fulltext_checkbox],
            outputs=[dr_header, dr_report, dr_guide_dummy, dr_arxiv, dr_openalex, papers_state]
        )

        submit_dr_event = dr_query_input.submit(
            fn=search_papers_stream,
            inputs=[dr_query_input, gr.State(value="æ·±åº¦ç ”ç©¶"), use_fulltext_checkbox],
            outputs=[dr_header, dr_report, dr_guide_dummy, dr_arxiv, dr_openalex, papers_state]
        )

        dr_stop_btn.click(fn=None, inputs=None, outputs=None, cancels=[dr_event, submit_dr_event])

        # è®ºæ–‡åº“Tab - PDF ä¸Šä¼ åˆ†æ
        analyze_btn.click(
            fn=analyze_pdf,
            inputs=[pdf_upload],
            outputs=[pdf_result_header, pdf_abstract, pdf_fulltext, pdf_chunks_info]
        )

        # ä¾§è¾¹æ ï¼šæ›´æ–°è®ºæ–‡é€‰æ‹©å™¨ï¼ˆå½“æœç´¢å®Œæˆåï¼‰
        def update_paper_selector(papers_json: str):
            """æ›´æ–°è®ºæ–‡ä¸‹æ‹‰èœå•"""
            import json
            try:
                papers = json.loads(papers_json) if papers_json else []
                if not papers:
                    return gr.Dropdown(choices=[], value=None)

                choices = []
                for i, p in enumerate(papers, 1):
                    title = p.get('title', 'æœªçŸ¥æ ‡é¢˜')[:60]
                    choices.append((f"[{i}] {title}...", i-1))  # (æ˜¾ç¤ºæ–‡æœ¬, å€¼)

                return gr.Dropdown(choices=choices, value=None)
            except:
                return gr.Dropdown(choices=[], value=None)

        # å½“æœç´¢å®Œæˆæ—¶æ›´æ–°é€‰æ‹©å™¨
        papers_state.change(
            fn=update_paper_selector,
            inputs=[papers_state],
            outputs=[paper_selector]
        )

        # ä¾§è¾¹æ ï¼šå½“é€‰æ‹©è®ºæ–‡æ—¶æ˜¾ç¤ºè¯¦æƒ…
        def show_selected_paper(paper_index, papers_json):
            """æ˜¾ç¤ºé€‰ä¸­çš„è®ºæ–‡è¯¦æƒ…"""
            import json
            if paper_index is None or not papers_json:
                return "## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\nè¯·å…ˆæœç´¢è®ºæ–‡ï¼Œç„¶åä»ä¸Šæ–¹ä¸‹æ‹‰èœå•é€‰æ‹©"

            try:
                papers = json.loads(papers_json)
                if 0 <= paper_index < len(papers):
                    paper = papers[paper_index]
                    return show_paper_details(json.dumps(paper, ensure_ascii=False))
                else:
                    return "## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\nè®ºæ–‡ç´¢å¼•æ— æ•ˆ"
            except Exception as e:
                return f"## ğŸ“„ è®ºæ–‡è¯¦æƒ…\n\nè§£æå‡ºé”™: {str(e)}"

        paper_selector.change(
            fn=show_selected_paper,
            inputs=[paper_selector, papers_state],
            outputs=[sidebar_content]
        )

    return app


if __name__ == "__main__":
    app = create_app()
    app.launch(share=False)
