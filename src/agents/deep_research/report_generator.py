"""ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå™¨

å°†å¤šä¸ªå­é—®é¢˜çš„ç ”ç©¶ç»“æœæ•´åˆä¸ºä¸€ä»½å®Œæ•´çš„ç ”ç©¶æŠ¥å‘Šã€‚
å‚è€ƒ Open Deep Research çš„ final_report_generation è®¾è®¡ã€‚
"""
import json
import re
from typing import List, Optional
from dataclasses import dataclass
from datetime import datetime

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from utils.llm_client import QwenClient
from .decomposer import DecompositionResult
from .research_agent import ResearchResult


@dataclass
class ResearchReport:
    """ç ”ç©¶æŠ¥å‘Š"""
    title: str                  # æŠ¥å‘Šæ ‡é¢˜
    overview: str               # æ¦‚è¿°
    sections: List[dict]        # å„éƒ¨åˆ†å†…å®¹
    conclusion: str             # ç»¼åˆç»“è®º
    sources: List[dict]         # æ‰€æœ‰å¼•ç”¨æ¥æº
    metadata: dict              # å…ƒæ•°æ®
    evidences: List[dict] = None  # v0.4.0: å¥çº§è¯æ®åˆ—è¡¨

    def __post_init__(self):
        if self.evidences is None:
            self.evidences = []


class ReportGenerator:
    """
    ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå™¨

    å°†å¤šä¸ªå­é—®é¢˜çš„ç ”ç©¶ç»“æœæ•´åˆä¸ºä¸€ä»½ç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Šã€‚
    æ”¯æŒ Markdown æ ¼å¼è¾“å‡ºã€‚
    """

    REPORT_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯ç ”ç©¶æŠ¥å‘Šæ’°å†™ä¸“å®¶ã€‚æ ¹æ®ç ”ç©¶é—®é¢˜å’Œå„å­é—®é¢˜çš„ç ”ç©¶ç»“æœï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Šã€‚

åŸå§‹ç ”ç©¶é—®é¢˜ï¼š{original_query}
é—®é¢˜ç±»å‹ï¼š{query_type}
ç ”ç©¶ç­–ç•¥ï¼š{research_strategy}

å„å­é—®é¢˜ç ”ç©¶ç»“æœï¼š
{research_findings}

**å¯å¼•ç”¨çš„è®ºæ–‡åˆ—è¡¨**ï¼ˆè¯·åœ¨æ­£æ–‡ä¸­ä½¿ç”¨ [ç¼–å·] æ ¼å¼å¼•ç”¨ï¼‰ï¼š
{paper_list}

ä»»åŠ¡ï¼šç”Ÿæˆä¸€ä»½å®Œæ•´çš„ç ”ç©¶æŠ¥å‘Šï¼Œ**å¿…é¡»åœ¨æ­£æ–‡ä¸­æ ‡æ³¨å¼•ç”¨æ¥æº**ã€‚

è¾“å‡º JSON æ ¼å¼ï¼š
{{
  "title": "æŠ¥å‘Šæ ‡é¢˜ï¼ˆç®€æ´æœ‰åŠ›ï¼Œ10-20å­—ï¼‰",
  "overview": "ç ”ç©¶æ¦‚è¿°ï¼ŒåŒ…å«å¼•ç”¨æ ‡æ³¨å¦‚[1][2]ï¼ˆ100-150å­—ï¼‰",
  "sections": [
    {{
      "heading": "ç« èŠ‚æ ‡é¢˜",
      "content": "ç« èŠ‚å†…å®¹ï¼Œ**å¿…é¡»åŒ…å«å¼•ç”¨æ ‡æ³¨**å¦‚[1][3]ï¼ˆè¯¦ç»†åˆ†æï¼Œ100-200å­—ï¼‰",
      "key_findings": ["å‘ç°1 [å¼•ç”¨ç¼–å·]", "å‘ç°2 [å¼•ç”¨ç¼–å·]"]
    }}
  ],
  "conclusion": "ç»¼åˆç»“è®ºï¼ˆ100-150å­—ï¼Œå›ç­”åŸå§‹é—®é¢˜ï¼Œç»™å‡ºå»ºè®®æˆ–å±•æœ›ï¼‰",
  "key_takeaways": [
    "æ ¸å¿ƒè¦ç‚¹1ï¼ˆ20å­—å†…ï¼‰",
    "æ ¸å¿ƒè¦ç‚¹2ï¼ˆ20å­—å†…ï¼‰",
    "æ ¸å¿ƒè¦ç‚¹3ï¼ˆ20å­—å†…ï¼‰"
  ]
}}

**å¼•ç”¨è¦æ±‚**ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š
1. æ¯ä¸ªå…·ä½“è§‚ç‚¹/å‘ç°åé¢å¿…é¡»æ ‡æ³¨æ¥æºï¼Œå¦‚"Transformerçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿæ•è·é•¿ç¨‹ä¾èµ–[1][3]"
2. ä½¿ç”¨æ–¹æ‹¬å·+æ•°å­—æ ¼å¼ï¼Œå¦‚ [1]ã€[2]ã€[1][3]
3. åªå¼•ç”¨ä¸è¯¥è§‚ç‚¹ç›´æ¥ç›¸å…³çš„è®ºæ–‡ï¼Œä¸è¦æ»¥ç”¨å¼•ç”¨
4. å¦‚æœæŸä¸ªè§‚ç‚¹æ¥è‡ªå¤šç¯‡è®ºæ–‡ï¼Œå¯ä»¥æ ‡æ³¨å¤šä¸ªï¼Œå¦‚ [1][2][5]

æŠ¥å‘Šè¦æ±‚ï¼š
1. æ ‡é¢˜åº”è¯¥ç›´æ¥åæ˜ ç ”ç©¶ä¸»é¢˜
2. æ¦‚è¿°éœ€è¦è¯´æ˜ç ”ç©¶äº†ä»€ä¹ˆã€æ€ä¹ˆç ”ç©¶çš„ã€å‘ç°äº†ä»€ä¹ˆ
3. æ¯ä¸ª section å¯¹åº”ä¸€ä¸ªå­é—®é¢˜ï¼Œä½†æ ‡é¢˜åº”è¯¥æ˜¯å†…å®¹å¯¼å‘çš„
4. ç»“è®ºéœ€è¦ç»¼åˆæ‰€æœ‰å‘ç°ï¼Œç›´æ¥å›ç­”ç”¨æˆ·çš„åŸå§‹é—®é¢˜
5. **æ‰€æœ‰å…·ä½“å‘ç°å¿…é¡»æœ‰å¼•ç”¨æ”¯æ’‘**

ç‰¹æ®Šå¤„ç†ï¼š
- å¯¹æ¯”ç±»é—®é¢˜ï¼šsections åº”è¯¥åŒ…æ‹¬å„å¯¹è±¡åˆ†æ + å¯¹æ¯”æ€»ç»“
- ç»¼è¿°ç±»é—®é¢˜ï¼šsections åº”è¯¥æŒ‰ä¸»é¢˜æˆ–æ—¶é—´çº¿ç»„ç»‡
- è¶‹åŠ¿ç±»é—®é¢˜ï¼šsections åº”è¯¥åŒ…æ‹¬å†å²å›é¡¾ã€ç°çŠ¶åˆ†æã€æœªæ¥å±•æœ›"""

    def __init__(self, qwen_api_key: Optional[str] = None):
        self.llm_client = QwenClient(api_key=qwen_api_key) if qwen_api_key else None

    def generate(
        self,
        decomposition: DecompositionResult,
        research_results: List[ResearchResult]
    ) -> ResearchReport:
        """
        ç”Ÿæˆç ”ç©¶æŠ¥å‘Š

        Args:
            decomposition: é—®é¢˜åˆ†è§£ç»“æœ
            research_results: å„å­é—®é¢˜çš„ç ”ç©¶ç»“æœ

        Returns:
            ResearchReport: ç ”ç©¶æŠ¥å‘Š
        """
        if not research_results:
            return self._empty_report(decomposition.original_query)

        if self.llm_client:
            report = self._generate_with_llm(decomposition, research_results)
        else:
            report = self._generate_fallback(decomposition, research_results)

        return report

    def _format_research_findings(self, results: List[ResearchResult]) -> str:
        """æ ¼å¼åŒ–ç ”ç©¶ç»“æœç”¨äº prompt"""
        lines = []
        for i, result in enumerate(results, 1):
            lines.append(f"## å­é—®é¢˜ {i}: {result.sub_question}")
            lines.append(f"ç ”ç©¶ç›®çš„: {result.purpose}")
            lines.append(f"æ‰¾åˆ°è®ºæ–‡: {result.papers_found} ç¯‡")
            lines.append(f"\nç ”ç©¶å‘ç°:\n{result.compressed_findings}")

            if result.key_points:
                lines.append("\nå…³é”®è¦ç‚¹:")
                for point in result.key_points:
                    lines.append(f"- {point}")

            lines.append("\n" + "-" * 40 + "\n")

        return "\n".join(lines)

    def _collect_all_papers(self, results: List[ResearchResult]) -> List[dict]:
        """æ”¶é›†æ‰€æœ‰è®ºæ–‡å¹¶å»é‡"""
        seen_titles = set()
        all_papers = []

        for result in results:
            for src in result.sources:
                title = src.get("title", "").lower().strip()
                if title and title not in seen_titles:
                    seen_titles.add(title)
                    all_papers.append(src)

        # æŒ‰å¼•ç”¨æ•°æ’åºï¼Œé«˜å¼•ç”¨è®ºæ–‡ä¼˜å…ˆ
        all_papers.sort(
            key=lambda x: x.get("citation_count", 0) or 0,
            reverse=True
        )

        return all_papers

    def _collect_all_evidences(self, results: List[ResearchResult]) -> List[dict]:
        """æ”¶é›†æ‰€æœ‰å¥çº§è¯æ®ï¼ˆä»… FulltextResearchResult æœ‰ï¼‰"""
        all_evidences = []
        for result in results:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ FulltextResearchResultï¼ˆæœ‰ evidences å±æ€§ï¼‰
            if hasattr(result, 'evidences') and result.evidences:
                for evidence in result.evidences:
                    all_evidences.append({
                        "sentence": evidence.sentence,
                        "page": evidence.page,
                        "position": evidence.position,
                        "paper_title": evidence.paper_title,
                        "paper_index": evidence.paper_index,
                        "sub_question": result.sub_question
                    })
        return all_evidences

    def _format_paper_list(self, papers: List[dict]) -> str:
        """æ ¼å¼åŒ–è®ºæ–‡åˆ—è¡¨ï¼Œå¸¦ç¼–å·"""
        if not papers:
            return "ï¼ˆæ— å¯å¼•ç”¨çš„è®ºæ–‡ï¼‰"

        lines = []
        for i, p in enumerate(papers, 1):
            title = p.get("title", "æœªçŸ¥æ ‡é¢˜")
            year = p.get("year", "N/A")
            source = p.get("source", "unknown").upper()
            relevance = p.get("relevance", "")

            line = f"[{i}] {title} ({source}, {year})"
            if relevance:
                line += f" - {relevance}"
            lines.append(line)

        return "\n".join(lines)

    def _generate_with_llm(
        self,
        decomposition: DecompositionResult,
        research_results: List[ResearchResult]
    ) -> ResearchReport:
        """ä½¿ç”¨ LLM ç”ŸæˆæŠ¥å‘Š"""
        try:
            findings_text = self._format_research_findings(research_results)

            # æ”¶é›†æ‰€æœ‰è®ºæ–‡å¹¶ç¼–å·
            all_papers = self._collect_all_papers(research_results)
            paper_list_text = self._format_paper_list(all_papers)

            prompt = self.REPORT_PROMPT.format(
                original_query=decomposition.original_query,
                query_type=decomposition.query_type,
                research_strategy=decomposition.research_strategy,
                research_findings=findings_text,
                paper_list=paper_list_text
            )

            # ä½¿ç”¨é€šä¹‰åƒé—® plus æ¨¡å‹ï¼ˆæŠ¥å‘Šç”Ÿæˆæ˜¯å¤æ‚ä»»åŠ¡ï¼‰
            content = self.llm_client.chat(
                prompt=prompt,
                task_type="report",
                max_tokens=2000,
                temperature=0.3,
                timeout=20.0
            )
            parsed = self._parse_response(content)

            if parsed:
                return self._format_report(decomposition, research_results, parsed)

        except Exception as e:
            print(f"[ReportGenerator] LLM ç”Ÿæˆå‡ºé”™: {e}")

        return self._generate_fallback(decomposition, research_results)

    def _parse_response(self, content: str) -> Optional[dict]:
        """è§£æ LLM å“åº”"""
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            pass

        json_match = re.search(r'\{[\s\S]*\}', content)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass

        return None

    def _format_report(
        self,
        decomposition: DecompositionResult,
        research_results: List[ResearchResult],
        parsed: dict
    ) -> ResearchReport:
        """æ ¼å¼åŒ–æŠ¥å‘Š"""
        # ä½¿ç”¨ç»Ÿä¸€çš„è®ºæ–‡æ”¶é›†æ–¹æ³•ï¼Œä¿æŒç¼–å·ä¸€è‡´æ€§
        all_sources = self._collect_all_papers(research_results)

        # æ”¶é›†å¥çº§è¯æ®
        all_evidences = self._collect_all_evidences(research_results)

        # æ£€æŸ¥æ˜¯å¦æœ‰å…¨æ–‡ç ”ç©¶ç»“æœ
        has_fulltext = any(
            hasattr(r, 'papers_with_fulltext') and r.papers_with_fulltext > 0
            for r in research_results
        )

        return ResearchReport(
            title=parsed.get("title", decomposition.original_query),
            overview=parsed.get("overview", ""),
            sections=parsed.get("sections", []),
            conclusion=parsed.get("conclusion", ""),
            sources=all_sources,
            metadata={
                "original_query": decomposition.original_query,
                "query_type": decomposition.query_type,
                "research_strategy": decomposition.research_strategy,
                "sub_questions_count": len(decomposition.sub_questions),
                "total_papers": sum(r.papers_found for r in research_results),
                "generated_at": datetime.now().isoformat(),
                "key_takeaways": parsed.get("key_takeaways", []),
                "has_fulltext": has_fulltext,
                "evidences_count": len(all_evidences)
            },
            evidences=all_evidences
        )

    def _generate_fallback(
        self,
        decomposition: DecompositionResult,
        research_results: List[ResearchResult]
    ) -> ResearchReport:
        """å›é€€æ–¹æ¡ˆï¼šä»ç ”ç©¶ç»“æœä¸­æ•´åˆæŠ¥å‘Š"""
        print("[ReportGenerator] ä½¿ç”¨å›é€€æ–¹æ¡ˆç”ŸæˆæŠ¥å‘Š")

        sections = []
        for i, result in enumerate(research_results, 1):
            # ä½¿ç”¨æ›´å¥½çš„ç« èŠ‚æ ‡é¢˜
            heading = f"å­é—®é¢˜{i}: {result.sub_question[:40]}..."

            # å†…å®¹ä½¿ç”¨å‹ç¼©åçš„å‘ç°
            content = result.compressed_findings if result.compressed_findings else f"åŸºäº {result.papers_found} ç¯‡è®ºæ–‡çš„åˆ†æã€‚"

            # å…³é”®å‘ç°
            key_findings = result.key_points if result.key_points else []

            sections.append({
                "heading": heading,
                "content": content,
                "key_findings": key_findings
            })

        # æ”¶é›†æ‰€æœ‰æ¥æº
        all_sources = []
        for result in research_results:
            all_sources.extend(result.sources)

        # æ”¶é›†å¥çº§è¯æ®
        all_evidences = self._collect_all_evidences(research_results)

        # æ¦‚è¿°ï¼šåŒ…å«æ›´å¤šä¿¡æ¯
        total_papers = sum(r.papers_found for r in research_results)
        overview = f"æœ¬ç ”ç©¶å›´ç»•ã€Œ{decomposition.original_query}ã€å±•å¼€ï¼Œåˆ†è§£ä¸º {len(research_results)} ä¸ªå­é—®é¢˜è¿›è¡Œç ”ç©¶ï¼Œå…±æ£€ç´¢åˆ° {total_papers} ç¯‡ç›¸å…³è®ºæ–‡ã€‚"

        # å¦‚æœæœ‰ç ”ç©¶ç­–ç•¥ï¼ŒåŠ å…¥æ¦‚è¿°
        if decomposition.research_strategy:
            overview += f"\n\nç ”ç©¶ç­–ç•¥ï¼š{decomposition.research_strategy}"

        # ç»“è®ºï¼šæ•´åˆæ‰€æœ‰å…³é”®è¦ç‚¹
        all_points = []
        for result in research_results:
            # åªå–å‰2ä¸ªè¦ç‚¹ï¼Œé¿å…è¿‡é•¿
            for point in result.key_points[:2]:
                # è¿‡æ»¤æ‰åªæ˜¯æ ‡é¢˜çš„è¦ç‚¹
                if not point.endswith("...") or len(point) > 60:
                    all_points.append(point)

        if all_points:
            conclusion = "ç»¼åˆç ”ç©¶å‘ç°ï¼š\n\n" + "\n".join(f"â€¢ {p}" for p in all_points[:6])
        else:
            conclusion = f"æœ¬ç ”ç©¶å…±åˆ†æäº† {total_papers} ç¯‡ç›¸å…³è®ºæ–‡ï¼Œä¸ºã€Œ{decomposition.original_query}ã€æä¾›äº†åˆæ­¥å‚è€ƒã€‚"

        return ResearchReport(
            title=f"å…³äºã€Œ{decomposition.original_query}ã€çš„ç ”ç©¶æŠ¥å‘Š",
            overview=overview,
            sections=sections,
            conclusion=conclusion,
            sources=all_sources,
            metadata={
                "original_query": decomposition.original_query,
                "query_type": decomposition.query_type,
                "sub_questions_count": len(decomposition.sub_questions),
                "total_papers": total_papers,
                "generated_at": datetime.now().isoformat()
            },
            evidences=all_evidences
        )

    def _empty_report(self, query: str) -> ResearchReport:
        """ç©ºæŠ¥å‘Š"""
        return ResearchReport(
            title=f"å…³äºã€Œ{query}ã€çš„ç ”ç©¶æŠ¥å‘Š",
            overview="æœªèƒ½å®Œæˆç ”ç©¶ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            sections=[],
            conclusion="",
            sources=[],
            metadata={
                "original_query": query,
                "error": "No research results"
            },
            evidences=[]
        )

    def format_as_markdown(self, report: ResearchReport) -> str:
        """
        å°†æŠ¥å‘Šæ ¼å¼åŒ–ä¸º Markdown

        Args:
            report: ç ”ç©¶æŠ¥å‘Š

        Returns:
            str: Markdown æ ¼å¼çš„æŠ¥å‘Š
        """
        lines = []

        # æ ‡é¢˜
        lines.append(f"# {report.title}")
        lines.append("")

        # å…ƒæ•°æ®
        meta = report.metadata
        lines.append(f"> ç ”ç©¶ç±»å‹: {meta.get('query_type', 'N/A')} | "
                    f"å­é—®é¢˜: {meta.get('sub_questions_count', 0)} ä¸ª | "
                    f"è®ºæ–‡: {meta.get('total_papers', 0)} ç¯‡")
        lines.append("")

        # æ¦‚è¿°
        lines.append("## æ¦‚è¿°")
        lines.append("")
        lines.append(report.overview)
        lines.append("")

        # æ ¸å¿ƒè¦ç‚¹
        if meta.get("key_takeaways"):
            lines.append("### æ ¸å¿ƒè¦ç‚¹")
            lines.append("")
            for i, point in enumerate(meta["key_takeaways"], 1):
                lines.append(f"{i}. {point}")
            lines.append("")

        # å„ç« èŠ‚
        for i, section in enumerate(report.sections, 1):
            heading = section.get("heading", f"éƒ¨åˆ† {i}")
            lines.append(f"## {i}. {heading}")
            lines.append("")
            lines.append(section.get("content", ""))
            lines.append("")

            if section.get("key_findings"):
                lines.append("**å…³é”®å‘ç°ï¼š**")
                for finding in section["key_findings"]:
                    lines.append(f"- {finding}")
                lines.append("")

        # ç»“è®º
        lines.append("## ç»“è®º")
        lines.append("")
        lines.append(report.conclusion)
        lines.append("")

        # å¥çº§è¯æ®ï¼ˆå…¨æ–‡ç ”ç©¶æ¨¡å¼ï¼‰
        if report.evidences:
            lines.append("## ğŸ“Œ æ”¯æŒè¯æ®")
            lines.append("")
            lines.append("> *ä»¥ä¸‹æ˜¯ä»è®ºæ–‡åŸæ–‡ä¸­æ‘˜å½•çš„å…³é”®æ”¯æŒå¥ï¼Œå¯ç”¨äºéªŒè¯æŠ¥å‘Šä¸­çš„ç»“è®ºã€‚*")
            lines.append("")

            # æŒ‰è®ºæ–‡åˆ†ç»„æ˜¾ç¤º
            evidence_by_paper = {}
            for ev in report.evidences:
                paper_title = ev.get("paper_title", "æœªçŸ¥è®ºæ–‡")
                if paper_title not in evidence_by_paper:
                    evidence_by_paper[paper_title] = []
                evidence_by_paper[paper_title].append(ev)

            for paper_title, evidences in evidence_by_paper.items():
                lines.append(f"**{paper_title[:60]}...**")
                for ev in evidences[:3]:  # æ¯ç¯‡è®ºæ–‡æœ€å¤šæ˜¾ç¤º3æ¡
                    sentence = ev.get("sentence", "")[:200]
                    page = ev.get("page", "?")
                    lines.append(f"- ğŸ“„ *\"{sentence}...\"* (ç¬¬{page}é¡µ)")
                lines.append("")

        # å¼•ç”¨æ¥æº
        if report.sources:
            lines.append("## å‚è€ƒæ¥æº")
            lines.append("")
            for i, src in enumerate(report.sources[:10], 1):
                title = src.get("title", "æœªçŸ¥æ ‡é¢˜")
                year = src.get("year", "")
                url = src.get("url", "")
                source = src.get("source", "").upper()
                arxiv_id = src.get("arxiv_id", "")
                has_fulltext = src.get("has_fulltext", False)

                # æ ¼å¼: [ç¼–å·] **æ ‡é¢˜** (æ¥æº, å¹´ä»½) [é“¾æ¥]
                line = f"**{i}. {title}**"
                meta_parts = []
                if source:
                    meta_parts.append(source)
                if year:
                    meta_parts.append(str(year))
                if has_fulltext:
                    meta_parts.append("ğŸ“„å…¨æ–‡")
                if meta_parts:
                    line += f" ({', '.join(meta_parts)})"

                # arXiv é“¾æ¥
                if arxiv_id:
                    line += f" â†’ [arXiv](https://arxiv.org/abs/{arxiv_id})"
                elif url:
                    line += f" â†’ [æŸ¥çœ‹è®ºæ–‡]({url})"
                lines.append(line)
                lines.append("")
            lines.append("")

        return "\n".join(lines)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import os
    from dotenv import load_dotenv
    from .decomposer import SubQuestion

    load_dotenv()

    # æ¨¡æ‹Ÿæ•°æ®
    decomposition = DecompositionResult(
        original_query="å¯¹æ¯” Transformer å’Œ RNN åœ¨ NLP ä¸­çš„åº”ç”¨",
        query_type="comparison",
        sub_questions=[],
        research_strategy="åˆ†åˆ«ç ”ç©¶ä¸¤ç§æ¶æ„ï¼Œå†è¿›è¡Œç»¼åˆå¯¹æ¯”"
    )

    research_results = [
        ResearchResult(
            sub_question="Transformer æ¶æ„çš„æ ¸å¿ƒåŸç†å’Œåœ¨ NLP ä¸­çš„å…¸å‹åº”ç”¨",
            purpose="äº†è§£ Transformer åŸºç¡€",
            papers_found=10,
            compressed_findings="Transformer æ¶æ„åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—æ•°æ®ã€‚åœ¨ NLP é¢†åŸŸï¼Œå®ƒå·²æˆä¸ºä¸»æµæ¶æ„ï¼ŒBERTã€GPT ç­‰æ¨¡å‹éƒ½åŸºäºæ­¤ã€‚",
            key_points=["è‡ªæ³¨æ„åŠ›æœºåˆ¶", "å¹¶è¡Œè®¡ç®—", "é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼"],
            sources=[{"title": "Attention Is All You Need", "year": 2017, "url": "https://arxiv.org/abs/1706.03762", "source": "arxiv"}]
        ),
        ResearchResult(
            sub_question="RNN/LSTM æ¶æ„çš„æ ¸å¿ƒåŸç†å’Œåœ¨ NLP ä¸­çš„å…¸å‹åº”ç”¨",
            purpose="äº†è§£ RNN åŸºç¡€",
            papers_found=8,
            compressed_findings="RNN é€šè¿‡å¾ªç¯ç»“æ„å¤„ç†åºåˆ—æ•°æ®ï¼ŒLSTM è§£å†³äº†é•¿æœŸä¾èµ–é—®é¢˜ã€‚æ›¾æ˜¯ NLP çš„ä¸»æµé€‰æ‹©ï¼Œä½†è®¡ç®—æ•ˆç‡è¾ƒä½ã€‚",
            key_points=["å¾ªç¯ç»“æ„", "LSTMé—¨æ§æœºåˆ¶", "åºåˆ—å¤„ç†"],
            sources=[{"title": "Long Short-Term Memory", "year": 1997, "url": "", "source": "openalex"}]
        )
    ]

    generator = ReportGenerator(
        deepseek_api_key=os.getenv("DEEPSEEK_API_KEY")
    )

    print("=" * 60)
    print("æŠ¥å‘Šç”Ÿæˆå™¨æµ‹è¯•")
    print("=" * 60)

    report = generator.generate(decomposition, research_results)

    print("\nç”Ÿæˆçš„ Markdown æŠ¥å‘Š:\n")
    print(generator.format_as_markdown(report))
