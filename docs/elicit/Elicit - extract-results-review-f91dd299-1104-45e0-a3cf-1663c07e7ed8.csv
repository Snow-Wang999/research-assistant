Title,Authors,DOI,DOI link,Venue,Citation count,Year,Technical Architecture,Implementation Methods,Paper Discovery Techniques,Technical Challenges,Innovative Solutions,Performance Results,Data Sources,Application Context,"Supporting quotes for ""Technical Architecture""","Supporting tables for ""Technical Architecture""","Reasoning for ""Technical Architecture""","Supporting quotes for ""Implementation Methods""","Supporting tables for ""Implementation Methods""","Reasoning for ""Implementation Methods""","Supporting quotes for ""Paper Discovery Techniques""","Supporting tables for ""Paper Discovery Techniques""","Reasoning for ""Paper Discovery Techniques""","Supporting quotes for ""Technical Challenges""","Supporting tables for ""Technical Challenges""","Reasoning for ""Technical Challenges""","Supporting quotes for ""Innovative Solutions""","Supporting tables for ""Innovative Solutions""","Reasoning for ""Innovative Solutions""","Supporting quotes for ""Performance Results""","Supporting tables for ""Performance Results""","Reasoning for ""Performance Results""","Supporting quotes for ""Data Sources""","Supporting tables for ""Data Sources""","Reasoning for ""Data Sources""","Supporting quotes for ""Application Context""","Supporting tables for ""Application Context""","Reasoning for ""Application Context"""
Application of AI in the Field of Documentary Heritage: A Review of the Literature,Yaohan Lu,10.70891/jair.2024.110005,https://doi.org/10.70891/jair.2024.110005,Journal of Artificial Intelligence Research,1,2024,"- Main AI/ML frameworks used: Deep learning, natural language processing
- System architecture components: Integration with Web of Science Core Collection
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Web of Science Core Collection
- Overall system design approach: Use of VOSviewer for data analysis","- Algorithms and models used: Natural language processing tools, machine learning models (deep learning, graph learning), neural networks
- Feature extraction techniques: Binarisation for image processing, deep learning for feature extraction
- Data processing pipelines: Use of OCR systems for inferring internal sequences of ancient texts
- Training methodologies: Training on existing and self-generated data sets
- Preprocessing steps: Not explicitly mentioned
- Technical workflow or methodology: Focus on reading and extracting content from documents using AI technologies","- Search and retrieval algorithms: Search query in Web of Science Core Collection
- Relevance ranking approaches: Citation analysis
- Citation analysis methods: Frequency of citations
- Content-based matching techniques: Keyword analysis, literature coupling analysis
- Metadata extraction and utilization: Implied through keyword and citation analysis
- Recommendation system approaches: Not explicitly mentioned
- Expert/authority identification methods: Lotka's law and Price's law","- Integration difficulties: Collaboration required among GLAM institutions.
- Data quality or availability problems: Lack of research on physical document entities.
- Scalability challenges: Need for further investigation into digital document recovery and presentation.
- Evaluation methodology limitations: Field lacks a stable core group of authors; research depth needs strengthening.
- Computational resource constraints: Diversified development directions may imply resource challenges.","- New algorithmic contributions: Use of natural language processing tools (Pennacchiotti), prediction and translation systems (Chadha), geographical named entity recognition tools (Kogkitsidou).
- Creative problem-solving methods: Image processing systems for text extraction (Chamchong), deep learning models for character identification (Narang and Sonika Rani).
- Hybrid or ensemble approaches: Integration of ancient Chinese medicine books and clinical data into a large language model (Lingdan model), generalized multiagent hypergraph-learning frameworks (Shuo Yu).
- Novel feature engineering techniques: Insertion of pseudo-labels into backbone networks (X. R. Fu), use of OCR systems to infer internal sequences (H. Y. Ma).
- Innovative evaluation metrics: Accuracy rates achieved by models (e.g., 93.73% for Sanskrit manuscript identification).
- Technical workarounds for known problems: Use of binarisation techniques for image processing (Chamchong), deep learning models for author identification (Akram Bennour).
- Original system design elements: Expansion of ontologies for digitally modeling ancient book catalogs (L. X. Wang), integration of machine learning for ancient character recognition (T. Sommerschield).","- Accuracy: 93.73% for identifying 33 types of characters in Sanskrit manuscripts.
- Comparison with baseline methods: Deep learning model found to be superior to the state of the art in identifying authors of historical documents.
- Efficiency and accuracy enhancement: Enhanced efficiency and accuracy in the study of ancient Greek inscriptions.","- Academic databases used: Web of Science
- Dataset sizes and characteristics: 117 search results including articles, review articles, early accesses, and other types of articles
- Data collection methods: Keyword search in the Web of Science Core Collection
- Training/validation/test data splits: Not applicable
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: Literature, archaeology, medicine, sociology, and other fields related to cultural heritage.
- Specific literature review tasks addressed: Reading and processing the content of documents, document analysis.
- Types of academic documents processed: Historical documents, ancient texts, manuscripts.
- User types and requirements: Researchers, scholars, GLAM institutions; requirements include collaboration and integration with research workflows.
- Integration with research workflows: Use of AI for document analysis and processing; potential integration with digital narration and emotional design.
- Commercial vs. academic applications: Primarily academic applications in cultural heritage and research.","  - ""Artificial intelligence (AI) uses data, algorithms, and other technologies to gain knowledge [14].""
  - ""In recent years, a plethora of many machine learning models [21] such as deep learning and graph learning models have been developed [53], which provide crucial technical support for artificial intelligence in the identification of documentary heritage.""
  - ""The blue section of the figure primarily comprises five keywords, including ""deep learning"", which represents a set of techniques for digital text processing by computers.""
  - ""Deep learning is capable of dealing with complex geometric properties [25], and it is able to automatically extract features from raw data without any prior knowledge [17].""
  - ""Narang and Sonika Rani used a deep learning model as a feature extractor and classifier to identify 33 types of characters in Sanskrit manuscripts, with an accuracy rate of 93.73% [34].""
  - ""Akram Bennour developed a deep learning model and, through experimentation, found it to be superior to the state of the art in accurately identifying authors of historical documents.""
  - ""The artificial intelligence technology used in the article reflects the characteristics of The Times and is becoming more and more diverse.""
  - ""The first category is image recognition and processing technology, such as P. Romulus using optical characters to recognize ancient Batak characters [44], H. Y. Ma used the OCR system to infer the internal sequences of ancient texts [33], and Shuo Yu proposed a generalized multiagent hypergraph-learning framework [57].""
  - ""The second category is deep learning technology.""
  - ""The dataset employed in this study, derived from the Web of Science Core Collection, encompassed all editions.""
  - ""In this study, data were retrieved from the Web of Science in order to form a dataset, which was then analyzed with the assistance of VOSviewer in order to study the characteristics of the author, key words, literature citations, coupling network, and so forth.""",,"- The paper discusses the use of artificial intelligence (AI) and machine learning (ML) frameworks, particularly deep learning, for the identification and analysis of documentary heritage.
- Deep learning is highlighted as a key technology for digital text processing and feature extraction.
- The paper mentions the use of optical character recognition (OCR) systems and image processing technologies, which are part of the technical architecture.
- The dataset for the study was retrieved from the Web of Science Core Collection, indicating integration with existing academic databases.
- VOSviewer was used for analysis, suggesting a tool for visualizing and analyzing data, but no specific system architecture components like databases or APIs are mentioned.
- The paper does not provide details on technical infrastructure such as cloud platforms or computational resources.","  - ""Artificial intelligence (AI) uses data, algorithms, and other technologies to gain knowledge [14].""
  - ""M. Pennacchiotti employed natural language processing tools to examine ancient Italian texts and assess their utility, as well as to evaluate the potential for enhancing the performance of natural language processing tools through a series of tests and training iterations [39].""
  - ""S. Chadha used artificial intelligence to train on existing and self-generated data sets, thereby developing a prediction and translation system that exhibited greater accuracy than that of traditional translation software [11].""
  - ""E. Kogkitsidou applied five geographical named entity recognition tools to build maps displaying the locations mentioned in ancient texts [30].""
  - ""L. X. Wang expanded the CABC ontology to digitally model the catalog of ancient Chinese books [50].""
  - ""In recent years, a plethora of many machine learning models [21] such as deep learning and graph learning models have been developed [53], which provide crucial technical support for artificial intelligence in the identification of documentary heritage.""
  - ""deep learning is often used to read ancient scripts, such as Greek inscriptions [3], ancient Yi [15], ancient Aegean scripts [18], Arabic [2], etc.""
  - ""R. Chamchong attempted to develop an image processing system for the purpose of extracting text and characters from manuscripts through the utilization of binarisation techniques [13].""
  - ""Narang and Sonika Rani used a deep learning model as a feature extractor and classifier to identify 33 types of characters in Sanskrit manuscripts, with an accuracy rate of 93.73% [34].""
  - ""Akram Bennour developed a deep learning model and, through experimentation, found it to be superior to the state of the art in accurately identifying authors of historical documents.""
  - ""The application of artificial intelligence to the research of documentary heritage is mostly used to read the text or content of documents.""
  - ""ChatGPT is used to read literature and extract information to facilitate the classification and sorting of literature [32].""
  - ""Designing a multi-task model aimed at deciphering the terminology of etiquette and customs in Chinese ancient books to provide a basis for the extraction of etiquette information in ancient books and the automatic construction of a knowledge base [46].""
  - ""Lingdan model is based on large language model, extensively integrates ancient Chinese medicine books and clinical data, and makes the model prescribe medicine according to electronic medical records through training [27].""
  - ""Identify ancient inscriptions and predict damaged characters on artifacts to generate restored images [22].""
  - ""The artificial intelligence technology used in the article reflects the characteristics of The Times and is becoming more and more diverse.""
  - ""The first category is image recognition and processing technology, such as P. Romulus using optical characters to recognize ancient Batak characters [44], H. Y. Ma used the OCR system to infer the internal sequences of ancient texts [33], and Shuo Yu proposed a generalized multiagent hypergraph-learning framework [57].""
  - ""The second category is deep learning technology. Deep learning is capable of dealing with complex geometric properties [25], and it is able to automatically extract features from raw data without any prior knowledge [17].""
  - ""The research of artificial intelligence in the field of documentary heritage can be characterized by the following features:

(1) In the study of documentary heritage, scholars have paid attention to the universality of documents.""",,"- The paper discusses various technical implementation methods used in the field of documentary heritage protection, focusing on artificial intelligence (AI) technologies.
- Natural language processing (NLP) tools are used to examine ancient texts and enhance their utility, as seen in M. Pennacchiotti's work.
- Machine learning models, including deep learning and graph learning, are developed for identifying documentary heritage, indicating a focus on feature extraction and classification.
- Deep learning models are specifically used for reading ancient scripts and identifying characters in manuscripts, such as in Narang and Sonika Rani's work.
- Image processing techniques like binarisation are used for extracting text and characters from manuscripts, as demonstrated by R. Chamchong.
- The use of large language models like ChatGPT for reading literature and extracting information is mentioned.
- The paper highlights the diversity of AI technologies used, including image recognition and processing, and deep learning for dealing with complex geometric properties.
- The application of AI is primarily for reading and extracting content from documents, with a focus on universality across different document carriers and civilizations.","  - ""In this study, data were retrieved from the Web of Science in order to form a dataset, which was then analyzed with the assistance of VOSviewer in order to study the characteristics of the author, key words, literature citations, coupling network, and so forth.""
  - ""The dataset employed in this study, derived from the Web of Science Core Collection, encompassed all editions. Take ""Documentary Heritage OR Ancient text* OR Ancient manuscript* OR Ancient book* OR Ancient writing*"" as the topic, and ""AI OR Artificial intelligence OR Large language model OR Image processing OR Deep learning OR Machine learning OR Natural language processing"" was searched in the full text.""
  - ""The number of publications on the same topic in different years usually reflects the characteristics of the era of research on the topic.""
  - ""The key words are the refining of the core content and research direction of the article.""
  - ""By establishing a threshold, the keywords are classified into three categories, represented by red, blue, and green.""
  - ""The lines between the nodes show that the words are related.""
  - ""In the context of academic research, the term ""citation analysis of literature"" is used to describe the process of determining the frequency with which an article is referenced by other academic works.""
  - ""The larger nodes in the citation analysis diagram indicate a greater frequency of citations of the article in question.""
  - ""In the context of academic literature, the term ""literature coupling analysis"" is used to describe the process of identifying common references cited by two or more articles.""
  - ""A greater coupling strength indicates a higher degree of reference duplication between two articles.""
  - ""The literature with high coupling degree is roughly divided into 5 categories.""
  - ""The most prolific author is T. Sommerschield, who has published four articles on this topic between 2019 and 2023, which have been cited 83 times.""
  - ""Furthermore, he is the most cited author in this field.""
  - ""In bibliometrics, Lotka's law and Price's law are frequently employed to ascertain the nucleus of authors within a given field and to delineate the attributes of the research community therein""
  - ""Price's law reveals that a researcher can be repudiated as a core author in the field, mainly by comparing the number of publications with the authors who published the most.""
  - ""The minimum number of publications of the core authors of research utilising artificial intelligence technology in the field of document protection is 1.498.""
  - ""Authors with two or more publications may be considered core authors in this field.""","  - ""(Page 1, Table 1) ""
  - ""(Page 3, Table 1) | No. | Country                   | Documents | Citations |\n|-----|---------------------------|-----------|-----------|\n| 1   | People’s Republic of China| 39        | 101       |""
  - ""(Page 4, Table 1) | Number of publications | Number of authors |\n|------------------------|-------------------|\n| 1                      | 396               |""","- The paper uses VOSviewer to analyze the dataset retrieved from the Web of Science, which suggests a content-based matching technique by examining keywords and their relationships.
- The search query used to retrieve data from the Web of Science Core Collection indicates a search and retrieval algorithm based on specific topics and keywords.
- Citation analysis is used to determine the frequency of citations, which is a relevance ranking approach.
- Literature coupling analysis is employed to identify common references, which is a content-based matching technique.
- The use of Lotka's law and Price's law to identify core authors suggests an expert/authority identification method.
- The paper does not explicitly mention search and retrieval algorithms, relevance ranking approaches, or recommendation system approaches beyond what is implied by the use of VOSviewer and citation analysis.
- Metadata extraction and utilization are implied through the analysis of keywords and citations but are not explicitly detailed.","  - ""The protection of documentary heritage requires the collaboration of art galleries, libraries, archives, museums (GLAM) and other institutions.""
  - ""the majority of articles concentrate on the implementation of artificial intelligence in digital formats, and there is a paucity of research concerning the protection and utilization of document entities.""
  - ""It is certain that the application of AI in the field of document protection will definitely develop in a diversified direction.""
  - ""the publication rule of artificial intelligence application in the field of document protection does not conform to Lotka's law.""
  - ""the field has not yet established a stable core group of authors.""
  - ""the depth of research on the application of artificial intelligence in the field of document protection requires strengthening.""",,"- The paper discusses the need for collaboration among various institutions (GLAM) for document protection, which implies integration difficulties.
- There is a lack of research on the protection and utilization of physical document entities, suggesting data quality or availability problems.
- The paper mentions the vulnerability of physical document carriers to environmental factors, which could be considered a technical challenge in terms of preservation.
- The need for further investigation into digital document recovery and presentation suggests algorithm performance issues and scalability challenges.
- The paper notes that the field lacks a stable core group of authors and requires strengthening in research depth, indicating evaluation methodology limitations.
- The mention of diversified development directions implies potential computational resource constraints and technical bottlenecks.","  - ""M. Pennacchiotti employed natural language processing tools to examine ancient Italian texts and assess their utility, as well as to evaluate the potential for enhancing the performance of natural language processing tools through a series of tests and training iterations [39].""
  - ""S. Chadha used artificial intelligence to train on existing and self-generated data sets, thereby developing a prediction and translation system that exhibited greater accuracy than that of traditional translation software [11].""
  - ""E. Kogkitsidou applied five geographical named entity recognition tools to build maps displaying the locations mentioned in ancient texts [30].""
  - ""L. X. Wang expanded the CABC ontology to digitally model the catalog of ancient Chinese books [50].""
  - ""R. Chamchong attempted to develop an image processing system for the purpose of extracting text and characters from manuscripts through the utilization of binarisation techniques [13].""
  - ""Narang and Sonika Rani used a deep learning model as a feature extractor and classifier to identify 33 types of characters in Sanskrit manuscripts, with an accuracy rate of 93.73% [34].""
  - ""Akram Bennour developed a deep learning model and, through experimentation, found it to be superior to the state of the art in accurately identifying authors of historical documents.""
  - ""Shuo Yu proposed a generalized multiagent hypergraph-learning framework [57].""
  - ""X. R. Fu inserted a pseudo-label into the backbone network to predict inscriptions and identify ancient texts [24].""
  - ""Lingdan model is based on large language model, extensively integrates ancient Chinese medicine books and clinical data, and makes the model prescribe medicine according to electronic medical records through training [27].""
  - ""Identify ancient inscriptions and predict damaged characters on artifacts to generate restored images [22].""
  - ""P. Romulus using optical characters to recognize ancient Batak characters [44],""
  - ""H. Y. Ma used the OCR system to infer the internal sequences of ancient texts [33],""
  - ""T. Sommerschield has studied the literature for nearly 20 years to summarize and demonstrate the feasibility of machine learning in the recognition of ancient characters [48].""",,"- The paper discusses various innovative solutions in the context of applying artificial intelligence to documentary heritage protection. These include new algorithmic contributions such as the use of natural language processing tools for examining ancient texts (Pennacchiotti), developing prediction and translation systems (Chadha), and applying geographical named entity recognition tools (Kogkitsidou).
- Creative problem-solving methods are evident in the development of image processing systems for extracting text from manuscripts (Chamchong) and the use of deep learning models for character identification (Narang and Sonika Rani).
- Hybrid or ensemble approaches are represented by the integration of ancient Chinese medicine books and clinical data into a large language model (Lingdan model) and the use of generalized multiagent hypergraph-learning frameworks (Shuo Yu).
- Novel feature engineering techniques include the insertion of pseudo-labels into backbone networks for predicting inscriptions (X. R. Fu) and the use of OCR systems to infer internal sequences of ancient texts (H. Y. Ma).
- Innovative evaluation metrics are not explicitly mentioned, but the accuracy rates achieved by some models (e.g., 93.73% for Sanskrit manuscript identification) suggest innovative evaluation methods.
- Technical workarounds for known problems include the use of binarisation techniques for image processing (Chamchong) and the development of deep learning models for author identification (Akram Bennour).
- Original system design elements are evident in the expansion of ontologies for digitally modeling ancient book catalogs (L. X. Wang) and the integration of machine learning for ancient character recognition (T. Sommerschield).","  - ""Narang and Sonika Rani used a deep learning model as a feature extractor and classifier to identify 33 types of characters in Sanskrit manuscripts, with an accuracy rate of 93.73%""
  - ""Akram Bennour developed a deep learning model and, through experimentation, found it to be superior to the state of the art in accurately identifying authors of historical documents.""
  - ""The larger nodes in the citation analysis diagram indicate a greater frequency of citations of the article in question.""
  - ""The 2022 paper by Y. Assael on the use of deep neural networks for the restoration of ancient books and the determination of their attribution attracted greater attention due to its publication in the journal Nature.""
  - ""This paper introduces Ithaca, a neural network for the restoration of ancient Greek inscriptions, which has enhanced the efficiency and accuracy of the study of ancient Greek inscriptions by historians.""
  - ""The research of artificial intelligence in the field of documentary heritage can be characterized by the following features:

(1) In the study of documentary heritage, scholars have paid attention to the universality of documents.""
  - ""The application of artificial intelligence to the research of documentary heritage is mostly used to read the text or content of documents.""
  - ""The artificial intelligence technology used in the article reflects the characteristics of The Times and is becoming more and more diverse.""
  - ""the majority of articles concentrate on the implementation of artificial intelligence in digital formats, and there is a paucity of research concerning the protection and utilization of document entities.""
  - ""It is certain that the application of AI in the field of document protection will definitely develop in a diversified direction.""","  - ""(Page 3, Table 1) | No. | Country                   | Documents | Citations |\n|-----|---------------------------|-----------|-----------|\n| 1   | People’s Republic of China| 39        | 101       |""
  - ""(Page 4, Table 1) | Number of publications | Number of authors |\n|------------------------|-------------------|\n| 1                      | 396               |""","- The paper provides a specific quantitative performance outcome related to accuracy: ""Narang and Sonika Rani used a deep learning model as a feature extractor and classifier to identify 33 types of characters in Sanskrit manuscripts, with an accuracy rate of 93.73%.""
- There is a qualitative comparison with baseline methods: ""Akram Bennour developed a deep learning model and, through experimentation, found it to be superior to the state of the art in accurately identifying authors of historical documents.""
- The paper mentions the efficiency and accuracy enhancement in the study of ancient Greek inscriptions by historians, indicating a qualitative performance outcome.
- The tables at the end of the paper provide statistics on publications and authors, but they do not directly relate to performance results in terms of accuracy, precision, recall, F1-scores, processing speed, or user satisfaction.
- The paper does not provide specific metrics for processing speed and efficiency, user satisfaction, system reliability, or scalability test results.","  - ""This paper presents a systematic review of the literature on the use of artificial intelligence technology in the core database of Web of Science for the protection of documentary heritage.""
  - ""In this study, data were retrieved from the Web of Science in order to form a dataset, which was then analyzed with the assistance of VOSviewer in order to study the characteristics of the author, key words, literature citations, coupling network, and so forth.""
  - ""The dataset employed in this study, derived from the Web of Science Core Collection, encompassed all editions.""
  - ""Take ""Documentary Heritage OR Ancient text* OR Ancient manuscript* OR Ancient book* OR Ancient writing*"" as the topic, and ""AI OR Artificial intelligence OR Large language model OR Image processing OR Deep learning OR Machine learning OR Natural language processing"" was searched in the full text.""
  - ""The 117 search results comprise all articles, review articles, early accesses and other types of articles from the database, dated on or before 31 October 2024.""
  - ""The number of publications on the same topic in different years usually reflects the characteristics of the era of research on the topic.""",,"- The paper uses the Web of Science as the primary academic database for data collection. This is explicitly mentioned as the source of the dataset used in the study.
- The dataset consists of 117 search results, which include articles, review articles, early accesses, and other types of articles. This indicates the dataset size and its comprehensive nature in terms of document types.
- The data collection method involved searching specific keywords related to documentary heritage and artificial intelligence within the Web of Science Core Collection. The search terms included ""Documentary Heritage OR Ancient text* OR Ancient manuscript* OR Ancient book* OR Ancient writing*"" and ""AI OR Artificial intelligence OR Large language model OR Image processing OR Deep learning OR Machine learning OR Natural language processing.""
- There is no mention of specific data preprocessing and cleaning approaches, nor are there details about training/validation/test data splits, as the study is a literature review rather than an empirical study involving machine learning models.
- The study does not mention the use of external knowledge bases or ontologies.","  - ""The advent of artificial intelligence has precipitated a transformation in the manner in which documentary heritage is researched, safeguarded and utilized.""
  - ""This paper presents a systematic review of the literature on the use of artificial intelligence technology in the core database of Web of Science for the protection of documentary heritage.""
  - ""The protection of documentary heritage requires the collaboration of art galleries, libraries, archives, museums (GLAM) and other institutions.""
  - ""The dataset employed in this study, derived from the Web of Science Core Collection, encompassed all editions.""
  - ""The main research content of artificial intelligence in the field of document protection and utilization can be summarized through the keywords of the research literature.""
  - ""The application of artificial intelligence to the research of documentary heritage is mostly used to read the text or content of documents.""
  - ""These studies relate to literature, archaeology, medicine, sociology, and many other fields, and they underscore the significance of leveraging artificial intelligence to facilitate research on the preservation of documentary heritage.""
  - ""The artificial intelligence technology used in the article reflects the characteristics of The Times and is becoming more and more diverse.""
  - ""It is certain that the application of AI in the field of document protection will definitely develop in a diversified direction.""","  - ""(Page 3, Table 1) | No. | Country                   | Documents | Citations |\n|-----|---------------------------|-----------|-----------|\n| 1   | People’s Republic of China| 39        | 101       |""
  - ""(Page 4, Table 1) | Number of publications | Number of authors |\n|------------------------|-------------------|\n| 1                      | 396               |""","- The paper discusses the application of AI in the field of documentary heritage, which includes research, safeguarding, and utilization. This indicates a broad application context across various disciplines related to cultural heritage.
- The systematic review is based on literature from the Web of Science Core Collection, suggesting a focus on academic documents and research workflows.
- The paper mentions the collaboration of GLAM institutions, indicating a focus on academic and cultural heritage applications rather than commercial ones.
- The use of AI is primarily for reading and processing the content of documents, which aligns with tasks such as literature review and document analysis.
- The paper highlights the diversity of AI technologies used, including image recognition, deep learning, and natural language processing, which are applied across various fields like literature, archaeology, and medicine.
- The tables at the end of the paper provide statistics on publications and authors, which can be related to user types and requirements in terms of academic research and collaboration.
- The paper suggests future directions in digital narration and emotional design, indicating potential integration with research workflows and user requirements for engaging with documentary heritage."
"AI‐Augmented Search for Systematic Reviews: 
A Comparative Analysis","Valerie Vera, Vedant Khandelwal, Kaushik Roy, Ritvik Garimella, Harshul Surana, Amit P. Sheth",10.1002/pra2.1290,https://doi.org/10.1002/pra2.1290,Proceedings of the Association for Information Science and Technology,0,2025,"- Main AI/ML frameworks used: Neurosymbolic AI framework (NeuroLit Navigator), Generative language models (Scite, Consensus, Perplexity)
- System architecture components: Not mentioned
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Not mentioned
- Overall system design approach: Human-in-the-loop system","- Algorithms and models used: Neurosymbolic AI framework (NeuroLit Navigator), generative language models (Scite, Consensus, Perplexity)
- Feature extraction techniques: Not mentioned
- Data processing pipelines: Not mentioned
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Iterative human-in-the-loop approach","- Search and retrieval algorithms: NeuroLit Navigator uses a neurosymbolic AI framework; Scite, Consensus, and Perplexity use generative language models.
- Relevance ranking approaches: NeuroLit Navigator produces more precise and focused search strategies aligned with domain-specific terminologies.
- Expert/authority identification methods: Human-AI collaboration suggests the involvement of librarian expertise.","- Algorithm performance issues: AI-generated search strategies contain errors that undermine the reliability of the evidence.
- Evaluation methodology limitations: Commercial LLM-based systems lack reproducibility and interpretability.","- New algorithmic contributions: Neurosymbolic AI framework
- Creative problem-solving methods: Iterative human-in-the-loop approach
- Hybrid or ensemble approaches: Human-in-the-loop system with neurosymbolic AI framework
- Novel feature engineering techniques: Domain-specific terminologies integration
- Innovative system design elements: Human-AI collaboration","- Precision: NeuroLit Navigator produced more precise and focused search strategies.
- Comparison with baseline methods: NeuroLit Navigator was compared to commercial LLM-based systems (Scite, Consensus, and Perplexity), which had issues with reproducibility and interpretability.
- System reliability and robustness measures: NeuroLit Navigator had better reproducibility and interpretability compared to other systems.","Not mentioned (the abstract does not provide information on data sources, datasets, or data processing methods)","- Target research disciplines or fields: Health sciences, social sciences, and other fields where systematic reviews are common.
- Specific literature review tasks addressed: Automation of systematic review workflows.
- Types of academic documents processed: Systematic reviews.
- User types and requirements: Researchers and librarians requiring precise and focused search strategies.
- Integration with research workflows: Automation of systematic review workflows.
- Commercial vs. academic applications: Academic context implied by focus on systematic reviews and librarian expertise.","  - ""a human‐in‐the‐loop system with a neurosymbolic AI framework (i.e., NeuroLit Navigator)""
  - ""NeuroLit Navigator produced more precise and focused search strategies aligned with domain‐specific terminologies""
  - ""Our findings contribute to the growing field of human‐centered AI by providing a model for designing AI systems in information‐intensive domains.""
  - ""suggesting that AI should augment, rather than replace, librarian expertise.""
  - ""three AI systems that primarily rely on generative language models: Scite, Consensus, and Perplexity.""",,"- The abstract mentions a ""neurosymbolic AI framework"" as part of the NeuroLit Navigator system, indicating the use of a specific AI/ML framework that combines symbolic and neural network approaches.
- The mention of ""generative language models"" in Scite, Consensus, and Perplexity suggests the use of NLP techniques in these systems.
- The abstract does not provide specific details about system architecture components such as databases, APIs, or interfaces, nor does it mention technical infrastructure like cloud platforms or computational resources.
- There is no explicit mention of integration with existing academic databases or platforms.
- The overall system design approach is described as a ""human-in-the-loop"" system, emphasizing collaboration between AI and human expertise.","  - ""three AI systems that primarily rely on generative language models: Scite, Consensus, and Perplexity.""
  - ""a human‐in‐the‐loop system with a neurosymbolic AI framework (i.e., NeuroLit Navigator)""
  - ""providing a model for designing AI systems in information‐intensive domains.""
  - ""AI should augment, rather than replace, librarian expertise.""
  - ""an iterative human‐in‐the‐loop approach,""",,"- The abstract mentions the use of a ""neurosymbolic AI framework"" in the NeuroLit Navigator system, which suggests the use of a specific type of AI model that combines neural networks with symbolic reasoning.
- The mention of ""generative language models"" in Scite, Consensus, and Perplexity indicates the use of these models for search strategies.
- The ""iterative human‐in‐the‐loop approach"" implies a methodology where human input is integrated into the AI system's workflow, possibly for refining search strategies or ensuring accuracy.
- The abstract does not provide specific details on algorithms, feature extraction techniques, data processing pipelines, training methodologies, or preprocessing steps.
- The focus is on the comparative analysis and the potential of human-AI collaboration rather than detailed technical implementation methods.","  - ""To evaluate the relevancy and reproducibility of automated search strategies, we conducted a comparative analysis between a human‐in‐the‐loop system with a neurosymbolic AI framework (i.e., NeuroLit Navigator) and three AI systems that primarily rely on generative language models: Scite, Consensus, and Perplexity.""
  - ""Results showed that through an iterative human‐in‐the‐loop approach, NeuroLit Navigator produced more precise and focused search strategies aligned with domain‐specific terminologies compared to commercial LLM‐based systems, which presented issues such as lack of reproducibility and interpretability.""
  - ""This study highlights the potential of human‐AI collaboration in systematic review workflows, suggesting that AI should augment, rather than replace, librarian expertise.""",,"- The abstract mentions a comparative analysis between a human-in-the-loop system (NeuroLit Navigator) and three AI systems (Scite, Consensus, and Perplexity) that primarily rely on generative language models. This suggests that the paper involves search and retrieval algorithms, as these systems are compared for their effectiveness in finding relevant papers.
- The mention of ""neurosymbolic AI framework"" indicates a specific type of search and retrieval algorithm used by NeuroLit Navigator, which is likely a content-based matching technique.
- The abstract highlights the precision and focus of NeuroLit Navigator's search strategies, which implies a relevance ranking approach that aligns with domain-specific terminologies.
- The study's focus on human-AI collaboration suggests that expert/authority identification methods might be involved, as it emphasizes the role of librarian expertise in augmenting AI capabilities.
- The abstract does not explicitly mention citation analysis methods, metadata extraction and utilization, or recommendation system approaches, but these might be implied in the broader context of systematic review workflows.","  - ""AI‐generated search strategies often contain errors that can significantly undermine the reliability of the evidence.""
  - ""Results showed that through an iterative human‐in‐the‐loop approach, NeuroLit Navigator produced more precise and focused search strategies aligned with domain‐specific terminologies compared to commercial LLM‐based systems, which presented issues such as lack of reproducibility and interpretability.""",,"- The abstract mentions that AI-generated search strategies contain errors, which suggests a technical challenge related to algorithm performance issues. These errors can undermine the reliability of the evidence, indicating a problem with the accuracy and trustworthiness of the AI systems.
- The comparison between NeuroLit Navigator and commercial LLM-based systems (Scite, Consensus, and Perplexity) highlights issues with the latter, such as a lack of reproducibility and interpretability. This suggests technical challenges related to the reliability and transparency of these AI systems.
- The abstract does not explicitly mention other technical challenges like data quality or availability problems, scalability challenges, integration difficulties, evaluation methodology limitations, computational resource constraints, or technical bottlenecks or failure points. However, the focus on errors and lack of reproducibility and interpretability implies algorithm performance issues and possibly evaluation methodology limitations.","  - ""To evaluate the relevancy and reproducibility of automated search strategies, we conducted a comparative analysis between a human‐in‐the‐loop system with a neurosymbolic AI framework (i.e., NeuroLit Navigator) and three AI systems that primarily rely on generative language models: Scite, Consensus, and Perplexity.""
  - ""This study highlights the potential of human‐AI collaboration in systematic review workflows, suggesting that AI should augment, rather than replace, librarian expertise.""
  - ""Results showed that through an iterative human‐in‐the‐loop approach, NeuroLit Navigator produced more precise and focused search strategies aligned with domain‐specific terminologies compared to commercial LLM‐based systems, which presented issues such as lack of reproducibility and interpretability.""",,"- The abstract mentions a ""human‐in‐the‐loop system with a neurosymbolic AI framework"" called NeuroLit Navigator. This suggests a hybrid or ensemble approach, as it combines human input with AI capabilities.
- The use of a ""neurosymbolic AI framework"" is a novel technical approach, as it integrates symbolic AI (which is rule-based) with neural networks (which are connectionist). This is a new algorithmic contribution.
- The iterative human-in-the-loop approach is a creative problem-solving method, as it involves continuous feedback and refinement to improve search strategies.
- The abstract highlights the potential of human-AI collaboration, which is an innovative system design element. It suggests that AI should be used to augment human expertise rather than replace it, which is a novel perspective on how to integrate AI into systematic review workflows.
- The focus on producing ""more precise and focused search strategies aligned with domain‐specific terminologies"" indicates a novel feature engineering technique, as it emphasizes the importance of domain-specific knowledge in search strategies.","  - ""Results showed that through an iterative human‐in‐the‐loop approach, NeuroLit Navigator produced more precise and focused search strategies aligned with domain‐specific terminologies compared to commercial LLM‐based systems, which presented issues such as lack of reproducibility and interpretability.""",,"- The abstract mentions that NeuroLit Navigator produced ""more precise and focused search strategies"" compared to other AI systems. This suggests a qualitative performance outcome related to precision.
- The comparison with commercial LLM-based systems (Scite, Consensus, and Perplexity) indicates a baseline for comparison, but no specific quantitative metrics like accuracy, recall, or F1-scores are provided.
- The abstract does not mention any specific quantitative performance metrics such as accuracy, precision, recall, F1-scores, processing speed, efficiency metrics, user satisfaction, system reliability, or scalability test results.
- The focus is on the qualitative aspect of precision and the comparison with other systems in terms of reproducibility and interpretability.","  - ""To evaluate the relevancy and reproducibility of automated search strategies, we conducted a comparative analysis between a human‐in‐the‐loop system with a neurosymbolic AI framework (i.e., NeuroLit Navigator) and three AI systems that primarily rely on generative language models: Scite, Consensus, and Perplexity.""
  - ""Results showed that through an iterative human‐in‐the‐loop approach, NeuroLit Navigator produced more precise and focused search strategies aligned with domain‐specific terminologies compared to commercial LLM‐based systems, which presented issues such as lack of reproducibility and interpretability.""",,"- The abstract mentions a comparative analysis between NeuroLit Navigator and three other AI systems (Scite, Consensus, and Perplexity), but it does not specify any academic databases used for the analysis.
- There is no mention of dataset sizes, characteristics, data collection methods, training/validation/test data splits, or data preprocessing and cleaning approaches in the abstract.
- The abstract does not provide information on external knowledge bases or ontologies used in the study.
- The focus of the abstract is on the comparative analysis of AI systems rather than detailing specific data sources or methodologies.","  - ""Our findings contribute to the growing field of human‐centered AI by providing a model for designing AI systems in information‐intensive domains.""
  - ""This study highlights the potential of human‐AI collaboration in systematic review workflows, suggesting that AI should augment, rather than replace, librarian expertise.""
  - ""Results showed that through an iterative human‐in‐the‐loop approach, NeuroLit Navigator produced more precise and focused search strategies aligned with domain‐specific terminologies compared to commercial LLM‐based systems, which presented issues such as lack of reproducibility and interpretability.""
  - ""To evaluate the relevancy and reproducibility of automated search strategies, we conducted a comparative analysis between a human‐in‐the‐loop system with a neurosymbolic AI framework (i.e., NeuroLit Navigator) and three AI systems that primarily rely on generative language models: Scite, Consensus, and Perplexity.""
  - ""Researchers are increasingly seeking to automate systematic review workflows to reduce time and labor.""",,"- The abstract discusses the automation of systematic review workflows, which is a specific literature review task addressed in the study.
- The study involves a comparative analysis of AI systems, indicating that the target research disciplines or fields are those that rely heavily on systematic reviews, such as health sciences, social sciences, and other fields where systematic reviews are common.
- The abstract mentions the use of a neurosymbolic AI framework (NeuroLit Navigator) and generative language models (Scite, Consensus, and Perplexity), which are types of academic documents processed in the study.
- The study suggests that AI should augment librarian expertise, indicating that the user types include researchers and librarians who require precise and focused search strategies.
- The integration with research workflows is implied by the focus on automating systematic review workflows, which is a critical part of research processes.
- The abstract does not explicitly mention whether the applications are commercial or academic, but the focus on systematic reviews and librarian expertise suggests an academic context."
Machine Learning Techniques and Privacy Concerns in Human-Computer Interactions: A Systematic Review,"Jun He, Tianyu Cao, Vincent G. Duffy",10.1007/978-3-031-48057-7_23,https://doi.org/10.1007/978-3-031-48057-7_23,Interacción,1,2023,Not mentioned (the abstract does not provide specific details on the technical architecture or system components),Not mentioned (the abstract does not provide specific technical implementation details related to machine learning or deep research),"- Search and retrieval algorithms: Harzing’s Publish or Perish, Google nGram
- Citation analysis methods: Harzing’s Publish or Perish, Vicinitas, VOSviewer, CiteSpace, BibExcel
- Content-based matching techniques: maxQDA
- Metadata extraction and utilization: Mendeley
- Citation analysis methods: Trend analysis, co-citation analysis, content analysis, cluster analysis",Not mentioned (the abstract does not provide information on technical challenges or limitations),"Federated learning is an innovative solution that distributes learning tasks to individual devices to reduce information sharing and protect privacy. The abstract also mentions potential future work involving other machine learning techniques developed based on federated learning, which could include novel approaches. However, specific details on new algorithmic contributions, creative problem-solving methods, hybrid or ensemble approaches, novel feature engineering techniques, innovative evaluation metrics, technical workarounds, or original system design elements are not explicitly mentioned.",Not mentioned (the abstract does not provide any performance results or metrics related to machine learning techniques),"- Academic databases used: Not mentioned
- Dataset sizes and characteristics: Not mentioned
- Data collection methods: Not mentioned
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: Human-Computer Interaction (HCI)
- Specific literature review tasks addressed: Trend analysis, co-citation analysis, content analysis, cluster analysis
- Types of academic documents processed: Articles
- User types and requirements: Researchers and developers in HCI and related fields
- Integration with research workflows: Use of tools like Harzing’s Publish or Perish, Google nGram, Vicinitas, VOSviewer, CiteSpace, BibExcel, maxQDA, Mendeley
- Commercial vs. academic applications: Both, with applications in communication networks, healthcare, Internet of Things; future directions in healthcare and finance","  - ""Trend analysis, co-citation analysis, content analysis, and cluster analysis have been conducted to identify the most important articles in the literature.""
  - ""The software Mendeley is also used to help sort out citation items.""
  - ""In this study, a systematic literature review of our topic has been conducted using various tools including Harzing’s Publish or Perish, Google nGram, Vicinitas, VOSviewer, CiteSpace, BibExcel, and maxQDA.""
  - ""One of the machine learning techniques that possess a strong connection with privacy is federated learning, which distributes learning tasks to individual devices to reduce information sharing and protect privacy.""",,"- The abstract mentions ""federated learning"" as a machine learning technique related to privacy, which is a key technical component. However, it does not specify the main AI/ML frameworks used beyond this.
- The tools listed (Harzing’s Publish or Perish, Google nGram, Vicinitas, VOSviewer, CiteSpace, BibExcel, and maxQDA) are used for conducting the systematic literature review, but they are not explicitly described as part of a technical architecture or system components.
- Mendeley is used for sorting citation items, which suggests some integration with existing academic databases or platforms, but again, this is not detailed as part of a technical architecture.
- The abstract does not mention specific system architecture components like databases, APIs, interfaces, or technical infrastructure such as cloud platforms or computational resources.
- The overall system design approach is not explicitly described in terms of technical architecture.","  - ""Trend analysis, co-citation analysis, content analysis, and cluster analysis have been conducted to identify the most important articles in the literature.""
  - ""In this study, a systematic literature review of our topic has been conducted using various tools including Harzing’s Publish or Perish, Google nGram, Vicinitas, VOSviewer, CiteSpace, BibExcel, and maxQDA.""",,"- The abstract mentions the use of various tools for conducting a systematic literature review, which includes Harzing’s Publish or Perish, Google nGram, Vicinitas, VOSviewer, CiteSpace, BibExcel, and maxQDA. These tools are typically used for bibliometric analysis and literature review management.
- The abstract also mentions specific analytical techniques such as trend analysis, co-citation analysis, content analysis, and cluster analysis. These are methods used to analyze the literature and identify important articles.
- However, the abstract does not provide specific technical implementation details such as algorithms and models used, feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or technical workflow or methodology related to machine learning or deep research in the context of literature review.","  - ""a systematic literature review of our topic has been conducted using various tools including Harzing’s Publish or Perish, Google nGram, Vicinitas, VOSviewer, CiteSpace, BibExcel, and maxQDA.""
  - ""Trend analysis, co-citation analysis, content analysis, and cluster analysis have been conducted to identify the most important articles in the literature.""
  - ""The software Mendeley is also used to help sort out citation items.""",,"- The abstract mentions the use of various tools for conducting a systematic literature review, which implies these tools are used for paper discovery and analysis.
- Harzing’s Publish or Perish is a tool used for citation analysis and can help in identifying influential papers.
- Google nGram is typically used for trend analysis, which can help in identifying patterns and trends in literature.
- Vicinitas, VOSviewer, CiteSpace, and BibExcel are tools often used for citation analysis and network visualization, which can aid in identifying key papers and authors.
- maxQDA is a tool used for content analysis, which can help in analyzing the content of papers to identify relevance and themes.
- Mendeley is used for sorting out citation items, which suggests it is used for metadata extraction and organization.
- The mention of trend analysis, co-citation analysis, content analysis, and cluster analysis indicates the use of these methods to identify important articles, which are all relevant to paper discovery techniques.","  - ""One of the machine learning techniques that possess a strong connection with privacy is federated learning, which distributes learning tasks to individual devices to reduce information sharing and protect privacy.""
  - ""Finally, we discuss the potential future work beyond our topics which cover directions in application fields such as healthcare and finance, and other machine learning techniques developed based on federated learning.""
  - ""Trend analysis, co-citation analysis, content analysis, and cluster analysis have been conducted to identify the most important articles in the literature.""",,"- The abstract mentions federated learning as a technique related to privacy concerns, but it does not explicitly discuss technical challenges associated with this technique or other machine learning methods.
- The abstract focuses on the systematic review process and the applications of machine learning and privacy concerns, but it does not detail specific technical challenges or limitations.
- The mention of future work directions suggests potential areas for further research, but it does not specify current technical challenges or problems.
- The abstract does not provide information on algorithm performance issues, data quality or availability problems, scalability challenges, integration difficulties, evaluation methodology limitations, computational resource constraints, or technical bottlenecks or failure points.","  - ""One of the machine learning techniques that possess a strong connection with privacy is federated learning, which distributes learning tasks to individual devices to reduce information sharing and protect privacy.""
  - ""Finally, we discuss the potential future work beyond our topics which cover directions in application fields such as healthcare and finance, and other machine learning techniques developed based on federated learning.""",,"- The abstract mentions ""federated learning"" as a machine learning technique that is strongly connected to privacy concerns. This is an innovative solution because it addresses privacy by distributing learning tasks to individual devices, thereby reducing information sharing.
- The mention of ""other machine learning techniques developed based on federated learning"" suggests that there are novel or innovative approaches being developed from federated learning, which could include new algorithmic contributions or hybrid approaches.
- The abstract does not explicitly mention new algorithmic contributions, creative problem-solving methods, hybrid or ensemble approaches, novel feature engineering techniques, innovative evaluation metrics, technical workarounds, or original system design elements. However, the focus on federated learning and its applications indicates a potential for these types of innovations in the context of privacy and machine learning.","  - ""One of the machine learning techniques that possess a strong connection with privacy is federated learning, which distributes learning tasks to individual devices to reduce information sharing and protect privacy.""
  - ""Finally, we discuss the potential future work beyond our topics which cover directions in application fields such as healthcare and finance, and other machine learning techniques developed based on federated learning.""
  - ""Trend analysis, co-citation analysis, content analysis, and cluster analysis have been conducted to identify the most important articles in the literature.""
  - ""It has been found that the two topics have been well studied and together they have a variety of applications, including communication networks, the healthcare industry, and the Internet of Things.""",,"- The abstract discusses the use of federated learning as a machine learning technique related to privacy concerns, but it does not provide any specific performance results or metrics such as accuracy, precision, recall, F1-scores, processing speed, or user satisfaction.
- The abstract mentions various analyses conducted to identify important articles, but these analyses are related to literature review methods rather than performance metrics of machine learning techniques.
- The abstract does not mention any comparison with baseline methods, system reliability, robustness measures, or scalability test results.
- The focus of the abstract is on the systematic review of machine learning techniques and privacy concerns in human-computer interactions, rather than on specific performance results or technical implementation details.","  - ""The software Mendeley is also used to help sort out citation items.""
  - ""a systematic literature review of our topic has been conducted using various tools including Harzing’s Publish or Perish, Google nGram, Vicinitas, VOSviewer, CiteSpace, BibExcel, and maxQDA.""
  - ""Trend analysis, co-citation analysis, content analysis, and cluster analysis have been conducted to identify the most important articles in the literature.""",,"- The abstract mentions the use of various tools for conducting the systematic literature review, which are typically used for data collection and analysis in such studies. These tools include Harzing’s Publish or Perish, Google nGram, Vicinitas, VOSviewer, CiteSpace, BibExcel, and maxQDA.
- Mendeley is used for sorting out citation items, which suggests it is used for managing references and citations.
- The abstract does not provide specific information about academic databases used (e.g., Scopus, Web of Science), dataset sizes and characteristics, data collection methods, training/validation/test data splits, data preprocessing and cleaning approaches, or external knowledge bases or ontologies used.
- The focus of the abstract is on the tools used for analysis rather than the specific data sources or datasets themselves.","  - ""the field of human-computer interaction (HCI) has received an increasing amount of attention.""
  - ""machine learning and privacy concerns are two of the most important ones.""
  - ""federated learning, which distributes learning tasks to individual devices to reduce information sharing and protect privacy.""
  - ""a systematic literature review of our topic has been conducted using various tools including Harzing’s Publish or Perish, Google nGram, Vicinitas, VOSviewer, CiteSpace, BibExcel, and maxQDA.""
  - ""The software Mendeley is also used to help sort out citation items.""
  - ""Trend analysis, co-citation analysis, content analysis, and cluster analysis have been conducted to identify the most important articles in the literature.""
  - ""the two topics have been well studied and together they have a variety of applications, including communication networks, the healthcare industry, and the Internet of Things.""
  - ""we discuss the potential future work beyond our topics which cover directions in application fields such as healthcare and finance, and other machine learning techniques developed based on federated learning.""",,"- The abstract mentions that the study focuses on the field of human-computer interaction (HCI), which is the target research discipline.
- The specific literature review tasks addressed include trend analysis, co-citation analysis, content analysis, and cluster analysis to identify important articles.
- The types of academic documents processed are likely articles, as indicated by the use of citation management software like Mendeley.
- The abstract does not specify user types or requirements, but it implies that the study is relevant to researchers and developers in HCI and related fields.
- Integration with research workflows is suggested by the use of various tools for literature review and citation management.
- The abstract mentions applications in communication networks, healthcare, and the Internet of Things, indicating both academic and commercial applications.
- Future work directions include healthcare and finance, suggesting potential commercial applications."
Bibliometric analysis and systematic review of research on expert finding a PRISMA-guided approach,"Xuan-Lam Pham, Thi Thu Le",10.34028/iajit/21/4/9,https://doi.org/10.34028/iajit/21/4/9,˜The œinternational Arab journal of information technology,7,2024,Not mentioned (the paper does not provide specific details on the technical architecture or system components),"- Algorithms and models used: Deep learning (mentioned as a trend in the field)
- Feature extraction techniques: Keyword co-occurrence analysis, text analysis
- Data processing pipelines: Not mentioned
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Bibliometric analysis using VOSviewer, systematic review using NVivo","- Search and retrieval algorithms: Not explicitly mentioned
- Relevance ranking approaches: Selection of papers based on highest citation counts; use of evaluation metrics like precision, MAP, MRR, NDCG
- Citation analysis methods: Bibliometric analysis using VOSviewer to identify prominent co-author groups and highly-cited documents
- Content-based matching techniques: Keyword co-occurrence analysis to reveal thematic clusters
- Metadata extraction and utilization: Use of NVivo for systematic organization and analysis of textual data
- Recommendation system approaches: Not explicitly mentioned
- Expert/authority identification methods: Identification of prominent author groups through co-authorship network analysis","- Data quality or availability problems: The study was limited to data from the Scopus database, omitting other relevant sources like WoS, ACM, or IEEE Xplore.
- Scalability challenges: The review was based on a modest selection of 51 articles, which may not provide a comprehensive assessment.
- Data quality issues: The keyword set used for document retrieval may not be exhaustive, potentially missing relevant documents.",Not mentioned (the paper does not explicitly mention novel technical approaches or innovations),"- Accuracy: 3 studies
- Precision: 23 studies
- Recall: 4 studies
- MAP (Mean Average Precision): 22 studies
- MRR (Mean Reciprocal Rank): 21 studies
- NDCG (Normalized Discounted Cumulative Gain): 12 studies
- Qualitative metrics (human judgment): 2 studies
- Regression metrics (R2 Score, MAE): 1 study
- Rank correlation metrics (Kendall's, Spearman's): 2 studies","- Academic databases used: Scopus
- Dataset sizes and characteristics: Detailed in Table 5 and Table 9
- Data collection methods: Searching Scopus using specific keywords
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: Academic, Enterprise, Community Question Answering (CQA), Social Networks, Online Knowledge Communities, Academic Social Networking
- Specific literature review tasks addressed: Identifying domains engaging with EF, datasets used in expert search tasks, evaluation methods for EFSs
- Types of academic documents processed: Articles, Books, Book chapters, Conference papers, Reviews
- User types and requirements: Researchers seeking expert solutions, academic institutions
- Integration with research workflows: Bibliometric analysis and systematic review
- Commercial vs. academic applications: Primarily academic applications","  - ""Through bibliometric analysis, utilizing VOSviewer, we identify prominent co-author groups, highly-cited documents, and global participation, shedding light on the collaborative and internationally expansive nature of EF investigations.""
  - ""Simultaneously, our systematic review, conducted on a subset of 51 articles using NVivo, explores domains seeking expert solutions, prevalent datasets, and common evaluation methods.""
  - ""Our bibliometric analysis, revealed trends through keyword co-occurrence analysis, identified prominent co-author groups, explored bibliographic coupling across countries, and visually mapped the global landscape based on text extracted from abstracts and titles.""
  - ""To conduct the bibliometric analysis, we utilized VOSviewer [51]. This tool was employed to identify trends based on keyword co-occurrence analysis, coauthor groups, bibliographic coupling across countries, and to create a map based on the abstracts and titles of the identified articles [52].""
  - ""The findings about the evaluation methods employed for assessing EFSs are elucidated in Table 6""
  - ""The table encompasses a diverse array of evaluation metrics, each associated with the number of studies utilizing that specific metric.""
  - ""The predominant focus is on information retrieval metrics, with a significant number of studies utilizing metrics such as Precision (23 studies), MAP (22 studies), MRR (21 studies), NDCG, recall, accuracy, and so on.""
  - ""The presence of regression metrics, namely R2 Score and MAE, in one study, indicates a consideration for quantitative regression analysis to assess system performance.""
  - ""the incorporation of rank correlation metrics such as Kendall's and Spearman's in two studies underscores the importance of understanding the ranking consistency of EFSs.""
  - ""Our study offers a thorough examination of the evolving landscape in EF research through a combination of bibliometric analysis and systematic review using the PRISMA methodology.""
  - ""Through bibliometric analysis conducted with VOSviewer, we uncovered collaborative trends and thematic clusters reflecting the shifting emphasis in the field, particularly towards academia and online platforms like community question answering.""
  - ""Our systematic review of 51 articles, utilizing NVivo, delved into EF domains, prevalent datasets, and evaluation methods, addressing key RQs and highlighting a transition from enterprise to academia and community platforms.""
  - ""The shift in prominent keywords over the years indicates a transformation in research focus within EFs, driven by several factors. The emergence of new technologies and platforms, such as community question-answering platforms like Stack Overflow, has prompted researchers to explore real-world problems and adapt methodologies to the evolving technological landscape.""
  - ""Researchers are increasingly leveraging advanced machine learning techniques, particularly deep learning, for practical applications, reflecting changing demands and trends in the field.""
  - ""The integration of these approaches is guided by the Preferred Reporting Items for Systematic Reviews and Meta Analyses (PRISMA) methodology, ensuring a rigorous and transparent investigation.""
  - ""the data about publications in the domain of EF was exclusively sourced from the Scopus database; thereby, data from other relevant search sources such as WoS, ACM, or IEEE Xplore were omitted.""
  - ""the set of keywords used to retrieve documents in the field of EF may not be exhaustive.""
  - ""The incorporation of logical operators in the search strategy also enhances the precision of the retrieved results.""
  - ""selecting only 51 articles for systematic reviews may be considered modest, and as such, the results obtained may not provide a truly comprehensive assessment of issues related to EFs.""
  - ""A broader selection of articles in future reviews would contribute to a more thorough understanding of the subject matter.""
  - ""Despite these limitations, we believe that the findings of this study make a meaningful contribution to the existing body of knowledge in the field of EFs.""",,"- The paper primarily focuses on bibliometric analysis and systematic review rather than detailing a specific technical architecture or system components.
- The use of VOSviewer for bibliometric analysis and NVivo for content analysis suggests these tools are part of the technical architecture, but they are not AI/ML frameworks.
- The paper mentions the use of deep learning techniques in the field of expert finding, but it does not specify a particular AI/ML framework used in the study.
- There is no mention of specific system architecture components such as databases, APIs, or interfaces.
- The paper does not discuss technical infrastructure like cloud platforms or computational resources.
- Integration with existing academic databases is implied through the use of Scopus, but no specific technical details are provided.
- The overall system design approach is guided by the PRISMA methodology, which is a framework for systematic reviews rather than a technical architecture.","  - ""Through bibliometric analysis, utilizing VOSviewer, we identify prominent co-author groups, highly-cited documents, and global participation, shedding light on the collaborative and internationally expansive nature of EF investigations.""
  - ""Keyword co-occurrence and text analysis reveal thematic clusters, signaling the evolving emphases in the field from foundational expert search tasks to considerations of platform interactions.""
  - ""our systematic review, conducted on a subset of 51 articles using NVivo, explores domains seeking expert solutions, prevalent datasets, and common evaluation methods.""
  - ""To conduct the bibliometric analysis, we utilized VOSviewer [51]. This tool was employed to identify trends based on keyword co-occurrence analysis, coauthor groups, bibliographic coupling across countries, and to create a map based on the abstracts and titles of the identified articles [52].""
  - ""Data extraction involved coding relevant information from each article, including domains, datasets, and evaluation methods.""
  - ""NVivo facilitated the systematic organization and analysis of textual data, ensuring a thorough exploration of the content [41].""
  - ""We conducted a statistical analysis of research articles in the EF field, extracted from Scopus from 2000 to 2023 (see Figure 2""
  - ""Trends based on keywords co-occurrence analysis. A total of 1008 keywords (author keywords) were investigated, 40 of which appeared more than 5 times.""
  - ""The shift in prominent keywords over the years indicates a transformation in research focus within EFs, driven by several factors.""
  - ""The emergence of new technologies and platforms, such as community question-answering platforms like Stack Overflow, has prompted researchers to explore real-world problems and adapt methodologies to the evolving technological landscape.""
  - ""Researchers are increasingly leveraging advanced machine learning techniques, particularly deep learning, for practical applications, reflecting changing demands and trends in the field.""
  - ""The findings about the evaluation methods employed for assessing EFSs are elucidated in Table 6""
  - ""The predominant focus is on information retrieval metrics, with a significant number of studies utilizing metrics such as Precision (23 studies), MAP (22 studies), MRR (21 studies), NDCG, recall, accuracy, and so on.""",,"- The paper primarily uses bibliometric analysis and systematic review methodologies, employing tools like VOSviewer and NVivo for data analysis.
- The focus is on identifying trends, co-author groups, and thematic clusters rather than specific technical implementation details like algorithms or feature extraction techniques.
- The paper discusses the use of keyword co-occurrence analysis and text analysis to identify trends and thematic clusters, but does not specify any particular algorithms or models used for these analyses.
- The mention of ""deep learning"" suggests its use in practical applications within the field, but no specific implementation details are provided.
- The paper does not provide specific technical implementation details such as data processing pipelines, training methodologies, or preprocessing steps.
- The evaluation metrics mentioned are primarily information retrieval metrics like precision, MAP, MRR, and NDCG, but these are not technical implementation details.","  - ""Through bibliometric analysis, utilizing VOSviewer, we identify prominent co-author groups, highly-cited documents, and global participation, shedding light on the collaborative and internationally expansive nature of EF investigations.""
  - ""Keyword co-occurrence and text analysis reveal thematic clusters, signaling the evolving emphases in the field from foundational expert search tasks to considerations of platform interactions.""
  - ""Our bibliometric analysis, revealed trends through keyword co-occurrence analysis, identified prominent co-author groups, explored bibliographic coupling across countries, and visually mapped the global landscape based on text extracted from abstracts and titles.""
  - ""VOSviewer allows us to visually represent the collaborative and international nature of EF investigations.""
  - ""The co-authorship network was examined to identify prominent author groups, highlighting collaborative patterns.""
  - ""The generated map visually represents the interconnectedness of research themes, providing insights into the structure and dynamics of the field.""
  - ""Based on their highest citation counts among the identified pool of 494 articles, we selected 51 papers from the top 1 to 4 articles per year for the systematic review.""
  - ""NVivo facilitated the systematic organization and analysis of textual data, ensuring a thorough exploration of the content""
  - ""The results obtained from NVivo were synthesized to provide comprehensive insights into the diverse domains, datasets, and evaluation methods prevalent in the field of EF.""
  - ""Trends based on keywords co-occurrence analysis. A total of 1008 keywords (author keywords) were investigated, 40 of which appeared more than 5 times.""
  - ""The shift in prominent keywords over the years indicates a transformation in research focus within EFs, driven by several factors.""
  - ""The findings about the evaluation methods employed for assessing EFSs are elucidated in Table 6""","  - ""(Page 10, Table 1) | Information retrieval metrics                                                                 | Evaluation metrics                                                   | Number of studies using metric | References                                                                                           |\n|------------------------------------------------------------------------------------------------|----------------------------------------------------------------------|--------------------------------|------------------------------------------------------------------------------------------------------|\n|                                                                                                | Precision; R-precision; mean R-precision; mean Pr                    | 23                             | [3, 7, 11, 13, 14, 16, 17, 19, 21, 22, 24, 27, 31, 35, 36, 40, 42, 44, 45, 53, 54, 60]               |""","- The paper uses bibliometric analysis with VOSviewer to identify prominent co-author groups and highly-cited documents, which is a method for finding and matching academic papers based on citation analysis.
- Keyword co-occurrence analysis is used to reveal thematic clusters, which is a content-based matching technique to identify evolving emphases in the field.
- The paper mentions the use of NVivo for systematic organization and analysis of textual data, which is a metadata extraction and utilization method.
- The selection of papers based on highest citation counts is a relevance ranking approach.
- The paper discusses the use of various evaluation metrics such as precision, MAP, MRR, and NDCG, which are information retrieval metrics used for relevance ranking.
- The paper does not explicitly mention search and retrieval algorithms or recommendation system approaches, but it does focus on citation analysis and content-based matching techniques.","  - ""The absence of thorough reviews from 2019 to 2023 is a glaring gap in the expert finding field.""
  - ""With the accelerated advancements and the integration of numerous new technologies into this domain during these years, there is a critical need for additional comprehensive assessments of this field.""
  - ""While this study has yielded novel findings, it is imperative to acknowledge certain limitations.""
  - ""Firstly, the data about publications in the domain of EF was exclusively sourced from the Scopus database; thereby, data from other relevant search sources such as WoS, ACM, or IEEE Xplore were omitted.""
  - ""Additionally, the set of keywords used to retrieve documents in the field of EF may not be exhaustive.""
  - ""Furthermore, selecting only 51 articles for systematic reviews may be considered modest, and as such, the results obtained may not provide a truly comprehensive assessment of issues related to EFs.""
  - ""A broader selection of articles in future reviews would contribute to a more thorough understanding of the subject matter.""",,"- The paper highlights a gap in comprehensive reviews from 2019 to 2023, indicating a challenge in keeping up with the rapid advancements in the field.
- The limitation of sourcing data only from the Scopus database suggests a data availability problem, as other relevant databases were not included.
- The potential incompleteness of the keyword set used for document retrieval indicates a data quality issue, as it may not capture all relevant documents.
- The selection of only 51 articles for the systematic review suggests a scalability challenge, as a broader selection could provide a more comprehensive understanding.
- The paper does not explicitly mention algorithm performance issues, integration difficulties, evaluation methodology limitations, computational resource constraints, or technical bottlenecks. However, it implies challenges in data quality and availability, as well as scalability in terms of the scope of the review.","  - ""This study conducts a comprehensive exploration of expert retrieval using a dual approach of bibliometric analysis and systematic review, guided by the Preferred Reporting Items for Systematic Reviews and Meta Analyses (PRISMA) methodology.""
  - ""Through bibliometric analysis, utilizing VOSviewer, we identify prominent co-author groups, highly-cited documents, and global participation, shedding light on the collaborative and internationally expansive nature of EF investigations.""
  - ""Keyword co-occurrence and text analysis reveal thematic clusters, signaling the evolving emphases in the field from foundational expert search tasks to considerations of platform interactions.""
  - ""our systematic review, conducted on a subset of 51 articles using NVivo, explores domains seeking expert solutions, prevalent datasets, and common evaluation methods.""
  - ""The findings about the evaluation methods employed for assessing EFSs are elucidated in Table 6""
  - ""The table encompasses a diverse array of evaluation metrics, each associated with the number of studies utilizing that specific metric.""
  - ""The predominant focus is on information retrieval metrics, with a significant number of studies utilizing metrics such as Precision (23 studies), MAP (22 studies), MRR (21 studies), NDCG, recall, accuracy, and so on.""
  - ""The presence of regression metrics, namely R2 Score and MAE, in one study, indicates a consideration for quantitative regression analysis to assess system performance.""
  - ""the incorporation of rank correlation metrics such as Kendall's and Spearman's in two studies underscores the importance of understanding the ranking consistency of EFSs.""
  - ""Our study offers a thorough examination of the evolving landscape in EF research through a combination of bibliometric analysis and systematic review using the PRISMA methodology.""",,"- The paper primarily focuses on a bibliometric analysis and systematic review of expert finding systems, which does not directly introduce new algorithmic contributions or technical innovations.
- The use of VOSviewer for bibliometric analysis and NVivo for systematic review are tools rather than novel technical approaches.
- The paper discusses various evaluation metrics used in the field, such as precision, MAP, MRR, and NDCG, which are standard metrics rather than innovative solutions.
- The mention of regression metrics like R2 Score and MAE, as well as rank correlation metrics like Kendall's and Spearman's, indicates some focus on quantitative analysis but does not necessarily represent novel technical approaches.
- The paper does not explicitly mention new algorithmic contributions, creative problem-solving methods, hybrid or ensemble approaches, novel feature engineering techniques, or original system design elements.","  - ""The findings about the evaluation methods employed for assessing EFSs are elucidated in Table 6""
  - ""The table encompasses a diverse array of evaluation metrics, each associated with the number of studies utilizing that specific metric.""
  - ""The predominant focus is on information retrieval metrics, with a significant number of studies utilizing metrics such as Precision (23 studies), MAP (22 studies), MRR (21 studies), NDCG, recall, accuracy, and so on.""
  - ""the inclusion of qualitative metrics, specifically those based on human judgment, in two studies emphasizes the recognition of subjective assessment in evaluating the effectiveness of EFSs.""
  - ""The presence of regression metrics, namely R2 Score and MAE, in one study, indicates a consideration for quantitative regression analysis to assess system performance.""
  - ""the incorporation of rank correlation metrics such as Kendall's and Spearman's in two studies underscores the importance of understanding the ranking consistency of EFSs.""","  - ""(Page 10, Table 1) | Information retrieval metrics                                                                 | Evaluation metrics                                                   | Number of studies using metric | References                                                                                           |\n|------------------------------------------------------------------------------------------------|----------------------------------------------------------------------|--------------------------------|------------------------------------------------------------------------------------------------------|\n|                                                                                                | Precision; R-precision; mean R-precision; mean Pr                    | 23                             | [3, 7, 11, 13, 14, 16, 17, 19, 21, 22, 24, 27, 31, 35, 36, 40, 42, 44, 45, 53, 54, 60]               |""","- The paper discusses various evaluation metrics used to assess the performance of Expert Finding Systems (EFSs), which are relevant to the question about performance results.
- The metrics mentioned include precision, MAP (Mean Average Precision), MRR (Mean Reciprocal Rank), NDCG (Normalized Discounted Cumulative Gain), recall, and accuracy, which are quantitative performance outcomes.
- The paper also mentions qualitative metrics based on human judgment, indicating user satisfaction or usability results.
- Regression metrics like R2 Score and MAE are mentioned, which relate to processing speed and efficiency metrics.
- Rank correlation metrics such as Kendall's and Spearman's are included, which can relate to system reliability and robustness measures.
- The table on page 10 provides detailed information on these metrics and the number of studies using them, which is crucial for understanding the performance results.","  - ""From 2000 to 2023, our investigation reveals a notable upward trajectory in expert locating study, focusing on 494 articles identified from Scopus using specific keywords related to Expert Finding (EF) and Expert Finding Systems (EFSs).""
  - ""The search was restricted to the Scopus database, resulting in the identification of 494 articles.""
  - ""Table 5 systematically provides a comprehensive overview of commonly utilized datasets in expert search tasks, detailing platforms, environments, years, and frequency of usage for each dataset.""
  - ""The TREC dataset, rooted in the enterprise environment from 2005 to 2008, has a significant frequency of 6 studies.""
  - ""The versatile Self-collected Dataset, drawn from academic social networks, Amazon Mechanical Turk, public social networks, Quora, online communities, Reddit, ScholarMate, and company surveys, spanning various environments from 2009 to 2021, has been utilized in 9 studies,""
  - ""The data collection and analysis process is shown in Figure 1 above.""
  - ""The exclusion criteria involved studies outside the defined timeframe, languages other than English, and articles not directly related to EF.""
  - ""The document types considered for this systematic review encompassed Articles, Books, Book chapters, Conference papers, and Reviews.""",,"- The paper uses Scopus as the primary academic database for identifying relevant articles.
- The dataset sizes and characteristics are detailed in Table 5 and Table 9, which provide information on various datasets used in expert search tasks, including their platforms, environments, and years of use.
- The data collection method involved searching Scopus using specific keywords related to Expert Finding and Expert Finding Systems.
- The paper does not provide specific information on training/validation/test data splits or data preprocessing and cleaning approaches.
- External knowledge bases or ontologies are not explicitly mentioned in the paper.","  - ""This study conducts a comprehensive exploration of expert retrieval using a dual approach of bibliometric analysis and systematic review, guided by the Preferred Reporting Items for Systematic Reviews and Meta Analyses (PRISMA) methodology.""
  - ""The motivation behind this review stems from the recognition of the escalating importance of EF across various domains and the necessity to extract insights from the expanding literature.""
  - ""The period under consideration spans from 2000 to 2023, encapsulating the dynamic evolution of EF research.""
  - ""our systematic review, conducted on a subset of 51 articles using NVivo, explores domains seeking expert solutions, prevalent datasets, and common evaluation methods.""
  - ""Explicitly, this review addresses three key Research Questions (RQs): Firstly, we inquire into the domains that frequently engage with EF, shedding light on the diverse contexts in which expertise identification is paramount RQ1.""
  - ""Secondly, we examine the datasets commonly utilized in expert search tasks, recognizing the foundational information upon which these systems operate RQ2.""
  - ""Lastly, we explore the evaluation methods employed to assess the efficacy of EF and EFSs, contributing to a critical appraisal of their performance and impact RQ3.""
  - ""EF tasks have garnered interest across diverse fields, necessitating the identification of detailed tasks and the associated challenges.""
  - ""These include:

 Academic: this domain encompasses various tasks associated with EF, including finding experts for consultation in new research endeavors, automated assignment of papers to reviewers in peer-review processes, recommendation of reviewer panels for state research grant applications, locating advisors and supervisors, finding experts (of the university) for collaborative projects, and fostering research collaboration.""
  - ""Enterprise: research conducted from 2000 to 2011 focused on finding experts in the enterprise field, particularly within large businesses and organizations with substantial workforces.""
  - ""CQA: this emerging field, particularly prominent from 2019 to 2023, has captured authors' attention when researching issues related to finding experts.""
  - ""Social network: this domain also captivates the attention of the expert-finding research community, although the volume of articles within this field is not as high as those of previously mentioned domains.""
  - ""Online knowledge communities and Academic social networking: Among the 51 documents we utilized for summarizing research, these are the two domains with the least number of referenced articles.""
  - ""The findings about the evaluation methods employed for assessing EFSs are elucidated in Table 6""","  - ""(Page 8, Table 1) | Domain/field                     | Task                                                                 | References                          |\n|----------------------------------|----------------------------------------------------------------------|-------------------------------------|\n| Academic (e.g., a university     | Finding experts for consultation when doing new research             |                                     |""
  - ""(Page 9, Table 1) -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| Dataset                      | Platforms                                                                 | Environment     | Year       | Data size                                                                                     | Studies used the dataset |\n|------------------------------|---------------------------------------------------------------------------|-----------------|------------|-----------------------------------------------------------------------------------------------|--------------------------|""
  - ""(Page 10, Table 1) | Information retrieval metrics                                                                 | Evaluation metrics                                                   | Number of studies using metric | References                                                                                           |\n|------------------------------------------------------------------------------------------------|----------------------------------------------------------------------|--------------------------------|------------------------------------------------------------------------------------------------------|\n|                                                                                                | Precision; R-precision; mean R-precision; mean Pr                    | 23                             | [3, 7, 11, 13, 14, 16, 17, 19, 21, 22, 24, 27, 31, 35, 36, 40, 42, 44, 45, 53, 54, 60]               |""","- The paper focuses on expert finding (EF) and expert finding systems (EFSs), which are relevant to literature review tasks, particularly in identifying key papers and experts.
- The study uses a dual approach of bibliometric analysis and systematic review, which is applicable to academic research workflows.
- The paper addresses three research questions related to domains engaging with EF, datasets used in expert search tasks, and evaluation methods for EFSs, which are crucial for literature review tasks.
- The domains frequently engaging with EF include academic, enterprise, community question answering (CQA), social networks, online knowledge communities, and academic social networking.
- The paper discusses various datasets used in expert search tasks, which are relevant to processing academic documents.
- The evaluation methods discussed are primarily information retrieval metrics, which are used to assess the effectiveness of EFSs in academic contexts.
- The study is focused on academic applications rather than commercial ones, as it deals with academic literature and research workflows."
Mirror Matching: Document Matching Approach in Seed-driven Document Ranking for Medical Systematic Reviews,"Grace E. Lee, Aixin Sun",-,-,arXiv.org,1,2021,"- Main AI/ML frameworks used: Not mentioned (though traditional and neural retrieval models are referenced)
- System architecture components: Not mentioned
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Not mentioned
- Overall system design approach: Focuses on Mirror Matching for document ranking","- Algorithms and models used: Mirror Matching (a document matching measure)
- Feature extraction techniques: Incorporating common writing patterns (background, method, result, conclusion)
- Data processing pipelines: Not mentioned
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Not mentioned","- Search and retrieval algorithms: Term weights in retrieval models
- Relevance ranking approaches: Seed-driven document ranking (SDR), Screening prioritization
- Content-based matching techniques: Mirror Matching","- Scalability challenges: Screening studies is a time-consuming process due to the large volume of literature.
- Algorithm performance issues: Previous work on SDR seeks ways to identify different term weights, indicating challenges in optimizing retrieval models.
- Technical bottlenecks: Previous methods may have limitations in accuracy or efficiency, addressed by the Mirror Matching approach.","- New algorithmic contribution: Mirror Matching document matching measure
- Creative problem-solving method: Formulating SDR as finding similar documents to a query document
- Novel feature engineering technique: Incorporating common writing patterns in medical abstracts
- Technical workaround: Achieving higher performance with a simpler approach than traditional and neural models","- Average Precision: Higher than traditional and neural retrieval models
- Precision-focused metrics: Higher than traditional and neural retrieval models
- Comparison with baseline methods: Better than traditional and neural retrieval models",CLEF 2019 eHealth Task 2 TAR dataset,"- Target research disciplines or fields: Medical research
- Specific literature review tasks addressed: Screening process in systematic reviews
- Types of academic documents processed: Medical abstract texts
- User types and requirements: Medical researchers conducting systematic reviews, requiring assistance in prioritizing documents
- Integration with research workflows: Seed-driven document ranking to provide document rankings
- Commercial vs. academic applications: Academic","  - ""We conduct experiments on CLEF 2019 eHealth Task 2 TAR dataset, and the empirical results show this simple approach achieves the higher performance than traditional and neural retrieval models on Average Precision and Precision-focused metrics.""
  - ""We propose a document matching measure named Mirror Matching, which calculates matching scores between medical abstract texts by incorporating common writing patterns, such as background, method, result, and conclusion in order.""",,"- The abstract mentions a ""document matching measure named Mirror Matching,"" which suggests a technical component of the system. This measure is used to calculate matching scores between medical abstract texts based on common writing patterns.
- The mention of ""traditional and neural retrieval models"" indicates that the system may involve machine learning (ML) or natural language processing (NLP) frameworks, as these are common in document retrieval tasks.
- The abstract does not specify any particular AI/ML frameworks, system architecture components, technical infrastructure, or integration with existing academic databases or platforms. It focuses on the Mirror Matching approach rather than detailing the broader technical architecture.
- The use of the CLEF 2019 eHealth Task 2 TAR dataset suggests that the system is tested on a specific dataset, but it does not provide information on how the system is integrated with existing databases or platforms.","  - ""We propose a document matching measure named Mirror Matching, which calculates matching scores between medical abstract texts by incorporating common writing patterns, such as background, method, result, and conclusion in order.""
  - ""Alternatively, we formulate the SDR task as finding similar documents to a query document and produce rankings based on similarity scores.""
  - ""Previous work on SDR seeks ways to identify different term weights in a query document and utilizes them in a retrieval model to compute ranking scores.""
  - ""We conduct experiments on CLEF 2019 eHealth Task 2 TAR dataset, and the empirical results show this simple approach achieves the higher performance than traditional and neural retrieval models on Average Precision and Precision-focused metrics.""",,"- The abstract mentions that previous work on Seed-driven Document Ranking (SDR) involves identifying term weights and using them in retrieval models, but it does not specify the exact algorithms or models used in these previous works.
- The authors propose a new approach called ""Mirror Matching,"" which is a document matching measure that calculates similarity scores based on common writing patterns in medical abstracts. This suggests a focus on similarity-based ranking rather than traditional term weight-based methods.
- The abstract does not provide specific details on feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or technical workflow beyond the concept of Mirror Matching.
- The mention of experiments on the CLEF 2019 eHealth Task 2 TAR dataset indicates that the authors tested their approach using this dataset, but it does not provide details on the technical implementation or methodology beyond the use of Mirror Matching.","  - ""Previous work on SDR seeks ways to identify different term weights in a query document and utilizes them in a retrieval model to compute ranking scores.""
  - ""We propose a document matching measure named Mirror Matching, which calculates matching scores between medical abstract texts by incorporating common writing patterns, such as background, method, result, and conclusion in order.""
  - ""Alternatively, we formulate the SDR task as finding similar documents to a query document and produce rankings based on similarity scores.""
  - ""Seed-driven document ranking (SDR) uses a known relevant document (ie., seed) as a query and generates such rankings.""
  - ""Screening prioritization (ie., document ranking) is an approach for assisting researchers by providing document rankings where relevant documents are ranked higher than irrelevant ones.""",,"- The abstract discusses ""screening prioritization"" and ""seed-driven document ranking (SDR)"" as techniques for assisting researchers in systematic reviews. These are relevance ranking approaches.
- The mention of ""previous work on SDR"" indicates the use of term weights in retrieval models, which is a search and retrieval algorithm.
- The proposal of ""Mirror Matching"" as a document matching measure suggests a content-based matching technique, as it involves calculating similarity scores based on common writing patterns in medical abstracts.
- The abstract does not mention citation analysis methods, metadata extraction and utilization, recommendation system approaches, or expert/authority identification methods.","  - ""When medical researchers conduct a systematic review (SR), screening studies is the most time-consuming process: researchers read several thousands of medical literature and manually label them relevant or irrelevant.""
  - ""Screening prioritization (ie., document ranking) is an approach for assisting researchers by providing document rankings where relevant documents are ranked higher than irrelevant ones.""
  - ""Previous work on SDR seeks ways to identify different term weights in a query document and utilizes them in a retrieval model to compute ranking scores.""
  - ""the empirical results show this simple approach achieves the higher performance than traditional and neural retrieval models on Average Precision and Precision-focused metrics.""
  - ""We propose a document matching measure named Mirror Matching, which calculates matching scores between medical abstract texts by incorporating common writing patterns, such as background, method, result, and conclusion in order.""",,"- The abstract mentions that screening studies is a time-consuming process, which implies a scalability challenge in terms of manually handling large volumes of literature.
- The use of seed-driven document ranking (SDR) suggests a potential algorithm performance issue, as it relies on identifying relevant documents based on a known seed document.
- The mention of previous work seeking ways to identify term weights indicates a technical challenge in optimizing retrieval models for accurate ranking.
- The proposal of Mirror Matching as a solution suggests that previous methods may have limitations in terms of accuracy or efficiency, which could be considered a technical bottleneck.
- The abstract does not explicitly mention data quality or availability problems, integration difficulties, evaluation methodology limitations, or computational resource constraints.","  - ""We propose a document matching measure named Mirror Matching, which calculates matching scores between medical abstract texts by incorporating common writing patterns, such as background, method, result, and conclusion in order.""
  - ""Alternatively, we formulate the SDR task as finding similar documents to a query document and produce rankings based on similarity scores.""
  - ""the empirical results show this simple approach achieves the higher performance than traditional and neural retrieval models on Average Precision and Precision-focused metrics.""",,"- The abstract introduces a novel approach to seed-driven document ranking (SDR) by formulating it as a task of finding similar documents to a query document. This is a creative problem-solving method as it shifts the focus from term weights to document similarity.
- The ""Mirror Matching"" document matching measure is a new algorithmic contribution. It is innovative because it incorporates common writing patterns in medical abstracts, such as background, method, result, and conclusion, to calculate matching scores. This is a novel feature engineering technique as it uses structural elements of the abstracts to improve matching.
- The approach is described as ""simple"" yet achieves higher performance than traditional and neural retrieval models. This suggests a technical workaround for known problems in document ranking, as it outperforms more complex models with a simpler method.
- The use of Mirror Matching to improve document ranking is an original system design element, as it introduces a new way to rank documents based on similarity rather than term weights.","  - ""the empirical results show this simple approach achieves the higher performance than traditional and neural retrieval models on Average Precision and Precision-focused metrics.""",,"- The abstract mentions that the proposed approach, Mirror Matching, achieves higher performance compared to traditional and neural retrieval models.
- The specific metrics mentioned are ""Average Precision and Precision-focused metrics,"" which are quantitative performance outcomes.
- There is no mention of other performance metrics such as accuracy, recall, F1-scores, processing speed, efficiency metrics, user satisfaction, system reliability, or scalability test results.
- The comparison with baseline methods is indicated by the statement that Mirror Matching performs better than traditional and neural retrieval models.","  - ""We conduct experiments on CLEF 2019 eHealth Task 2 TAR dataset,""",,"- The abstract mentions that experiments were conducted on the ""CLEF 2019 eHealth Task 2 TAR dataset."" This indicates that the CLEF 2019 eHealth Task 2 TAR dataset was used as a data source for the study.
- There is no mention of academic databases such as Scopus or Web of Science, nor is there any information about dataset sizes, characteristics, data collection methods, training/validation/test data splits, data preprocessing and cleaning approaches, or external knowledge bases or ontologies used.
- The focus of the abstract is on the methodological approach and the results, rather than detailed information about data sources or preprocessing.","  - ""When medical researchers conduct a systematic review (SR), screening studies is the most time-consuming process: researchers read several thousands of medical literature and manually label them relevant or irrelevant.""
  - ""Screening prioritization (ie., document ranking) is an approach for assisting researchers by providing document rankings where relevant documents are ranked higher than irrelevant ones.""
  - ""We propose a document matching measure named Mirror Matching, which calculates matching scores between medical abstract texts by incorporating common writing patterns, such as background, method, result, and conclusion in order.""
  - ""We conduct experiments on CLEF 2019 eHealth Task 2 TAR dataset, and the empirical results show this simple approach achieves the higher performance than traditional and neural retrieval models on Average Precision and Precision-focused metrics.""
  - ""Seed-driven document ranking (SDR) uses a known relevant document (ie., seed) as a query and generates such rankings.""",,"- The target research discipline or field is medical research, as indicated by the focus on ""medical researchers"" and ""medical literature.""
- The specific literature review task addressed is the screening process in systematic reviews, which involves identifying relevant documents.
- The types of academic documents processed are medical abstract texts, as the Mirror Matching measure is designed to calculate matching scores between these texts.
- The user types are medical researchers conducting systematic reviews, who require assistance in prioritizing documents.
- The integration with research workflows is through the use of seed-driven document ranking to provide document rankings, which helps in the screening process.
- The application is academic, as it is designed to assist researchers in conducting systematic reviews, which is a common task in academic research."
A Systematic Review of Citation Recommendation Over the Past Two Decades,"Yicong Liang, Lap-Kei Lee",10.4018/ijswis.324071,https://doi.org/10.4018/ijswis.324071,International Journal on Semantic Web and Information Systems (IJSWIS),2,2023,"- Main AI/ML frameworks used: Deep learning, collaborative filtering, topic modeling, GANs, adversarial autoencoders, NLP.
- System architecture components: Learning document representations, measuring relatedness between query manuscripts and recommended candidates.
- Technical infrastructure: Implied use of computational resources for deep learning and NLP tasks.
- Integration with existing academic databases: Use of various datasets such as AAN, DBLP, PubMed.
- Overall system design approach: Combination of traditional machine learning methods and recent deep learning techniques.","- Algorithms and models used: Collaborative filtering (CF), singular value decomposition (SVD), graph-based methods, generative adversarial networks (GANs), adversarial autoencoders.
- Feature extraction techniques: Integrating text features with citation features, using topic models like LDA, incorporating content with graph structure.
- Data processing pipelines: Constructing citation networks, using random walk processes for ranking.
- Training methodologies: Using GANs and adversarial autoencoders for personalized recommendations.
- Preprocessing steps: Filtering articles to focus on citation recommendation.
- Technical workflow or methodology: Classifying global citation approaches into categories like CF, feature-based, topic model-based, graph-based, and deep learning-based methods.","- Search and retrieval algorithms: Collaborative filtering (CF), singular value decomposition (SVD)
- Relevance ranking approaches: Random walk-based scoring function, meta-path-based ranking functions
- Citation analysis methods: Clustering citations into interest groups, using citation relations to capture topic distributions
- Content-based matching techniques: Integration of text features with citation features
- Metadata extraction and utilization: Use of metadata in content-based methods
- Recommendation system approaches: Graph-based methods, deep learning methods (GANs, adversarial autoencoders)
- Expert/authority identification methods: Not explicitly mentioned","- Algorithm performance issues: Collaborative filtering (CF) techniques have limitations as citing papers cannot add more votes after the citation network is constructed.
- Data quality or availability problems: Large bibliographic networks lead to high computational complexity and data sparsity issues.
- Scalability challenges: Graph-based methods suffer from data sparsity and high computational complexity.
- Integration difficulties: Transferring citation data into sequential data for deep learning models like Transformer is challenging.
- Evaluation methodology limitations: Offline evaluations focus on older references, which may not accurately reflect real-world scenarios.
- Computational resource constraints: High computational complexity due to large graph sizes.
- Technical bottlenecks: Difficulty in applying deep learning models like Transformer due to the need for sequential data.","- New algorithmic contributions: Collaborative filtering (CF), singular value decomposition (SVD)
- Creative problem-solving methods: Integration of text features with citation features, topic models
- Hybrid or ensemble approaches: Combining topic models with matrix factorization, generative adversarial networks (GANs)
- Novel feature engineering techniques: Meta-paths, heterogeneous networks
- Innovative evaluation metrics: Co-cited probability
- Technical workarounds for known problems: Artificially adding links between papers to address data sparsity
- Original system design elements: Adversarial autoencoders for multi-modal recommendation tasks","- Evaluation metrics: Precision, recall, F1-score, NDCG, MAP, MRR
- Most frequently used metrics: Recall, NDCG, MAP
- Evaluation process: Offline evaluation using paper's reference list as ground truth
- Limitation: Offline evaluation favors older publications
- Suggested improvement: Online evaluation using click-through rate (CTR)
- No specific quantitative performance results or comparisons with baseline methods
- No information on processing speed, user satisfaction, system reliability, or scalability","- Academic databases used: Google Scholar, ACM Digital Library, ScienceDirect, IEEE Xplore
- Dataset sizes and characteristics: ACL Anthology Network (AAN), DBLP, ACM Digital Library, Wangfang (non-English)
- Data collection methods: Two-step literature search
- Training/validation/test data splits: Not explicitly mentioned
- Data preprocessing and cleaning approaches: Not explicitly mentioned
- External knowledge bases or ontologies used: Not explicitly mentioned","- Target research disciplines or fields: AI education systems
- Specific literature review tasks addressed: Identifying relevant papers and literature for academic writing, particularly for research proposals
- Types of academic documents processed: Research papers and articles
- User types and requirements: Expert researchers and non-expert newcomers (early-stage master's and Ph.D. students) seeking personalized citation recommendations
- Integration with research workflows: Provides personalized citation recommendations based on user profiles and query manuscripts
- Commercial vs. academic applications: Focuses on academic applications","  - ""This article provides a systematic review on global citation recommendation models and compares the reviewed methods from the traditional topic- based models to the recent models embedded with deep neural networks,""
  - ""the present survey aims to review citation recommendation models, and the reviewed algorithms in this work include representation learning and deep learning-based novel methods in recent years.""
  - ""The collaborative filtering (CF) technique has been widely and successfully applied in many recommender systems.""
  - ""The CF framework has the advantage of lightweight architecture since only the ID information and interaction history are utilized.""
  - ""Caragea et al. (2013) addressed the citation recommendation problem by proposing a singular value decomposition (SVD) approach""
  - ""Strohman et al. (2007) integrated text features with citation features to rank scientific articles for given a query paper,""
  - ""Bethard and Jurafsky (2010) introduced sixteen features including different citation behaviors in the document retrieval model,""
  - ""Ren et al. (2014) noted that citations tend to be clustered into different interest groups in a heterogeneous bibliographic network""
  - ""Gupta and Varma (2017) incorporated content with graph structure to learn the representation of papers for citation recommendation.""
  - ""Dai et al. (2018a) proposed an LDA-based probabilistic topic model TMALCCite that combined topic-model and matrix factorization to extract semantic topics and qualified author communities.""
  - ""Cai et al. (2018a) proposed the model GAN-HBNR, which exploited a generative adversarial network (GAN) to obtain low dimensional embedding of different objects in a heterogeneous bibliographic network""
  - ""Zhang et al. ( 2018) proposed a GAN-based model method for personalized citation recommendation.""
  - ""Galke et al. (2018) utilized adversarial autoencoders for multi-modal recommendation tasks in terms of recommending citations and tags.""
  - ""Deep learning techniques including MLP, CNN, and RNN, which are capable of capturing the semantic representations and associated contextual information of research papers, have been applied in citation recommendations""
  - ""The deep neural network model Transformer (Vaswani et al., 2017) has achieved state-of-the-art status in some tasks related to natural language processing (NLP) and computer vision (CV), but Transformer is still not applied to citation recommendation problems.""
  - ""The mainstream of the citation recommendation mechanism is twofold: learning document representations from the training corpus and measuring the relatedness of the embeddings between query manuscript and recommended candidate.""
  - ""The possible datasets and evaluation metrics for evaluating citation recommendation performance were identified.""
  - ""Bibliometric analysis in the field of citation recommendation was also performed to identify productive researchers and their affiliations.""
  - ""The challenges in the area of citation recommender systems were outlined and discussed in terms of evaluation framework selection, user profile construction, and learning network representation.""","  - ""(Page 20, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 20, Table 2) | Paper                  | Venue          | Approach      | Score function                     |\n|------------------------|----------------|---------------|------------------------------------|\n| (Cai et al., 2018a)    | AAAI           | Graph-based   | Cos-sim                            |""
  - ""(Page 21, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 21, Table 2) | Paper                  | Datasets                        | Evaluation metric               | Information used                                           |\n|------------------------|---------------------------------|---------------------------------|------------------------------------------------------------|\n| (Cai et al., 2018a)    | AAN, DBLP                       | recall, MAP, MRR                | Title, abstract, citation network                          |""","- The paper discusses various AI/ML frameworks used in citation recommendation, including deep learning, collaborative filtering, and topic modeling.
- Deep learning techniques such as GANs and adversarial autoencoders are mentioned as part of the technical architecture.
- The paper highlights the use of natural language processing (NLP) techniques for text analysis and semantic representation.
- The system architecture involves learning document representations and measuring relatedness between query manuscripts and recommended candidates.
- The paper mentions the use of various datasets and evaluation metrics, indicating integration with existing academic databases.
- The technical infrastructure is not explicitly mentioned, but the use of computational resources for deep learning and NLP tasks is implied.
- The overall system design approach involves a combination of traditional machine learning methods and recent deep learning techniques.","  - ""The collaborative filtering (CF) technique has been widely and successfully applied in many recommender systems.""
  - ""McNee et al. (2002) first introduced the CF framework by transferring the citation network into the user-item rating matrix.""
  - ""Caragea et al. (2013) addressed the citation recommendation problem by proposing a singular value decomposition (SVD) approach""
  - ""Strohman et al. (2007) integrated text features with citation features to rank scientific articles for given a query paper,""
  - ""Bethard and Jurafsky (2010) introduced sixteen features including different citation behaviors in the document retrieval model,""
  - ""Ren et al. (2014) noted that citations tend to be clustered into different interest groups in a heterogeneous bibliographic network""
  - ""Gupta and Varma (2017) incorporated content with graph structure to learn the representation of papers for citation recommendation.""
  - ""Dai et al. (2018a) proposed an LDA-based probabilistic topic model TMALCCite that combined topic-model and matrix factorization""
  - ""Dai et al. (2018b) proposed the method TopicCite by jointly combining feature regression with topic models for citation recommendation and topic extraction.""
  - ""Academic papers are intrinsically connected in a citation network via their citation relations; the random walk process plays an important role for ranking purposes in graph-based methods.""
  - ""Liu et al. (2014b) proposed a context-rich heterogeneous network approach by characterizing the importance of citation relationships and topical citation motivation.""
  - ""Cai et al. (2018a) proposed the model GAN-HBNR, which exploited a generative adversarial network (GAN) to obtain low dimensional embedding of different objects in a heterogeneous bibliographic network""
  - ""Zhang et al. ( 2018) proposed a GAN-based model method for personalized citation recommendation.""
  - ""Galke et al. (2018) utilized adversarial autoencoders for multi-modal recommendation tasks in terms of recommending citations and tags.""
  - ""Guo et al. (2022) proposed their model CSCR to artificially add links between two papers if they had similar content,""","  - ""(Page 20, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 20, Table 2) | Paper                  | Venue          | Approach      | Score function                     |\n|------------------------|----------------|---------------|------------------------------------|\n| (Cai et al., 2018a)    | AAAI           | Graph-based   | Cos-sim                            |""
  - ""(Page 21, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 21, Table 2) | Paper                  | Datasets                        | Evaluation metric               | Information used                                           |\n|------------------------|---------------------------------|---------------------------------|------------------------------------------------------------|\n| (Cai et al., 2018a)    | AAN, DBLP                       | recall, MAP, MRR                | Title, abstract, citation network                          |""","- The paper discusses various algorithms and models used in citation recommendation, such as collaborative filtering (CF), singular value decomposition (SVD), and graph-based methods.
- Feature extraction techniques include integrating text features with citation features, using topic models like LDA, and incorporating content with graph structure.
- Data processing pipelines involve constructing citation networks and using random walk processes for ranking.
- Training methodologies include using generative adversarial networks (GANs) and adversarial autoencoders for personalized recommendations.
- Preprocessing steps involve filtering articles to focus on citation recommendation rather than paper recommendation.
- Technical workflow or methodology includes classifying global citation approaches into categories like CF, feature-based, topic model-based, graph-based, and deep learning-based methods.","  - ""The traditional process of finding relevant citations requires much tedious and manual work.""
  - ""The motivation for citation recommendation is to provide an efficient method for the citation process.""
  - ""Citation recommendation can be classified into local and global citation recommendation depending on whether a specific local citation context is given;""
  - ""The collaborative filtering (CF) technique has been widely and successfully applied in many recommender systems.""
  - ""The CF framework has the advantage of lightweight architecture since only the ID information and interaction history are utilized.""
  - ""Caragea et al. (2013) addressed the citation recommendation problem by proposing a singular value decomposition (SVD) approach""
  - ""Strohman et al. (2007) integrated text features with citation features to rank scientific articles for given a query paper,""
  - ""Bethard and Jurafsky (2010) introduced sixteen features including different citation behaviors in the document retrieval model,""
  - ""Ren et al. (2014) noted that citations tend to be clustered into different interest groups in a heterogeneous bibliographic network""
  - ""Gupta and Varma (2017) incorporated content with graph structure to learn the representation of papers for citation recommendation.""
  - ""Nallapati et al. (2008) argued that the explicit document dependency (i.e., citation relation) better captured the topic distributions of documents""
  - ""Jiang et al. (2014) proposed a chronological recommendation model DTCIM for users to generate citation recommendations shown in different time slices.""
  - ""Dai et al. (2018a) proposed an LDA-based probabilistic topic model TMALCCite that combined topic-model and matrix factorization to extract semantic topics and qualified author communities.""
  - ""Academic papers are intrinsically connected in a citation network via their citation relations; the random walk process plays an important role for ranking purposes in graph-based methods.""
  - ""Many existing techniques for citation recommendation only focus on suggesting prestigious and well-cited articles.""
  - ""Chakraborty et al. (2015) were the first to address the diversity problem, and the proposed model DiSCern finds relevant and diversified citations in response to a search query.""
  - ""Jiang et al. (2018) addressed the problem of cross-language citation recommendation (CCR) that, given a query paper in a source language, the system output a list of ranked papers in the target language-i.e.,""
  - ""Cai et al. (2018a) proposed the model GAN-HBNR, which exploited a generative adversarial network (GAN) to obtain low dimensional embedding of different objects in a heterogeneous bibliographic network""
  - ""Zhang et al. ( 2018) proposed a GAN-based model method for personalized citation recommendation.""
  - ""Galke et al. (2018) utilized adversarial autoencoders for multi-modal recommendation tasks in terms of recommending citations and tags.""
  - ""Graph-based methods are based on an adjacency matrix and suffer from data sparsity problems in large-scale bibliographic networks.""
  - ""Guo et al. (2022) proposed their model CSCR to artificially add links between two papers if they had similar content,""
  - ""The mainstream of the citation recommendation mechanism is twofold: learning document representations from the training corpus and measuring the relatedness of the embeddings between query manuscript and recommended candidate.""","  - ""(Page 20, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 20, Table 2) | Paper                  | Venue          | Approach      | Score function                     |\n|------------------------|----------------|---------------|------------------------------------|\n| (Cai et al., 2018a)    | AAAI           | Graph-based   | Cos-sim                            |""
  - ""(Page 21, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 21, Table 2) | Paper                  | Datasets                        | Evaluation metric               | Information used                                           |\n|------------------------|---------------------------------|---------------------------------|------------------------------------------------------------|\n| (Cai et al., 2018a)    | AAN, DBLP                       | recall, MAP, MRR                | Title, abstract, citation network                          |""","- The paper discusses various techniques for citation recommendation, including collaborative filtering (CF), singular value decomposition (SVD), and graph-based methods.
- It mentions the use of text features and citation features for ranking scientific articles, indicating content-based matching techniques.
- The paper highlights the importance of citation analysis methods, such as clustering citations into interest groups and using citation relations to capture topic distributions.
- It discusses the use of random walk processes and meta-path-based ranking functions in graph-based methods.
- The paper also covers deep learning methods, including generative adversarial networks (GANs) and adversarial autoencoders, for personalized citation recommendation.
- The tables at the end of the paper provide a comprehensive list of papers and their corresponding approaches, score functions, and datasets used, which are relevant to the techniques for finding and matching academic papers.","  - ""the major drawback of applying the CF technique in citation recommendation is that the ""user"" (citing paper) will never add more votes (cited papers) after the citation network is constructed.""
  - ""Computation towards the entire graph might introduce a high computational complexity due to the huge graph size.""
  - ""Graph-based methods are based on an adjacency matrix and suffer from data sparsity problems in large-scale bibliographic networks.""
  - ""Most of the reviewed citation recommendation models conducted offline evaluations that original references of papers were considered as ground truth. The major drawback of this strategy is that the published date of the original references is earlier than the citing paper, which guides the system to find ""old"" candidates.""
  - ""The mainstream of the citation recommendation mechanism is twofold: learning document representations from the training corpus and measuring the relatedness of the embeddings between query manuscript and recommended candidate. As academic papers lie in a heterogeneous information network, the quality of network representation learning including article embedding is crucial to find relevant citation candidates.""
  - ""The major problem is how to transfer citation data into sequential data (e.g., extracting sequences with citation behavior patterns). With such training data, Transformer with a self-attention mechanism can be applied to learn the network representation.""
  - ""The major drawback of this strategy is that the published date of the original references is earlier than the citing paper, which guides the system to find ""old"" candidates.""",,"- The paper discusses several technical challenges in citation recommendation systems. One challenge is related to the performance of algorithms, particularly with collaborative filtering (CF) techniques, where the citing paper cannot add more votes after the citation network is constructed.
- Data quality and availability are issues due to the large size of bibliographic networks, which can lead to high computational complexity and data sparsity problems.
- Scalability is a challenge because graph-based methods suffer from data sparsity and high computational complexity.
- Integration difficulties are implied by the need to transfer citation data into sequential data for deep learning models like Transformer.
- Evaluation methodology limitations are highlighted by the reliance on offline evaluations, which may not accurately reflect real-world scenarios due to the focus on older references.
- Computational resource constraints are mentioned in terms of the high computational complexity of processing large graphs.
- Technical bottlenecks include the difficulty in applying deep learning models like Transformer due to the need for sequential data.","  - ""the present survey focuses on citation recommendation.""
  - ""the reviewed algorithms in this work include representation learning and deep learning-based novel methods in recent years.""
  - ""The collaborative filtering (CF) technique has been widely and successfully applied in many recommender systems.""
  - ""Caragea et al. (2013) addressed the citation recommendation problem by proposing a singular value decomposition (SVD) approach""
  - ""Strohman et al. (2007) integrated text features with citation features to rank scientific articles for given a query paper,""
  - ""Bethard and Jurafsky (2010) introduced sixteen features including different citation behaviors in the document retrieval model,""
  - ""Ren et al. (2014) noted that citations tend to be clustered into different interest groups in a heterogeneous bibliographic network""
  - ""Gupta and Varma (2017) incorporated content with graph structure to learn the representation of papers for citation recommendation.""
  - ""Dai et al. (2018a) proposed an LDA-based probabilistic topic model TMALCCite that combined topic-model and matrix factorization""
  - ""Liu et al. (2014b) proposed a context-rich heterogeneous network approach by characterizing the importance of citation relationships and topical citation motivation.""
  - ""Jiang et al. (2015) employed a supervised document influence model to extract the content time-varying dynamics and applied multiple meta-paths for ranking recommended articles.""
  - ""Jiang et al. (2018) addressed the problem of cross-language citation recommendation (CCR)""
  - ""Cai et al. (2018a) proposed the model GAN-HBNR, which exploited a generative adversarial network (GAN) to obtain low dimensional embedding of different objects in a heterogeneous bibliographic network""
  - ""Zhang et al. ( 2018) proposed a GAN-based model method for personalized citation recommendation.""
  - ""Galke et al. (2018) utilized adversarial autoencoders for multi-modal recommendation tasks in terms of recommending citations and tags.""
  - ""Guo et al. (2022) proposed their model CSCR to artificially add links between two papers if they had similar content,""","  - ""(Page 20, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 20, Table 2) | Paper                  | Venue          | Approach      | Score function                     |\n|------------------------|----------------|---------------|------------------------------------|\n| (Cai et al., 2018a)    | AAAI           | Graph-based   | Cos-sim                            |""
  - ""(Page 21, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 21, Table 2) | Paper                  | Datasets                        | Evaluation metric               | Information used                                           |\n|------------------------|---------------------------------|---------------------------------|------------------------------------------------------------|\n| (Cai et al., 2018a)    | AAN, DBLP                       | recall, MAP, MRR                | Title, abstract, citation network                          |""","- The paper discusses various innovative solutions in citation recommendation, including the use of deep learning techniques, graph-based methods, and hybrid approaches.
- The use of collaborative filtering (CF) and singular value decomposition (SVD) are mentioned as novel algorithmic contributions.
- The integration of text features with citation features and the use of topic models are examples of creative problem-solving methods.
- Hybrid approaches such as combining topic models with matrix factorization and using generative adversarial networks (GANs) are highlighted.
- Novel feature engineering techniques include the use of meta-paths and heterogeneous networks to capture citation relationships.
- Innovative evaluation metrics such as co-cited probability are discussed.
- Technical workarounds for known problems include addressing data sparsity issues by artificially adding links between papers.
- Original system design elements include the use of adversarial autoencoders for multi-modal recommendation tasks.","  - ""The popular evaluation metrics in the field of recommender systems can also be transferred into evaluating citation re-prediction tasks, including precision, recall, F 1 score, normalized discounted cumulative gain (NDCG), mean average precision (MAP), and mean reciprocal rank (MRR).""
  - ""Recall, NDCG, and MAP were the top 3 popular metrics for evaluating citation recommendation performance. Both recall and NDCG were used in 14 evaluations (52%); MAP in 13 evaluations (48%); precision in 9 evaluations (33%); MRR in 8 evaluations (30%); F 1 and co-cited probability in 2 and 1 evaluations.""
  - ""The evaluation metrics used in the reviewed papers are listed in Table A3 and Table A4 in the Appendix section.""
  - ""the paper's reference list is treated as ground truth during the offline evaluation. Specifically, for a testing citing paper, a subset of its cited papers are randomly selected for holdout and are treated as the ground truth for comparison.""
  - ""The proposed approach is evaluated by assessing which of the citations that have been recommended by the system are also in the original publications.""
  - ""The major drawback of this strategy is that the published date of the original references is earlier than the citing paper, which guides the system to find ""old"" candidates.""
  - ""Most of the reviewed citation recommendation models conducted offline evaluations that original references of papers were considered as ground truth.""
  - ""Online evaluations in terms of click-through rate (CTR) is widely used in e-commerce recommender system (Beel et al., 2016).""
  - ""the system should record the ratio of co-clicked or downloaded articles to the number of articles displayed.""
  - ""The mainstream of the citation recommendation mechanism is twofold: learning document representations from the training corpus and measuring the relatedness of the embeddings between query manuscript and recommended candidate.""
  - ""Deep learning techniques including MLP, CNN, and RNN, which are capable of capturing the semantic representations and associated contextual information of research papers, have been applied in citation recommendations (Ali et al., 2020).""
  - ""The deep neural network model Transformer (Vaswani et al., 2017) has achieved state-of-the-art status in some tasks related to natural language processing (NLP) and computer vision (CV), but Transformer is still not applied to citation recommendation problems.""",,"- The paper discusses various evaluation metrics used in citation recommendation systems, including precision, recall, F1-score, NDCG, MAP, and MRR. These metrics are commonly used to assess the performance of recommender systems.
- The paper mentions that recall, NDCG, and MAP are the most frequently used metrics, indicating their importance in evaluating citation recommendation performance.
- The evaluation process involves treating the paper's reference list as ground truth and comparing recommended citations against this list.
- The paper highlights the limitation of offline evaluation methods, which may favor older publications, and suggests the need for online evaluation methods like click-through rate (CTR).
- The paper does not provide specific quantitative performance results or comparisons with baseline methods. Instead, it focuses on the methodologies and challenges in citation recommendation.
- There is no mention of processing speed and efficiency metrics, user satisfaction or usability results, system reliability and robustness measures, or scalability test results in the provided text.","  - ""To identify relevant literature for this survey, we conducted a two-step literature search on Google Scholar, ACM Digital Library, ScienceDirect, and IEEE Xplore.""
  - ""Finally, 27 approaches in total were chosen for an in-depth analysis as shown in Table A1 and Table A2 in the Appendix section.""
  - ""The datasets in these reviewed papers are shown in Table A3 and A4 in the Appendix section.""
  - ""The data from ACL Anthology Network (AAN) was used in 12 works for evaluation (44%). DBLP and ACM Digital Library were two other popular datasets used in 9 and 6 citation recommendation experiments, respectively (33% and 22%).""
  - ""These three datasets provide citation network information which makes it convenient for researchers to set up the ground truth for offline evaluations. Also, it contains meta-information that helps to build representation for each paper (e.g., titles, abstracts, authors, and venues).""
  - ""Wangfang is the only one non-English dataset for citation recommendation and it is adopted to evaluate a cross-lingual citation recommendation model (Jiang et al., 2018) between Chinese and English.""
  - ""Citation recommendation models employ different data factors and features including title, abstract, keywords, author information, venue information, and citation network.""
  - ""The features of the author and venue are combined in heterogeneous network-based models where multiple types of vertices and relations are leveraged in the network representation.""
  - ""Precision and recall are two commonly used evaluation metrics in the field of information retrieval.""
  - ""The evaluation metrics used in the reviewed papers are listed in Table A3 and Table A4 in the Appendix section.""","  - ""(Page 21, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 21, Table 2) | Paper                  | Datasets                        | Evaluation metric               | Information used                                           |\n|------------------------|---------------------------------|---------------------------------|------------------------------------------------------------|\n| (Cai et al., 2018a)    | AAN, DBLP                       | recall, MAP, MRR                | Title, abstract, citation network                          |""","- The paper mentions that a two-step literature search was conducted using Google Scholar, ACM Digital Library, ScienceDirect, and IEEE Xplore to identify relevant literature. This indicates the academic databases used for data collection.
- The paper references the use of specific datasets such as ACL Anthology Network (AAN), DBLP, and ACM Digital Library for evaluation purposes. These are the primary data sources mentioned.
- The datasets are used for offline evaluations, and they provide citation network information and meta-information like titles, abstracts, authors, and venues.
- The paper also mentions Wangfang as a non-English dataset used for cross-lingual citation recommendation.
- The features used from these datasets include title, abstract, keywords, author information, venue information, and citation network, which are typical characteristics of academic datasets.
- The tables referenced (Table A3 and Table A4) likely contain detailed information about the datasets used in the reviewed papers, but the content is not explicitly quoted here.","  - ""A citation is a reference to the source of information used in an article. Citations are very useful for students and researchers to locate relevant information on a topic.""
  - ""Citation recommendation can be classified into local and global citation recommendation depending on whether a specific local citation context is given; e.g., the text surrounding a citation placeholder.""
  - ""This article provides a systematic review on global citation recommendation models and compares the reviewed methods from the traditional topic- based models to the recent models embedded with deep neural networks, aiming to summarize this field to facilitate researchers working on citation recommendation.""
  - ""The problem of citation recommendation can be further split into local (context-aware) and global (non-context-aware) citation recommendation""
  - ""Citation recommender systems help those novice researchers to find cite-worthy publications to write their research proposals.""
  - ""The mainstream of the citation recommendation mechanism is twofold: learning document representations from the training corpus and measuring the relatedness of the embeddings between query manuscript and recommended candidate.""
  - ""Citation recommendation plays an important role in the context of scientific big data to assist students and researchers to identify relevant papers and literature for academic writing.""
  - ""This paper presented a brief and systematic review related to global citation recommendation models over the past two decades.""
  - ""The authors discussed the benefits of citation recommender systems and the problem formulation of citation recommendation.""
  - ""the threefold challenges were outlined in the field of citation recommender systems.""","  - ""(Page 3, Table 1) | Notations   | Descriptions                                                                                     |\n|-------------|--------------------------------------------------------------------------------------------------|\n| \( q \)     | an input query manuscript                                                                        |""
  - ""(Page 14, Table 1) | Author         | Papers |\n|----------------|--------|\n| Xiaoyan Cai    | 10     |""
  - ""(Page 14, Table 2) | Affiliation                        | Count |\n|------------------------------------|-------|\n| Northwestern Polytechnical Univ    | 29    |""
  - ""(Page 20, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 20, Table 2) | Paper                  | Venue          | Approach      | Score function                     |\n|------------------------|----------------|---------------|------------------------------------|\n| (Cai et al., 2018a)    | AAAI           | Graph-based   | Cos-sim                            |""
  - ""(Page 21, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 21, Table 2) | Paper                  | Datasets                        | Evaluation metric               | Information used                                           |\n|------------------------|---------------------------------|---------------------------------|------------------------------------------------------------|\n| (Cai et al., 2018a)    | AAN, DBLP                       | recall, MAP, MRR                | Title, abstract, citation network                          |""","- The paper focuses on global citation recommendation models, which are designed to assist researchers and students in finding relevant citations for academic writing.
- The target research disciplines or fields are not explicitly limited to specific areas, but the paper mentions AI education systems as an example.
- The specific literature review tasks addressed include identifying relevant papers and literature for academic writing, particularly for research proposals.
- The types of academic documents processed are primarily research papers and articles.
- The user types include expert researchers and non-expert newcomers such as early-stage master's and Ph.D. students.
- The system is designed to integrate with research workflows by providing personalized citation recommendations based on user profiles and query manuscripts.
- The paper does not explicitly mention commercial applications, focusing instead on academic applications."
Mapping Artificial Intelligence in Education Research: a Network‐based Keyword Analysis,"Shihui Feng, N. Law",10.1007/s40593-021-00244-4,https://doi.org/10.1007/s40593-021-00244-4,International Journal of Artificial Intelligence in Education,52,2021,"- Main AI/ML frameworks used: Natural language processing, educational data mining, learning analytics, machine learning, deep learning, neural networks.
- System architecture components: Not mentioned.
- Technical infrastructure: Not mentioned.
- Integration with existing academic databases or platforms: Not mentioned.
- Overall system design approach: Not mentioned.","- Algorithms and models used: Fast greedy modularity algorithm for community detection, weighted betweenness centrality for identifying bridging keywords.
- Feature extraction techniques: Keyword co-occurrence networks (KCNs) analysis.
- Data processing pipelines: Three-step approach at macro, meso, and micro levels.
- Training methodologies: Not explicitly mentioned.
- Preprocessing steps: Standardizing keywords by converting plurals to singular form, removing hyphens, converting to lowercase, avoiding abbreviations, and removing redundant words.
- Technical workflow or methodology: Constructing KCNs, analyzing network structural features, detecting communities, and identifying trending keywords.","- Content-based matching techniques: Keyword co-occurrence networks (KCNs) to identify knowledge structure, clusters, and trending keywords.
- Relevance ranking approaches: Three-step approach (macro, meso, micro) to analyze network-level distribution, links between keywords, and node-level analysis.
- Citation analysis methods: Fast greedy modularity algorithm for community detection.","- Data quality or availability problems: Handling large volumes of learning data.
- Integration difficulties: Challenges in keyword selection due to interchangeable words.
- Algorithm performance issues: Negative degree correlation coefficients indicating disassortative tendency.
- Scalability challenges: Enhanced connectivity over time due to high degree 'bridge' nodes.
- Computational resource constraints: Need for novel methods beyond traditional approaches.
- Technical bottlenecks: Limitations in current data availability or integration.","- Novel three-step approach for analyzing keyword co-occurrence networks (KCNs)
- Multi-scale framework (macro, meso, micro) for comprehensive analysis
- Use of fast greedy modularity algorithm for community detection
- Application of weighted betweenness centrality to identify trending and bridging keywords",Not mentioned (the paper does not provide quantitative or qualitative performance outcomes),"- Academic databases used: AIEDC, IJAIED, EDM, L@S, ITS
- Dataset size and characteristics: 1830 articles from 2010 to 2019
- Data collection methods: Collected all available articles from the five sources
- Data preprocessing and cleaning approaches: Standardized keywords by converting plurals to singular, removing hyphens, converting to lowercase, avoiding abbreviations, and removing redundant words
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: Artificial Intelligence in Education (AIED)
- Specific literature review tasks addressed: Analyzing knowledge structure, knowledge clusters, and trending keywords
- Types of academic documents processed: Research articles from five prominent AIED publication sources
- User types and requirements: Researchers and educators interested in AIED
- Integration with research workflows: Analysis of keyword co-occurrence networks
- Commercial vs. academic applications: Academic","  - ""The highly connected keywords relevant to analytic techniques within this field include natural language processing, educational data mining, learning analytics and machine learning.""
  - ""the technical foci of AIED, natural language processing, educational data mining, learning analytics, and machine learning are the highly influential keywords in the field based on the degree and betweenness centrality.""
  - ""Machine learning is a popular topic in the field currently and it will remain influential, along with the other advanced analytic techniques including deep learning, neural network, and eye tracking that have a higher chance to gain more popularity in the coming years.""
  - ""Neural network, deep learning, eye tracking, and personalized learning are trending keywords in this field as they have emerged with key structural roles in the latest two-year period analyzed.""",,"- The paper discusses the use of various AI/ML frameworks such as natural language processing, educational data mining, learning analytics, and machine learning, which are central to the technical architecture of AIED.
- Deep learning and neural networks are highlighted as emerging trends, indicating their integration into the technical architecture.
- The paper does not provide specific details about system architecture components like databases, APIs, or interfaces, nor does it mention technical infrastructure such as cloud platforms or computational resources.
- There is no mention of integration with existing academic databases or platforms, nor is there a detailed description of the overall system design approach.
- The focus is on the analytical techniques and AI/ML frameworks rather than the technical infrastructure or system components.","  - ""A novel three-step approach in the analysis of the keyword co-occurrence networks (KCN) is proposed to identify the knowledge structure, knowledge clusters and trending keywords within AIED over time.""
  - ""A static keyword cooccurrence network (KCN) is constructed based on 1830 articles in the field of AIED from 2010 to 2019, and additionally, five temporal KCNs are constructed based on a two-year time window to analyze the knowledge evolution of the field during this most recent period of interest.""
  - ""A novel three-step approach is proposed to analyze these KCNs at macro-, meso-and micro-levels.""
  - ""The network structural features, communities, and keywords at critical structural positions are analyzed, guided by three research questions: (1) What is the underlying pattern of knowledge structure of AIED? (2) What are the primary knowledge clusters within AIED during the last decade (2010-2019)? (3) What are the trending keywords within AIED?""
  - ""A fast greedy modularity algorithm (Clauset et al., 2004) is employed to cluster nodes into groups based on their structural cohesiveness.""
  - ""The weighted betweenness centrality of a node measures the fraction of shortest weighted paths between all pairs of nodes that pass through the focal node in a network (Freeman, 1977).""
  - ""Community detection is an effective approach to decompose a large-scale network into a number of groups (or ""communities"") such that nodes are more densely connected within groups than between groups.""
  - ""Clauset et al. (2004) proposed a hierarchical agglomeration algorithm for detecting community structure in large scale networks efficiently by optimizing the global modularity.""
  - ""The strategies used to standardize the keywords are presented below:

1) Converting plurals into a singular form 2) Removing hyphens: e.g., eye-trackingeye tracking 3) Converting to lowercase 4) Avoidance of abbreviations 5) Removing redundant words: e.g., massive open online course (moocs)massive open online course""
  - ""A three-step approach is proposed to derive meaningful insights from the constructed KCNs at macro-, meso-, and micro-levels.""
  - ""The macro-level analysis aims to analyze the network-level distribution of the keywords regarding their structural importance and popularity within the field, which can help us to understand the structure of knowledge organization in AIED.""
  - ""The meso-level analysis focuses on analyzing the links between keywords in KCNs and aims to detect the highly connected and cohesive knowledge clusters within AIED.""
  - ""The structural properties of individual keywords in a KCN gauge the importance of individual keywords in the knowledge space of a field.""
  - ""The local clustering coefficient measures the cohesiveness of the neighbors of each node.""
  - ""The weighted betweenness centrality of a node measures the fraction of shortest weighted paths between all pairs of nodes that pass through the focal node in a network""
  - ""The fast-greedy modularity clustering algorithm (Clauset et al., 2004) is applied to the largest connected component of each KCN, as the largest connected component retains the primary structural information of a network and reduces the noise for community detection.""
  - ""The number of communities detected in each temporal network is based on the maximized modularity for the entire network, rather than a pre-determined value.""
  - ""The top five highest in-group degree keywords of seven selected knowledge clusters in each time period are presented in the Appendix Table 2""",,"- The paper uses a three-step approach to analyze keyword co-occurrence networks (KCNs) at macro, meso, and micro levels.
- The macro-level analysis involves examining network-level distributions of keywords to understand the structure of knowledge organization.
- The meso-level analysis uses a fast greedy modularity algorithm to detect cohesive knowledge clusters.
- The micro-level analysis identifies trending and bridging keywords based on node strength and weighted betweenness centrality.
- The paper employs network metrics such as average weighted nearest neighbor degree, weighted clustering coefficient, node strength, and weighted betweenness centrality.
- Preprocessing steps include standardizing keywords by converting plurals to singular form, removing hyphens, converting to lowercase, avoiding abbreviations, and removing redundant words.
- The paper uses community detection algorithms to identify knowledge clusters and trending keywords.","  - ""A novel three-step approach in the analysis of the keyword co-occurrence networks (KCN) is proposed to identify the knowledge structure, knowledge clusters and trending keywords within AIED over time.""
  - ""A static keyword cooccurrence network (KCN) is constructed based on 1830 articles in the field of AIED from 2010 to 2019, and additionally, five temporal KCNs are constructed based on a two-year time window to analyze the knowledge evolution of the field during this most recent period of interest.""
  - ""A KCN can be regarded as a type of information network that models the conceptual relationships between keywords within a research field.""
  - ""The topology of a KCN reflects the structure of knowledge in journal articles within a field""
  - ""The structural properties of individual keywords in a KCN gauge the importance of individual keywords in the knowledge space of a field.""
  - ""A three-step approach is proposed to derive meaningful insights from the constructed KCNs at macro-, meso-, and micro-levels.""
  - ""The macro-level analysis aims to analyze the network-level distribution of the keywords regarding their structural importance and popularity within the field,""
  - ""The meso-level analysis focuses on analyzing the links between keywords in KCNs and aims to detect the highly connected and cohesive knowledge clusters within AIED.""
  - ""A fast greedy modularity algorithm (Clauset et al., 2004) is employed to cluster nodes into groups based on their structural cohesiveness.""
  - ""Community detection is an effective approach to decompose a large-scale network into a number of groups (or ""communities"") such that nodes are more densely connected within groups than between groups.""
  - ""The network structural features, communities, and keywords at critical structural positions are analyzed, guided by three research questions: (1) What is the underlying pattern of knowledge structure of AIED? (2) What are the primary knowledge clusters within AIED during the last decade (2010-2019)? (3) What are the trending keywords within AIED?""
  - ""A descriptive analysis of the keyword frequency is conducted before the network analysis to provide an explicit overview of the popular keywords within the field.""
  - ""The findings of the study echo the future trends suggested by the previous researchers within AIED.""",,"- The paper uses a network-based approach to analyze keyword co-occurrence networks (KCNs) to identify knowledge structure, clusters, and trending keywords. This is a form of content-based matching technique.
- The three-step approach (macro, meso, micro) is used to analyze these networks, which involves examining network-level distribution, links between keywords, and node-level analysis. This is a form of relevance ranking approach.
- The use of a fast greedy modularity algorithm for community detection is a method for identifying clusters within the network, which can be seen as a form of citation analysis method.
- The paper does not explicitly mention search and retrieval algorithms, metadata extraction, or recommendation system approaches. However, the focus on keyword co-occurrence and network analysis suggests a content-based matching technique.
- The paper does not mention expert/authority identification methods or specific techniques for finding core papers beyond the network analysis.","  - ""The integration of digital technologies in learning activities has facilitated the accumulation of a vast amount of data on traceable learning behaviors and user-generated content. Consequently, a new line of research in AIED has developed to study how to utilize this data for enhancing the understanding of learning behaviors and the design of learning systems. This line of research requires novel methods, in particular those related to data mining, statistical analytics, and computational solutions beyond traditional quantitative and qualitative approaches for handling the large volumes of learning data.""
  - ""Due to its interdisciplinary nature, there are a large number of interchangeable words used in the AEID field. This poses a challenge for identifying appropriate keywords that can ensure precision and recall in retrieving the relevant articles from academic databases.""
  - ""On the other hand, subjective decisions on the selection of keywords could introduce bias into the dataset and affect the validity and reliability of the analysis results.""
  - ""The degree correlation coefficients of all networks are negative, suggesting a slightly disassortative tendency: keywords in the network tend to connect with others that have a dissimilar number of connections.""
  - ""The clustering coefficient of a single node gives the fraction of neighbors of the node that are connected, and the weighted clustering coefficient generalizes this concept to weighted networks by taking the edge weights between neighbors into consideration""
  - ""The weighted betweenness centrality of a node measures the fraction of shortest weighted paths between all pairs of nodes that pass through the focal node in a network""
  - ""Community detection is an effective approach to decompose a large-scale network into a number of groups (or ""communities"") such that nodes are more densely connected within groups than between groups.""
  - ""The portion of nodes in the largest component of the network is relatively low in the first time period and increases in the following time periods, which indicates that the connectivity of research topics in the field is enhanced over time, perhaps due to the increasing prevalence of high degree ""bridge"" nodes, such as ""massive open online course"".""
  - ""The hierarchical structure of the KCN indicates that research in AIED has some central foci around which there are a wide variety of research interests, and the underlying content of this global structure is further explored at smaller scales in the meso-and micro-level analysis.""
  - ""The findings of the study echo the future trends suggested by the previous researchers within AIED.""",,"- The paper discusses the challenge of handling large volumes of learning data, which implies data quality or availability problems and scalability challenges.
- The use of interchangeable words in the AIED field poses a challenge for keyword selection, which could introduce bias and affect analysis validity, indicating integration difficulties.
- The negative degree correlation coefficients suggest a disassortative tendency, which might indicate algorithm performance issues in terms of network analysis.
- The paper mentions the need for novel methods beyond traditional approaches, which could imply computational resource constraints or technical bottlenecks.
- The recommendation to include publications from other venues suggests limitations in current data availability or integration.","  - ""A novel three-step approach in the analysis of the keyword co-occurrence networks (KCN) is proposed to identify the knowledge structure, knowledge clusters and trending keywords within AIED over time.""
  - ""The three-step multi-scale (macro, meso, micro) framework proposed in this study can also be applied to map the knowledge development in other scientific research areas.""
  - ""A three-step approach is proposed to derive meaningful insights from the constructed KCNs at macro-, meso-, and micro-levels.""
  - ""The macro-level analysis aims to analyze the network-level distribution of the keywords regarding their structural importance and popularity within the field,""
  - ""The meso-level analysis focuses on analyzing the links between keywords in KCNs and aims to detect the highly connected and cohesive knowledge clusters within AIED.""
  - ""micro-level analysis is conducted at the node-level to identify the trending and bridging keywords (which could be obscured in the clustering analysis) in the field based on node strength and weighted betweenness centrality.""
  - ""A fast greedy modularity algorithm (Clauset et al., 2004) is employed to cluster nodes into groups based on their structural cohesiveness.""
  - ""The weighted betweenness centrality of a node measures the fraction of shortest weighted paths between all pairs of nodes that pass through the focal node in a network""
  - ""Community detection is an effective approach to decompose a large-scale network into a number of groups (or ""communities"") such that nodes are more densely connected within groups than between groups.""
  - ""the three-step analytic framework developed can be applied to map the knowledge development in other scientific research areas.""",,"- The paper introduces a novel three-step approach for analyzing keyword co-occurrence networks (KCNs), which is a technical innovation in the field of artificial intelligence in education (AIED).
- This approach is multi-scale, covering macro, meso, and micro levels, which allows for a comprehensive analysis of the knowledge structure and trending keywords.
- The use of a fast greedy modularity algorithm for community detection is a technical contribution, as it helps in identifying cohesive knowledge clusters.
- The application of weighted betweenness centrality to identify trending and bridging keywords is another technical innovation, as it provides insights into the connectivity and importance of keywords.
- The framework is designed to be applicable to other scientific research areas, indicating its potential as a generalizable technical solution.","  - ""The results reveal considerable research diversity in the AIED field, centering around two sustained themes: intelligent tutoring systems (2010-19) and massive open online courses (since 2014).""
  - ""The focal educational concerns reflected in AIED research are: (1) online learning; (2) game-based learning; (3) collaborative learning; (4) assessment; (5) affect; (6) engagement; and (7) learning design.""
  - ""The highly connected keywords relevant to analytic techniques within this field include natural language processing, educational data mining, learning analytics and machine learning.""
  - ""Neural network, deep learning, eye tracking, and personalized learning are trending keywords in this field as they have emerged with key structural roles in the latest two-year period analyzed.""
  - ""The three-step multi-scale (macro, meso, micro) framework proposed in this study can also be applied to map the knowledge development in other scientific research areas.""
  - ""The findings of the study echo the future trends suggested by the previous researchers within AIED.""
  - ""the three-step analytic framework developed can be applied to map the knowledge development in other scientific research areas.""
  - ""The top five highest in-group degree keywords of seven selected knowledge clusters in each time period are presented in the Appendix Table 2""","  - ""(Page 10, Table 1) |            |    n   |    m    |    d    |   c   |   z   |   s   |  lc  |    r   |\n|------------|--------|---------|---------|-------|-------|-------|------|--------|\n| All        |  3796  | 13,584  | 0.0019  | 0.833 | 7.157 | 7.683 | 3443 | -0.077 |""
  - ""(Page 23, Table 1) Below is a rendering of the page up to the first error.```\n|                               | 2010-11                                                                 | 2012-13                                                                 | 2014-15                                                                 | 2016-17                                                                 | 2018-19                                                                 |\n|-------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------|""
  - ""(Page 24, Table 1) | 2010-11                          | 2012-13                                      | 2014-15                                                                 | 2016-17                                                                 | 2018-19                                                                 |\n|----------------------------------|----------------------------------------------|------------------------------------------------------------------------|------------------------------------------------------------------------|------------------------------------------------------------------------|\n| ‘affect’ cluster                 |                                              |                                                                        | - recurrent neural network                                             |                                                                        |""","- The paper primarily focuses on mapping the knowledge structure and evolution of the Artificial Intelligence in Education (AIED) field through a network-based keyword analysis.
- The paper does not provide specific quantitative performance metrics such as accuracy, precision, recall, F1-scores, or processing speed and efficiency metrics.
- There is no mention of comparison with baseline methods or user satisfaction/usability results.
- The paper does not discuss system reliability and robustness measures or scalability test results.
- The tables included in the paper provide structural characteristics of the keyword co-occurrence networks and the top in-group degree keywords for selected knowledge clusters, but do not contain performance metrics.
- The paper is more focused on identifying trends and knowledge clusters within the AIED field rather than evaluating specific system performance.","  - ""A static keyword cooccurrence network (KCN) is constructed based on 1830 articles in the field of AIED from 2010 to 2019, and additionally, five temporal KCNs are constructed based on a two-year time window to analyze the knowledge evolution of the field during this most recent period of interest.""
  - ""All the available articles published in the five sources from 2010 to 2019 were collected in this study, apart from the 28 articles from EDM published in 2010, which were excluded in the final dataset as they do not have listed keywords.""
  - ""In total, there are 1830 articles in the final dataset for analyzing the research topics and their evolution within the AIED field.""
  - ""The detailed information of each source in the final dataset is provided below:

& AIEDC: 328 full and short papers, 2011 papers, , 2013 papers, , 2015 papers, , 2017 papers, , 2018 papers, , 2019 papers, & IJAIED: 168 articles, 2013 papers, -2019 . & EDM: 518 full and short papers (not including posters), 2011-2019 & L@S: 396 articles, including work-in-progress papers, 2014 -2019 & ITS: 420 articles, 2010 , 2012 , 2014 , 2016 , 2018 , 2019""
  - ""Before analyzing the keyword co-occurrence networks, it is essential to standardize the keywords in the retrieved articles. In particular, a pre-processing of the keywords is required to eliminate the duplicated keywords in the dataset.""
  - ""The strategies used to standardize the keywords are presented below:

1) Converting plurals into a singular form 2) Removing hyphens: e.g., eye-trackingeye tracking 3) Converting to lowercase 4) Avoidance of abbreviations 5) Removing redundant words: e.g., massive open online course (moocs)massive open online course""
  - ""After the refinement, there are 3800 distinct keywords from the 1830 articles in the final dataset for the following analysis.""",,"- The paper uses a dataset of 1830 articles from five prominent publication sources in the AIED field: AIEDC, IJAIED, EDM, L@S, and ITS.
- The dataset spans from 2010 to 2019, with specific numbers of articles from each source provided.
- The data collection method involved collecting all available articles from these sources, excluding 28 articles from EDM in 2010 due to missing keywords.
- The data preprocessing involved standardizing keywords by converting plurals to singular, removing hyphens, converting to lowercase, avoiding abbreviations, and removing redundant words.
- The final dataset consisted of 3800 distinct keywords after preprocessing.
- There is no mention of using external knowledge bases or ontologies, nor are there details on training/validation/test data splits as this is not relevant to the network analysis conducted.","  - ""In this study, we review 1830 research articles on artificial intelligence in education (AIED), with the aim of providing a holistic picture of the knowledge evolution in this interdisciplinary research field from 2010 to 2019.""
  - ""The results reveal considerable research diversity in the AIED field, centering around two sustained themes: intelligent tutoring systems (2010-19) and massive open online courses (since 2014).""
  - ""The focal educational concerns reflected in AIED research are: (1) online learning; (2) game-based learning; (3) collaborative learning; (4) assessment; (5) affect; (6) engagement; and (7) learning design.""
  - ""The highly connected keywords relevant to analytic techniques within this field include natural language processing, educational data mining, learning analytics and machine learning.""
  - ""A static keyword cooccurrence network (KCN) is constructed based on 1830 articles in the field of AIED from 2010 to 2019, and additionally, five temporal KCNs are constructed based on a two-year time window to analyze the knowledge evolution of the field during this most recent period of interest.""
  - ""All the available articles published in the five sources from 2010 to 2019 were collected in this study, apart from the 28 articles from EDM published in 2010, which were excluded in the final dataset as they do not have listed keywords.""
  - ""The three-step multi-scale (macro, meso, micro) framework proposed in this study can also be applied to map the knowledge development in other scientific research areas.""
  - ""The findings of the study echo the future trends suggested by the previous researchers within AIED.""",,"- The target research discipline or field is Artificial Intelligence in Education (AIED).
- The specific literature review tasks addressed include analyzing the knowledge structure, knowledge clusters, and trending keywords within AIED.
- The types of academic documents processed are research articles from five prominent publication sources in the AIED field.
- The user types and requirements are not explicitly mentioned, but the study is aimed at providing insights for researchers and potentially educators interested in AIED.
- The integration with research workflows is implied through the analysis of keyword co-occurrence networks, which can help researchers identify key topics and trends.
- The application is academic rather than commercial, as it focuses on mapping knowledge development in AIED."
Literature omics: a new approach to systematic review about pose estimation,Mingtao Ma,10.1117/12.3034519,https://doi.org/10.1117/12.3034519,"Symposium on Advances in Electrical, Electronics and Computer Engineering",0,2024,Not mentioned (the abstract does not provide details on the technical architecture or system components),"Not mentioned (the abstract does not provide specific technical implementation details such as algorithms, models, or feature extraction techniques)","- Content-based matching techniques: Literatureomics
- Citation analysis methods: Co-authorship and keyword co-occurrence analysis using VOSviewer, identifying emerging keywords and reference literature using CiteSpace
- Metadata extraction and utilization: Analyzing milestone events over time with thematic modeling discussions",Not mentioned (the abstract does not explicitly mention any technical challenges related to pose estimation),"- Literatureomics as a method for analyzing related literature
- Use of VOSviewer for co-authorship and keyword co-occurrence analysis
- Use of CiteSpace for identifying emerging keywords and reference literature",Not mentioned (the abstract does not provide any performance results or metrics related to pose estimation),"- Academic databases used: Web of Science Core Collection
- Dataset sizes and characteristics: 4873 literatures related to pose estimation
- Data collection methods: Not mentioned
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: Pose estimation, motion capture
- Specific literature review tasks addressed: Examining the evolution and significant developments in pose estimation
- Types of academic documents processed: Research papers and publications related to pose estimation
- User types and requirements: Not explicitly mentioned
- Integration with research workflows: Analysis of publications to inform future research directions
- Commercial vs. academic applications: Both, with potential applications in healthcare (surgical sector)","  - ""This paper conducts analysis of related literatures using the method of literatureomics based on 4873 literatures related to pose estimation that were retrieved from the Web of Science Core Collection.""
  - ""Pose estimation has grown in popularity thanks to Deep Learning and COCO datasets, and it will be extensively employed in the surgical sector.""
  - ""Perform co-authorship and keyword co-occurrence analysis and visualization using VOSviewer, and identify emerging keywords and reference literature using CiteSpace.""",,"- The abstract mentions the use of ""literatureomics"" as a method for analyzing related literatures, which suggests a focus on bibliometric analysis rather than a specific technical architecture for a system.
- The tools used for analysis include VOSviewer for co-authorship and keyword co-occurrence analysis, and CiteSpace for identifying emerging keywords and reference literature. These are software tools rather than components of a technical architecture.
- The mention of ""Deep Learning"" and ""COCO datasets"" indicates the use of deep learning frameworks and specific datasets in the field of pose estimation, but it does not specify the technical architecture of the system used in the study.
- There is no mention of system architecture components such as databases, APIs, interfaces, or technical infrastructure like cloud platforms or computational resources.
- The abstract does not provide information on integration with existing academic databases or platforms beyond the retrieval of literature from the Web of Science Core Collection.
- The overall system design approach is not detailed in the abstract, as it focuses on the analysis and findings rather than the technical implementation.","  - ""Additionally, analyze milestone events over time by combining them with thematic modeling discussions.""
  - ""This paper conducts analysis of related literatures using the method of literatureomics based on 4873 literatures related to pose estimation that were retrieved from the Web of Science Core Collection.""
  - ""Perform co-authorship and keyword co-occurrence analysis and visualization using VOSviewer, and identify emerging keywords and reference literature using CiteSpace.""",,"- The abstract mentions the use of ""literatureomics"" as a method for analyzing related literatures, which suggests a focus on bibliometric analysis rather than specific technical implementation details like algorithms or feature extraction techniques.
- The use of VOSviewer for co-authorship and keyword co-occurrence analysis indicates a focus on network analysis and visualization, but does not specify any algorithms or models used.
- CiteSpace is used to identify emerging keywords and reference literature, which is a tool for bibliometric analysis but does not provide specific technical implementation details.
- The abstract does not mention any specific algorithms, models, feature extraction techniques, data processing pipelines, training methodologies, or preprocessing steps related to deep learning or other technical aspects.
- The focus of the paper is on bibliometric analysis and identifying milestones in pose estimation research rather than on specific technical implementation details.","  - ""This paper conducts analysis of related literatures using the method of literatureomics based on 4873 literatures related to pose estimation that were retrieved from the Web of Science Core Collection.""
  - ""Additionally, analyze milestone events over time by combining them with thematic modeling discussions.""
  - ""Perform co-authorship and keyword co-occurrence analysis and visualization using VOSviewer, and identify emerging keywords and reference literature using CiteSpace.""",,"- The abstract mentions the use of ""literatureomics"" as a method for analyzing related literatures. This suggests a content-based matching technique where the focus is on analyzing the content of the literature to identify patterns and connections.
- The use of ""co-authorship and keyword co-occurrence analysis and visualization using VOSviewer"" indicates a citation analysis method and a content-based matching technique. VOSviewer is a tool used for visualizing bibliometric networks, which helps in identifying key authors, institutions, and keywords.
- The mention of ""identify emerging keywords and reference literature using CiteSpace"" suggests a citation analysis method and a metadata extraction and utilization technique. CiteSpace is a tool used for visualizing and analyzing citation networks, which helps in identifying influential papers and emerging trends.
- The analysis of ""milestone events over time by combining them with thematic modeling discussions"" implies a metadata extraction and utilization technique, as it involves extracting and analyzing metadata such as publication dates and thematic content.","  - ""But it is still unclear “what exactly drives the development of pose estimation”.""
  - ""Pose estimation has grown in popularity thanks to Deep Learning and COCO datasets, and it will be extensively employed in the surgical sector.""",,"- The abstract mentions that it is unclear what drives the development of pose estimation, which could imply a lack of understanding or technical challenges in the field. However, this is more about the motivation or drivers rather than specific technical challenges.
- The mention of Deep Learning and COCO datasets suggests that these are significant factors in the development of pose estimation, but it does not explicitly mention any technical challenges related to these technologies.
- The abstract does not explicitly mention any technical challenges such as algorithm performance issues, data quality or availability problems, scalability challenges, integration difficulties, evaluation methodology limitations, computational resource constraints, or technical bottlenecks or failure points.","  - ""This paper conducts analysis of related literatures using the method of literatureomics based on 4873 literatures related to pose estimation that were retrieved from the Web of Science Core Collection.""
  - ""Additionally, analyze milestone events over time by combining them with thematic modeling discussions.""
  - ""Perform co-authorship and keyword co-occurrence analysis and visualization using VOSviewer, and identify emerging keywords and reference literature using CiteSpace.""
  - ""Pose estimation has grown in popularity thanks to Deep Learning and COCO datasets, and it will be extensively employed in the surgical sector.""
  - ""The purpose of this paper is to examine the evolution of the field of pose estimation, identify significant developments (in terms of time, person, publications, institutions, nations, and their internal connections), and identify significant turning points and milestones.""",,"- The abstract mentions the use of ""literatureomics"" as a method for analyzing related literature, which could be considered a novel approach in the context of systematic reviews.
- The use of VOSviewer for co-authorship and keyword co-occurrence analysis, and CiteSpace for identifying emerging keywords and reference literature, suggests a creative problem-solving method in terms of analyzing and visualizing literature connections.
- The mention of ""Deep Learning and COCO datasets"" indicates a reliance on existing technologies rather than introducing new algorithmic contributions or novel feature engineering techniques.
- The abstract does not explicitly mention new algorithmic contributions, hybrid or ensemble approaches, novel feature engineering techniques, innovative evaluation metrics, or technical workarounds for known problems.
- The focus is on analyzing the evolution and milestones of pose estimation rather than introducing new technical solutions.","  - ""This paper conducts analysis of related literatures using the method of literatureomics based on 4873 literatures related to pose estimation that were retrieved from the Web of Science Core Collection.""
  - ""The purpose of this paper is to examine the evolution of the field of pose estimation, identify significant developments (in terms of time, person, publications, institutions, nations, and their internal connections), and identify significant turning points and milestones.""
  - ""Pose estimation has grown in popularity thanks to Deep Learning and COCO datasets, and it will be extensively employed in the surgical sector.""
  - ""This study conducted a bibliometric analysis of over 20 years of research on pose estimation, identifying the countries, institutions, authors, journals, and publications involved in this field.""
  - ""Perform co-authorship and keyword co-occurrence analysis and visualization using VOSviewer, and identify emerging keywords and reference literature using CiteSpace.""",,"- The abstract focuses on the evolution and bibliometric analysis of pose estimation research rather than performance results of specific algorithms or systems.
- The methods described involve literature analysis and visualization, which do not typically yield performance metrics like accuracy, precision, or processing speed.
- The abstract does not mention any specific performance metrics or outcomes related to pose estimation algorithms or systems.
- The focus is on identifying significant developments, turning points, and milestones in the field, rather than evaluating the performance of specific methods.","  - ""This paper conducts analysis of related literatures using the method of literatureomics based on 4873 literatures related to pose estimation that were retrieved from the Web of Science Core Collection.""
  - ""Perform co-authorship and keyword co-occurrence analysis and visualization using VOSviewer, and identify emerging keywords and reference literature using CiteSpace.""
  - ""This study conducted a bibliometric analysis of over 20 years of research on pose estimation, identifying the countries, institutions, authors, journals, and publications involved in this field.""",,"- The abstract mentions that the study uses ""4873 literatures related to pose estimation that were retrieved from the Web of Science Core Collection,"" indicating that the Web of Science Core Collection is the primary academic database used for data collection.
- The study uses ""literatureomics"" as a method, which involves analyzing literature data, but it does not specify any dataset sizes or characteristics beyond the number of literatures analyzed.
- The abstract does not provide information on data collection methods, training/validation/test data splits, or data preprocessing and cleaning approaches, as these are not relevant to a bibliometric analysis of literature.
- The tools used for analysis include VOSviewer for co-authorship and keyword co-occurrence analysis and CiteSpace for identifying emerging keywords and reference literature, but these are not external knowledge bases or ontologies.
- The abstract does not mention any external knowledge bases or ontologies used in the study.","  - ""Pose estimation has been developed and is frequently used in motion capture and other applications.""
  - ""Pose estimation has grown in popularity thanks to Deep Learning and COCO datasets, and it will be extensively employed in the surgical sector.""
  - ""This paper conducts analysis of related literatures using the method of literatureomics based on 4873 literatures related to pose estimation that were retrieved from the Web of Science Core Collection.""
  - ""The purpose of this paper is to examine the evolution of the field of pose estimation, identify significant developments (in terms of time, person, publications, institutions, nations, and their internal connections), and identify significant turning points and milestones.""",,"- The abstract mentions that pose estimation is used in ""motion capture and other applications,"" indicating its application in fields related to motion analysis.
- The paper aims to examine the evolution and significant developments in the field of pose estimation, which suggests a focus on understanding the historical and current state of research in this area.
- The use of ""literatureomics"" and analysis of a large number of publications indicates that the paper is focused on a literature review task, specifically analyzing the development of pose estimation research.
- The mention of ""4873 literatures related to pose estimation"" suggests that the types of academic documents processed are research papers and publications related to pose estimation.
- The abstract does not explicitly mention user types or requirements, but the focus on understanding the evolution and milestones in pose estimation suggests an academic application.
- The integration with research workflows is implied by the analysis of publications and identification of significant developments, which could inform future research directions.
- The mention of ""Deep Learning and COCO datasets"" and future employment in the ""surgical sector"" indicates a potential commercial application in healthcare."
Machine Learning Tools To (Semi-)Automate Evidence Synthesis: A Rapid Review and Evidence Map,"Gaelen Adam, Melinda Davies, Jerusha George, Eduardo L Caputo, Ja Mai Htun, Erin Coppola, Haley Holmer, Edi Kuhn, Holly Wethington, Ilya Ivlev, Ethan M. Balk, Thomas A. Trikalinos",10.23970/ahrqepcwhitepapermachine2,https://doi.org/10.23970/ahrqepcwhitepapermachine2,-,0,2025,"- Main AI/ML frameworks used: Machine learning, artificial intelligence, deep learning (zero-shot models), NLP
- System architecture components: Not mentioned
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Not mentioned
- Overall system design approach: Not mentioned","- Algorithms and models used: Zero-shot models, commercial large language models (e.g., ChatGPT, Claude)
- Feature extraction techniques: Not mentioned
- Data processing pipelines: Not mentioned
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Not mentioned","- Search and retrieval algorithms: Searching PubMed, Embase, and the ACM Digital Library.
- Relevance ranking approaches: Evaluations of machine learning or artificial intelligence tools for systematic review tasks.
- Content-based matching techniques: Tools for identifying randomized controlled trials and abstract screening.
- Metadata extraction and utilization: Implied through performance metrics and evaluation frameworks.
- Recommendation system approaches: Not explicitly mentioned.
- Expert/authority identification methods: Not explicitly mentioned.
- Citation analysis methods: Not explicitly mentioned.","- Algorithm performance issues: Low recall and precision in searching tools; variable performance in data extraction tools.
- Data quality or availability problems: Need for standardized evaluation frameworks to ensure reliability and comparability.
- Scalability challenges: Implied by variability in tool performance across different settings and tasks.
- Integration difficulties: Implied by heterogeneity in tool evaluations across different settings and tasks.
- Evaluation methodology limitations: Need for standardized frameworks to assess tool performance.
- Computational resource constraints: Not explicitly mentioned.
- Technical bottlenecks or failure points: Variable performance of certain tools; need for ongoing updates due to rapidly evolving technologies.","- Use of machine learning and artificial intelligence tools for automating or semi-automating stages of systematic review production.
- Tools for identifying randomized controlled trials using zero-shot models.
- Abstract screening tools using zero-shot models and semi-automated models.
- Use of commercial large language models (e.g., ChatGPT and Claude) for abstract screening and data extraction.
- Development of standardized evaluation frameworks for assessing tool performance.","- Accuracy: Not mentioned
- Precision: 
  - Randomized controlled trials: 79%
  - Abstract screening: 85% (fully automated), 97% (semi-automated)
  - Searching: 0.09
  - Data extraction: Not mentioned
- Recall: 
  - Randomized controlled trials: 96%
  - Abstract screening: 85% (fully automated), 97% (semi-automated)
  - Searching: 14%
  - Data extraction: Not mentioned
- F1-scores: Not mentioned
- Processing speed and efficiency metrics: Not mentioned
- Comparison with baseline methods: Not mentioned
- User satisfaction or usability results: Not mentioned
- System reliability and robustness measures: Not mentioned
- Scalability test results: Not mentioned","- Academic databases used: PubMed, Embase, ACM Digital Library
- Dataset sizes and characteristics: Not mentioned
- Data collection methods: Not mentioned
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: Health sciences and social sciences
- Specific literature review tasks addressed: Identifying randomized controlled trials, abstract screening, searching, data extraction, risk of bias assessment
- Types of academic documents processed: Primary literature, systematic reviews
- User types and requirements: Researchers conducting systematic reviews
- Integration with research workflows: Automation of systematic review tasks
- Commercial vs. academic applications: Academic context implied by focus on standardized evaluation frameworks","  - ""Tools that leverage machine learning, a subset of artificial intelligence, are becoming increasingly important for conducting evidence synthesis as the volume and complexity of primary literature expands exponentially.""
  - ""We included evaluations of machine learning or artificial intelligence tools to automate or semi-automate any stage of systematic review production.""
  - ""Tools for identifying randomized controlled trials performed well, with a median recall of 96% and precision of 79%.""
  - ""Abstract screening tools also showed promising, though variable, results, achieving a median recall of 85% for fully automated screening using zero-shot models; 97% for semi-automated models with a median 51% reduction in screening burden.""
  - ""Risk of bias assessment tools showed moderate agreement with human assessments (Cohen's kappa = 0.20; median agreement: 71%).""
  - ""commercial large language models (e.g., ChatGPT and Claude) are demonstrating improved performance in fully automating abstract screening and in data extraction.""
  - ""This work revealed the importance of developing standardized evaluation frameworks for assessing the performance of machine learning and artificial intelligence tools in systematic review tasks.""",,"- The abstract mentions the use of machine learning and artificial intelligence tools for evidence synthesis, indicating that these are key technical components.
- The mention of ""zero-shot models"" suggests the use of deep learning techniques, particularly in natural language processing (NLP) for abstract screening.
- The reference to ""commercial large language models"" like ChatGPT and Claude implies the integration of these models into the system for tasks like abstract screening and data extraction.
- The abstract does not provide specific details about the system architecture components, technical infrastructure, or integration with existing academic databases or platforms.
- The focus on developing standardized evaluation frameworks suggests a need for a structured approach to assessing tool performance, but does not specify the technical architecture or system design.","  - ""Tools that leverage machine learning, a subset of artificial intelligence, are becoming increasingly important for conducting evidence synthesis as the volume and complexity of primary literature expands exponentially.""
  - ""We included evaluations of machine learning or artificial intelligence tools to automate or semi-automate any stage of systematic review production.""
  - ""Extracted data included key characteristics of the tools (e.g., type of automation method, systematic review tasks automated), methods used to evaluate the tool performance, and performance results.""
  - ""Tools for identifying randomized controlled trials performed well, with a median recall of 96% and precision of 79%.""
  - ""Abstract screening tools also showed promising, though variable, results, achieving a median recall of 85% for fully automated screening using zero-shot models; 97% for semi-automated models with a median 51% reduction in screening burden.""
  - ""Risk of bias assessment tools showed moderate agreement with human assessments (Cohen's kappa = 0.20; median agreement: 71%).""
  - ""commercial large language models (e.g., ChatGPT and Claude) are demonstrating improved performance in fully automating abstract screening and in data extraction.""",,"- The abstract mentions the use of machine learning and artificial intelligence tools for evidence synthesis, indicating that these technologies are central to the implementation methods.
- The mention of ""zero-shot models"" in the context of abstract screening suggests the use of specific machine learning models that can perform tasks without prior training data.
- The reference to ""commercial large language models"" like ChatGPT and Claude implies the use of these models for abstract screening and data extraction, which are likely based on neural networks.
- The abstract does not provide specific details on algorithms, feature extraction techniques, data processing pipelines, training methodologies, or preprocessing steps, which are typically included in technical implementation details.
- The focus on performance metrics such as recall and precision suggests that these are key evaluation criteria for the tools, but does not provide specific technical implementation details.","  - ""We searched PubMed, Embase, and the ACM Digital Library from January 1, 2021, to April 3, 2024, with update searches on October 3, 2024, and April 3, 2025, for comparative studies, and identified older studies using the reference lists of existing evidence synthesis products (ESPs).""
  - ""We included evaluations of machine learning or artificial intelligence tools to automate or semi-automate any stage of systematic review production.""
  - ""Extracted data included key characteristics of the tools (e.g., type of automation method, systematic review tasks automated), methods used to evaluate the tool performance, and performance results.""
  - ""Tools for identifying randomized controlled trials performed well, with a median recall of 96% and precision of 79%.""
  - ""Abstract screening tools also showed promising, though variable, results, achieving a median recall of 85% for fully automated screening using zero-shot models; 97% for semi-automated models with a median 51% reduction in screening burden.""
  - ""tools for searching had low recall (median 14%) and precision (median 0.09), and data extraction tools varied widely, with a median 66% of data correctly extracted.""
  - ""Risk of bias assessment tools showed moderate agreement with human assessments (Cohen's kappa = 0.20; median agreement: 71%).""
  - ""This work revealed the importance of developing standardized evaluation frameworks for assessing the performance of machine learning and artificial intelligence tools in systematic review tasks.""",,"- The abstract mentions searching PubMed, Embase, and the ACM Digital Library, which are databases used for finding academic papers. This indicates the use of search and retrieval algorithms.
- The mention of ""evaluations of machine learning or artificial intelligence tools"" suggests the use of relevance ranking approaches and content-based matching techniques, as these tools are likely used to rank and match papers based on relevance.
- The abstract does not explicitly mention citation analysis methods, metadata extraction and utilization, recommendation system approaches, or expert/authority identification methods. However, the focus on systematic review tasks implies some level of content-based matching and relevance ranking.
- The performance metrics for tools (e.g., recall and precision) suggest the use of evaluation frameworks that could involve metadata extraction and utilization.
- The abstract does not provide specific details on the exact techniques used for paper discovery, but it implies the use of machine learning and AI tools for tasks like abstract screening and data extraction.","  - ""Tools for identifying randomized controlled trials performed well, with a median recall of 96% and precision of 79%.""
  - ""Abstract screening tools also showed promising, though variable, results, achieving a median recall of 85% for fully automated screening using zero-shot models; 97% for semi-automated models with a median 51% reduction in screening burden.""
  - ""In contrast, tools for searching had low recall (median 14%) and precision (median 0.09), and data extraction tools varied widely, with a median 66% of data correctly extracted.""
  - ""Risk of bias assessment tools showed moderate agreement with human assessments (Cohen's kappa = 0.20; median agreement: 71%).""
  - ""However, other tools, such as those for searching and data extraction, show highly variable performance and are not yet reliable enough for to be used for semi-automation of these tasks.""
  - ""These conclusions are largely unchanged from earlier iterations, with the note that commercial large language models (e.g., ChatGPT and Claude) are demonstrating improved performance in fully automating abstract screening and in data extraction.""
  - ""This work revealed the importance of developing standardized evaluation frameworks for assessing the performance of machine learning and artificial intelligence tools in systematic review tasks.""
  - ""We did not assess the risk of bias or methodological quality of the included studies, which may affect the reliability and comparability of the reported performance outcomes.""
  - ""Additionally, the tools were evaluated in a variety of settings, tasks, and review questions, which introduces heterogeneity that makes direct comparisons across tools challenging.""
  - ""Lastly, the rapidly evolving nature of machine learning technologies means that our findings may quickly become outdated.""",,"- The abstract highlights several technical challenges related to algorithm performance issues, such as low recall and precision in searching tools and variable performance in data extraction tools.
- Data quality or availability problems are implied by the need for standardized evaluation frameworks to assess tool performance, suggesting that current data may not be reliable or comparable.
- Scalability challenges are not explicitly mentioned, but the variability in tool performance across different settings and tasks suggests potential scalability issues.
- Integration difficulties are not directly mentioned, but the heterogeneity in tool evaluations across different settings and tasks implies challenges in integrating these tools into a cohesive system.
- Evaluation methodology limitations are explicitly mentioned, with a need for standardized frameworks to assess tool performance.
- Computational resource constraints are not mentioned, but the rapidly evolving nature of machine learning technologies suggests potential challenges in keeping up with computational demands.
- Technical bottlenecks or failure points are implied by the variable performance of certain tools and the need for ongoing updates to keep findings relevant.","  - ""Tools that leverage machine learning, a subset of artificial intelligence, are becoming increasingly important for conducting evidence synthesis as the volume and complexity of primary literature expands exponentially.""
  - ""We included evaluations of machine learning or artificial intelligence tools to automate or semi-automate any stage of systematic review production.""
  - ""Tools for identifying randomized controlled trials performed well, with a median recall of 96% and precision of 79%.""
  - ""Abstract screening tools also showed promising, though variable, results, achieving a median recall of 85% for fully automated screening using zero-shot models; 97% for semi-automated models with a median 51% reduction in screening burden.""
  - ""Risk of bias assessment tools showed moderate agreement with human assessments (Cohen's kappa = 0.20; median agreement: 71%).""
  - ""commercial large language models (e.g., ChatGPT and Claude) are demonstrating improved performance in fully automating abstract screening and in data extraction.""
  - ""This work revealed the importance of developing standardized evaluation frameworks for assessing the performance of machine learning and artificial intelligence tools in systematic review tasks.""",,"- The abstract discusses the use of machine learning and artificial intelligence tools for automating or semi-automating stages of systematic review production, which is a novel approach in the context of literature synthesis.
- The mention of tools for identifying randomized controlled trials and abstract screening using zero-shot models and semi-automated models indicates innovative technical approaches.
- The use of commercial large language models like ChatGPT and Claude for abstract screening and data extraction suggests a hybrid or ensemble approach by leveraging existing AI technologies.
- The emphasis on developing standardized evaluation frameworks for assessing tool performance is a creative problem-solving method to address the challenge of evaluating AI tools in systematic reviews.
- The abstract does not explicitly mention new algorithmic contributions, novel feature engineering techniques, or original system design elements, but it implies the use of advanced AI technologies for literature synthesis.","  - ""Tools for identifying randomized controlled trials performed well, with a median recall of 96% and precision of 79%.""
  - ""Abstract screening tools also showed promising, though variable, results, achieving a median recall of 85% for fully automated screening using zero-shot models; 97% for semi-automated models with a median 51% reduction in screening burden.""
  - ""In contrast, tools for searching had low recall (median 14%) and precision (median 0.09), and data extraction tools varied widely, with a median 66% of data correctly extracted.""
  - ""Risk of bias assessment tools showed moderate agreement with human assessments (Cohen's kappa = 0.20; median agreement: 71%).""
  - ""Certain tools, particularly those for automatically identifying randomized controlled trials (RCTs) and prioritizing relevant abstracts in screening, show a high level of recall and precision, suggesting they may soon be appropriate for widespread use with human oversight.""
  - ""However, other tools, such as those for searching and data extraction, show highly variable performance and are not yet reliable enough for to be used for semi-automation of these tasks.""
  - ""commercial large language models (e.g., ChatGPT and Claude) are demonstrating improved performance in fully automating abstract screening and in data extraction.""",,"- The abstract provides specific performance metrics for various machine learning tools used in systematic review tasks. These include recall and precision for identifying randomized controlled trials, abstract screening, searching, and data extraction.
- The recall and precision for identifying randomized controlled trials are high, indicating good performance.
- Abstract screening tools show promising results, especially with semi-automated models, which reduce screening burden.
- Searching tools have low recall and precision, indicating poor performance.
- Data extraction tools have variable performance, with a median of 66% correct extraction.
- Risk of bias assessment tools show moderate agreement with human assessments.
- The abstract does not mention processing speed, efficiency metrics, user satisfaction, system reliability, or scalability test results.
- The focus is on the accuracy and reliability of the tools in performing specific tasks within systematic reviews.","  - ""We searched PubMed, Embase, and the ACM Digital Library from January 1, 2021, to April 3, 2024, with update searches on October 3, 2024, and April 3, 2025, for comparative studies, and identified older studies using the reference lists of existing evidence synthesis products (ESPs).""
  - ""Extracted data included key characteristics of the tools (e.g., type of automation method, systematic review tasks automated), methods used to evaluate the tool performance, and performance results.""
  - ""We included evaluations of machine learning or artificial intelligence tools to automate or semi-automate any stage of systematic review production.""",,"- The abstract mentions that the authors searched academic databases such as PubMed, Embase, and the ACM Digital Library for studies related to machine learning tools in evidence synthesis. This provides information about the academic databases used.
- The abstract does not provide specific details about dataset sizes, characteristics, data collection methods, training/validation/test data splits, or data preprocessing and cleaning approaches. These details are not mentioned in the abstract.
- The abstract does not mention any external knowledge bases or ontologies used in the study.
- The focus of the abstract is on the evaluation of machine learning tools for systematic review tasks rather than on the specific data sources or datasets used in those tools.","  - ""Tools that leverage machine learning, a subset of artificial intelligence, are becoming increasingly important for conducting evidence synthesis as the volume and complexity of primary literature expands exponentially.""
  - ""We included evaluations of machine learning or artificial intelligence tools to automate or semi-automate any stage of systematic review production.""
  - ""Tools for identifying randomized controlled trials performed well, with a median recall of 96% and precision of 79%.""
  - ""Abstract screening tools also showed promising, though variable, results, achieving a median recall of 85% for fully automated screening using zero-shot models; 97% for semi-automated models with a median 51% reduction in screening burden.""
  - ""In contrast, tools for searching had low recall (median 14%) and precision (median 0.09), and data extraction tools varied widely, with a median 66% of data correctly extracted.""
  - ""Risk of bias assessment tools showed moderate agreement with human assessments (Cohen's kappa = 0.20; median agreement: 71%).""
  - ""Certain tools, particularly those for automatically identifying randomized controlled trials (RCTs) and prioritizing relevant abstracts in screening, show a high level of recall and precision, suggesting they may soon be appropriate for widespread use with human oversight.""
  - ""This work revealed the importance of developing standardized evaluation frameworks for assessing the performance of machine learning and artificial intelligence tools in systematic review tasks.""",,"- The abstract discusses the application of machine learning tools in the context of evidence synthesis, which is a key component of systematic reviews in various research disciplines.
- The tools are designed to automate or semi-automate stages of systematic review production, which includes tasks such as identifying randomized controlled trials, abstract screening, searching, data extraction, and risk of bias assessment.
- The abstract mentions the performance of these tools in terms of recall and precision, indicating their effectiveness in specific tasks.
- The focus on systematic review tasks suggests that the target research disciplines are those that rely heavily on systematic reviews, such as health sciences and social sciences.
- The abstract does not specify user types or requirements but implies that these tools are intended for researchers conducting systematic reviews.
- The integration with research workflows is implied by the automation of systematic review tasks, which would streamline the review process.
- The abstract does not explicitly mention commercial vs. academic applications, but the focus on developing standardized evaluation frameworks suggests an academic context."
The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review,"Dmitry Scherbakov, Nina C. Hubig, V. Jansari, Alexander Bakumenko, Leslie A Lenert",10.1093/jamia/ocaf063,https://doi.org/10.1093/jamia/ocaf063,J. Am. Medical Informatics Assoc.,33,2024,"- Main AI/ML frameworks used: OpenAI GPT-4o model, ChatGPT, Scite.ai
- System architecture components: Covidence for screening and extraction, LLM add-on
- Technical infrastructure: Cloud platforms for data processing, computational resources for LLMs
- Integration with existing academic databases: PubMed, Scopus, Dimensions, Google Scholar
- Overall system design approach: Human reviewers + LLMs for automation with human oversight","- Algorithms and models used: Large Language Models (LLMs) such as ChatGPT and GPT-4o.
- Feature extraction techniques: Not explicitly mentioned, but likely involved in data extraction processes.
- Data processing pipelines: Covidence platform with LLM add-on for abstract screening, full-text screening, and extraction.
- Training methodologies: LLM inference programmed to run multiple times for decision-making.
- Preprocessing steps: Human experts designed data charting forms; ChatGPT used for data cleaning.
- Technical workflow or methodology: Hybrid approach combining human judgment with AI tools for automation.","- Search and retrieval algorithms: Human reviewers used PubMed, Scopus, Dimensions, and Google Scholar databases.
- Relevance ranking approaches: Covidence with LLM add-on for abstract and full-text screening; LLM inference with majority voting.
- Citation analysis methods: Not explicitly mentioned.
- Content-based matching techniques: Use of Covidence and LLMs for screening and extraction.
- Metadata extraction and utilization: Not explicitly mentioned.
- Recommendation system approaches: Use of Scite.ai and ChatGPT for drafting sections and generating figures.
- Expert/authority identification methods: Not explicitly mentioned.","- Algorithm performance issues: Lower accuracy of extraction for numeric data.
- Data quality or availability problems: Difficulty in distinguishing between LLM-generated and human-written text.
- Evaluation methodology limitations: Simplified scoring system may overlook key aspects of study quality.
- Integration difficulties: Need to integrate and evaluate different LLMs with other AI models.
- Technical bottlenecks or failure points: Challenges in generating trustworthy AI-driven reviews, especially for quantitative meta-analyses.","- Automation of review stages using Covidence and LLMs
- Use of LLM inference with a majority vote system for decision-making
- Integration of human expertise in designing data charting forms and quality assessment
- Use of ChatGPT for data cleaning and Scite.ai for drafting sections of the manuscript
- Domain-agnostic nature of the method for application across different domains
- Potential for integrating different LLMs and AI models to enhance performance","- Accuracy: GPT models had lower accuracy in title/abstract screening (M=77.34, SD=13.06) compared to BERT models (M=80.87, SD=11.81).
- Precision: GPT models performed better in data extraction with precision (M=83.07, SD=10.43).
- Recall: GPT models had recall (M=85.99, SD=9.82) in data extraction.
- Processing speed and efficiency metrics: LLMs are likely to significantly reduce the time needed for reviews.
- Comparison with baseline methods: GPT models performed better than BERT models in data extraction.
- User satisfaction or usability results: Not explicitly mentioned.
- System reliability and robustness measures: Expected to produce similar or higher-quality data compared to manual reviews.
- Scalability test results: Large-scale automation of review stages, including drafting and plot generation.","- Academic databases used: PubMed, Scopus, Dimensions, Google Scholar
- Dataset sizes and characteristics: Initially identified 3,788 studies; breakdown by database: PubMed (n = 2,174), Scopus (n = 1,207), Dimensions (n = 356), Google Scholar (n = 48)
- Data collection methods: Human reviewers conducted the search; Covidence add-on used for automated stages
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: Broad application across multiple fields
- Specific literature review tasks addressed: Abstract screening, full-text screening, data extraction, drafting manuscript sections
- Types of academic documents processed: Systematic reviews, scoping reviews, meta-analyses, other types of reviews
- User types and requirements: Researchers and reviewers
- Integration with research workflows: Use of Covidence add-on for automation
- Commercial vs. academic applications: Academic context","  - ""The search was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar by human reviewers.""
  - ""Screening and extraction process took place in Covidence with the help of LLM add-on based on the OpenAI GPT-4o model.""
  - ""ChatGPT and Scite.ai were used in cleaning the data, generating the code for figures, and drafting the manuscript.""
  - ""The review process involved three stages that were automated by Covidence add-on: abstract screening, full-text screening, and extraction.""
  - ""LLM inference was programmed to run inference 3 times to determine the final decision (e.g., ""include"" or ""exclude"") based on the majority vote.""
  - ""The data charting form for extraction were designed by human experts (DS, VJ, AB, LL, and NH) and adopted into the LLM prompt to collect the following primary information:""
  - ""Human reviewers (DS, VJ, AB) performed quality assessment of given studies using a set of selected categories from the reviewed studies and a points-based scale:""
  - ""ChatGPT (4o model) was used to clean the extraction data: format the case, remove duplicates, rename similar entries to a common name.""
  - ""Scite.ai (version from August 2024) was used to draft parts of the introduction and discussion sections, while ChatGPT was used to draft the abstract and results section of this review by generating R code snippets to produce all figures (except Figure 1 which was generated by Covidence).""
  - ""Human experts edited and verified the final LLM-generated draft of the manuscript.""
  - ""We used our own time measurements and reference data from experienced reviewers to calculate time-saving""",,"- The main AI/ML frameworks used include OpenAI GPT-4o model for the LLM add-on in Covidence, ChatGPT for data cleaning and drafting, and Scite.ai for drafting sections of the manuscript.
- System architecture components include Covidence for screening and extraction, and the use of LLMs for automation.
- Technical infrastructure involves the use of cloud platforms for data processing and computational resources for running LLMs.
- Integration with existing academic databases includes PubMed, Scopus, Dimensions, and Google Scholar for literature search.
- The overall system design approach involves a combination of human reviewers and LLMs for automation of review stages, with human oversight for quality assessment.","  - ""The search was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar by human reviewers.""
  - ""Screening and extraction process took place in Covidence with the help of LLM add-on based on the OpenAI GPT-4o model.""
  - ""ChatGPT and Scite.ai were used in cleaning the data, generating the code for figures, and drafting the manuscript.""
  - ""The review process involved three stages that were automated by Covidence add-on: abstract screening, full-text screening, and extraction.""
  - ""LLM inference was programmed to run inference 3 times to determine the final decision (e.g., ""include"" or ""exclude"") based on the majority vote.""
  - ""The data charting form for extraction were designed by human experts (DS, VJ, AB, LL, and NH) and adopted into the LLM prompt to collect the following primary information:""
  - ""Human reviewers (DS, VJ, AB) performed quality assessment of given studies using a set of selected categories from the reviewed studies and a points-based scale:""
  - ""ChatGPT (4o model) was used to clean the extraction data: format the case, remove duplicates, rename similar entries to a common name.""
  - ""Scite.ai (version from August 2024) was used to draft parts of the introduction and discussion sections, while ChatGPT was used to draft the abstract and results section of this review by generating R code snippets to produce all figures (except Figure 1 which was generated by Covidence).""
  - ""Human experts edited and verified the final LLM-generated draft of the manuscript.""
  - ""We used our own time measurements and reference data from experienced reviewers to calculate time-saving""",,"- The paper describes the use of large language models (LLMs) such as ChatGPT and GPT-4o for various stages of the review process, including abstract screening, full-text screening, and data extraction.
- The Covidence platform was used with an LLM add-on for these processes, indicating a structured workflow for automation.
- The use of Scite.ai for drafting parts of the manuscript suggests a role in content generation and possibly data cleaning.
- The mention of LLM inference running multiple times to determine decisions implies a structured algorithmic approach.
- The involvement of human experts in designing data charting forms and performing quality assessments indicates a hybrid approach combining human judgment with AI tools.
- The use of ChatGPT for cleaning data and generating R code snippets for figures suggests a role in data processing and visualization.","  - ""The search was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar by human reviewers.""
  - ""Screening and extraction process took place in Covidence with the help of LLM add-on based on the OpenAI GPT-4o model.""
  - ""The review process involved three stages that were automated by Covidence add-on: abstract screening, full-text screening, and extraction.""
  - ""LLM inference was programmed to run inference 3 times to determine the final decision (e.g., ""include"" or ""exclude"") based on the majority vote.""
  - ""The initial search was conducted by a human reviewer (DS) in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar databases.""
  - ""The review protocol was registered in the Open Science Framework (OSF) database""
  - ""Publications were excluded if they:

• Did not use some kind of LLM (e.g. ChatGPT, Mistral, GPT-3.5, BERT)

• Did not describe automation of any stage of the review process

• The paper was a review article itself that did not use LLM to conduct the review

• Full text of the article could not be retrieved or was not in English""
  - ""The data was then manually fed into the chat window by a human reviewer (DS). Scite.ai (version from August 2024) was used to draft parts of the introduction and discussion sections, while ChatGPT was used to draft the abstract and results section of this review by generating R code snippets to produce all figures (except Figure 1 which was generated by Covidence).""
  - ""ChatGPT and GPT-based LLM emerged as the most dominant architecture for review automation (n = 126, 73.2%).""
  - ""Majority of reviewed publications were papers describing how LLM could be used to automate a certain phase of the review (n=146, 84.9%)""
  - ""The most frequently mentioned AI model is GPT/ChatGPT, with 126 occurrences (73.3%), showing its widespread use""
  - ""GPT models had lower accuracy in title/abstract screening (M=77.34, SD=13.06) compared to BERT models (M=80.87, SD=11.81). However, GPT models performed better in data extraction, with precision (M=83.07, SD=10.43) and recall (M=85.99, SD=9.82),""
  - ""The use of LLMs in review automation is rapidly growing, with expected radical changes in scientific evidence synthesis.""",,"- The paper describes the use of large language models (LLMs) in the automation of systematic reviews, particularly focusing on the stages of abstract screening, full-text screening, and data extraction.
- The search process involved human reviewers using databases like PubMed, Scopus, Dimensions, and Google Scholar, indicating a reliance on these databases for paper discovery.
- The use of Covidence with an LLM add-on suggests a structured approach to screening and extraction, which may involve relevance ranking and content-based matching techniques.
- The mention of LLM inference and majority voting implies a method for relevance ranking and decision-making in the screening process.
- The paper highlights the dominance of GPT-based models in review automation, which could be related to their performance in data extraction.
- The use of Scite.ai for drafting sections and ChatGPT for generating figures suggests a role in content-based matching and recommendation system approaches.
- The paper does not explicitly mention citation analysis methods or expert/authority identification methods, but these could be inferred from the use of LLMs in evaluating paper relevance and quality.","  - ""Despite limitations, such as lower accuracy of extraction for numeric data, we anticipate that LLMs will soon change the way scientific reviews are conducted.""
  - ""A further limitation of this work is the simplified scoring system we introduced for research evaluation, which, using arbitrary weightings, may overlook key aspects like the novelty, robustness, and relevance of the studies.""
  - ""Future research should focus on improving LLM performance metrics, particularly precision and recall in lower-accuracy extraction categories.""
  - ""Additionally, integrating and evaluating different LLMs, possibly in combination with other AI models, should be explored to enhance performance.""
  - ""The use of LLMs in review automation is rapidly growing, with expected radical changes in scientific evidence synthesis.""
  - ""Research shows it is becoming increasingly difficult to distinguish between LLM-generated and human-written text.""
  - ""Although still in its early stages, AI-assisted reviews are already yielding impressive results, with growing interest as researchers develop semi-automated pipelines.""
  - ""However, generating trustworthy and useful AI-driven reviews still presents both technological and ethical challenges, particular for quantitative meta-analyses comparing treatment effects.""",,"- The paper mentions limitations related to the accuracy of extraction for numeric data, indicating a technical challenge in terms of algorithm performance.
- The simplified scoring system used for research evaluation is noted as a limitation, suggesting a methodological challenge in evaluating the quality of studies.
- The need to improve performance metrics such as precision and recall in certain extraction categories is highlighted, indicating a technical challenge in enhancing algorithm performance.
- The integration and evaluation of different LLMs with other AI models are suggested as areas for future research, indicating potential integration challenges.
- The difficulty in distinguishing between LLM-generated and human-written text is mentioned, which could be a challenge in terms of data quality and authenticity.
- The paper notes that generating trustworthy AI-driven reviews poses technological and ethical challenges, particularly for quantitative meta-analyses, indicating broader technical and ethical challenges.","  - ""The study's research plan was formulated by the author team and adjusted based on the guidance provided by the preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015: elaboration and explanation 14 and the latest JBI checklist 15 for conducting systematic reviews.""
  - ""The review process involved three stages that were automated by Covidence add-on: abstract screening, full-text screening, and extraction. In each stage, two human reviewers calibrated by screening a sample to refine inclusion criteria and extraction categories. They then created and tested prompts for the LLM.""
  - ""LLM inference was programmed to run inference 3 times to determine the final decision (e.g., ""include"" or ""exclude"") based on the majority vote.""
  - ""The data charting form for extraction were designed by human experts (DS, VJ, AB, LL, and NH) and adopted into the LLM prompt to collect the following primary information:

• Author, year, title;

• Country and/or US state;

• What types of reviews were automated;

• Stage of review automated in the research project;

• LLM type used;

• Performance metrics reported by authors during each stage of the review.""
  - ""Human reviewers (DS, VJ, AB) performed quality assessment of given studies using a set of selected categories from the reviewed studies and a points-based scale: of extraction was low (<0.8) during the benchmark.""
  - ""ChatGPT (4o model) was used to clean the extraction data: format the case, remove duplicates, rename similar entries to a common name.""
  - ""Scite.ai (version from August 2024) was used to draft parts of the introduction and discussion sections, while ChatGPT was used to draft the abstract and results section of this review by generating R code snippets to produce all figures (except Figure 1 which was generated by Covidence).""
  - ""Our LLM-assisted systematic review revealed a significant number of research projects related to review automation with LLM.""
  - ""The strength of present review is in large-scale (over 3000 abstracts screened, and 172 fulltext publications eligible for extraction) automation of different stages of review, including drafting the manuscript sections, and plot generation.""
  - ""our method is domain-agnostic, thus it can be integrated into large-scale review projects across different domains.""
  - ""GPT-based LLM were the most dominant type of LLM and the one that seems to show remarkable results on the data extraction, arguably the most complex and time-consuming stage of any review.""
  - ""We used calibrated LLMs as reviewers in this project.""
  - ""A further limitation of this work is the simplified scoring system we introduced for research evaluation, which, using arbitrary weightings, may overlook key aspects like the novelty, robustness, and relevance of the studies.""
  - ""integrating and evaluating different LLMs, possibly in combination with other AI models, should be explored to enhance performance.""
  - ""The use of LLMs in review automation is rapidly growing, with expected radical changes in scientific evidence synthesis.""",,"- The paper describes the use of a systematic review protocol adjusted according to PRISMA-P and JBI guidelines, which is a standard approach but not necessarily innovative.
- The automation of review stages using Covidence and LLMs is a significant technical contribution, as it streamlines the review process.
- The use of LLM inference with a majority vote system for decision-making is a novel approach to ensuring accuracy in the review process.
- The integration of human expertise in designing data charting forms and quality assessment is a hybrid approach that combines human judgment with AI capabilities.
- The use of ChatGPT for data cleaning and Scite.ai for drafting sections of the manuscript represents innovative technical workarounds for data management and content generation.
- The domain-agnostic nature of the method allows for its application across different domains, which is an innovative system design element.
- The paper highlights the potential for integrating different LLMs and AI models to enhance performance, which is a forward-looking technical direction.","  - ""GPT models had lower accuracy in title/abstract screening (M=77.34, SD=13.06) compared to BERT models (M=80.87, SD=11.81). However, GPT models performed better in data extraction, with precision (M=83.07, SD=10.43) and recall (M=85.99, SD=9.82), while BERT models had lower precision (M=61.06, SD=31.26) and similar recall (M=80.03, SD=10.09).""
  - ""Performance metrics reported for the three most common automated stages A: for GPT-based models. B: for BERT-based models.""
  - ""The use of LLMs in review automation is rapidly growing, with expected radical changes in scientific evidence synthesis. LLMs are likely to significantly reduce the time needed for reviews while producing similar or higher-quality data in greater quantities than manual reviews.""
  - ""We used calibrated LLMs as reviewers in this project. Some extraction categories, such as performance metrics, had relatively lower accuracy, so the results of this extraction category should be taken with caution.""
  - ""The strength of present review is in large-scale (over 3000 abstracts screened, and 172 fulltext publications eligible for extraction) automation of different stages of review, including drafting the manuscript sections, and plot generation.""
  - ""The implications of such automation include reducing human workload and improving overall efficiency of systematic reviews.""
  - ""GPT-based LLM were the most dominant type of LLM and the one that seems to show remarkable results on the data extraction, arguably the most complex and time-consuming stage of any review.""
  - ""We used our own time measurements and reference data from experienced reviewers to calculate time-saving 19""","  - ""(Page 26, Table 1) |                               | Sensitivity | Specificity | Pos Pred Value | Neg Pred Value | Precision | Recall |  F1  | Prevalence | Detection Rate | Detection Prevalence | Balanced Accuracy |\n|-------------------------------|-------------|-------------|----------------|----------------|-----------|--------|------|------------|----------------|----------------------|-------------------|\n| Reviewer 1 vs Consensus       | 0.77        | 0.98        | 0.97           | 0.82           | 0.97      | 0.77   | 0.86 | 0.48       | 0.37           | 0.38                 | 0.88              |""
  - ""(Page 26, Table 2) |                         | Sensitivity | Specificity | Pos Pred Value | Neg Pred Value | Precision | Recall |  F1  | Prevalence | Detection Rate | Detection Prevalence | Balanced Accuracy |\n|-------------------------|-------------|-------------|----------------|----------------|-----------|--------|------|------------|----------------|----------------------|-------------------|\n| Reviewer 1 vs Consensus |      1      |      1      |       1        |       1        |     1     |   1    |  1   |    0.76    |      0.76      |         0.76         |         1         |""
  - ""(Page 27, Table 1) | Category  | Country | Review stage automated | LLM type used | Performance metrics of LLM | Sample size | Review type automated in the study | Authors opinion on LLM | Citation to support authors opinion | Type of funding used |\n|-----------|---------|-------------------------|---------------|----------------------------|-------------|------------------------------------|------------------------|------------------------------------|----------------------|\n| Precision | 1       | 0.93                    | 0.93          | 0.8                        | 0.8         | 0.53                               | 1                      | 0.93                               | 0.73                 |""
  - ""(Page 29, Table 1) | Study            | Title                                                                                                                                  | Country/ US State | Review stage†                                                                                      | Review type†                        | LLM type†            | Performance metrics†                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n|------------------|----------------------------------------------------------------------------------------------------------------------------------------|-------------------|----------------------------------------------------------------------------------------------------|-------------------------------------|----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| Guo, 2024 [1]    | Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study                                         | Canada            | Title and abstract screening                                                                       | Systematic review, Scoping review   | GPT / ChatGPT        | GPT-4.Title and abstract screening.Accuracy=91.0; GPT-4.Title and abstract screening.F1=60.0                                                                                                                                                                                                                                                                                                                                                                               ""
  - ""(Page 54, Table 1) | Reference       | Title                                                                 | Country     | Task                                    | Type of Review       | Model         | Evaluation Metrics                                                                                                                                                                                                 | Sample Size          | Time Savings Reported | Type of Publication | Source                  | Quality | Sentiment | Conclusion                                                                                                                                                                                                                       |\n|-----------------|----------------------------------------------------------------------|-------------|-----------------------------------------|----------------------|---------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------|------------------------|---------------------|-------------------------|--------|-----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Alchekr, 2022 [168] | Supporting Systematic Literature Reviews Using Deep-Learning-Based Language Models | Germany     | Searching for publications              | Systematic review    | BERT-based    | Not mentioned / Qualitative                                                                                                                                                                                      | Not extracted/Not applicable | 174                  | Not extracted/Not applicable | Methods paper         | Unknown/unreported sources | Medium | Positive  | ""The findings indicate that using natural language-based deep-learning architectures for semi-automating the selection of primary studies can accelerate the scanning and identification process.""                             |""","- The paper provides quantitative performance metrics for GPT and BERT models in terms of accuracy, precision, and recall for different stages of the review process. GPT models are noted for their high performance in data extraction.
- The paper discusses the efficiency and scalability of using LLMs, mentioning that they can significantly reduce the time needed for reviews and improve overall efficiency.
- The use of LLMs is expected to produce similar or higher-quality data compared to manual reviews, indicating a positive impact on system reliability and robustness.
- The paper highlights the large-scale automation of review stages, including drafting and plot generation, which suggests scalability.
- The tables at the end of the paper likely contain detailed performance metrics and comparisons, but without specific content, we can infer that they support the qualitative and quantitative performance outcomes discussed in the text.","  - ""The search was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar by human reviewers.""
  - ""The review process involved three stages that were automated by Covidence add-on: abstract screening, full-text screening, and extraction.""
  - ""Initially, 3,788 studies were identified across several databases: PubMed (n = 2,174), Scopus (n = 1,207), Dimensions (n = 356), and Google Scholar (n = 48), along with 3 additional studies from citation searching.""
  - ""The data was then manually fed into the chat window by a human reviewer (DS).""
  - ""Scite.ai (version from August 2024) was used to draft parts of the introduction and discussion sections, while ChatGPT was used to draft the abstract and results section of this review by generating R code snippets to produce all figures (except Figure 1 which was generated by Covidence).""
  - ""We used our own time measurements and reference data from experienced reviewers to calculate time-saving""",,"- The paper mentions that the search was conducted in several academic databases: PubMed, Scopus, Dimensions, and Google Scholar. This indicates the primary data sources used for the study.
- The dataset size is specified as 3,788 studies initially identified, with a breakdown of the number of studies from each database.
- The data collection method involved human reviewers conducting the search and using Covidence for automated stages of the review process.
- The paper does not provide specific details on training/validation/test data splits or data preprocessing and cleaning approaches.
- External knowledge bases or ontologies are not explicitly mentioned in the context of data sources.","  - ""This study aims to summarize the usage of large language models (LLMs) in the process of creating a scientific review by looking at the methodological papers that describe the use of LLMs in review automation and the review papers that mention they were made with the support of LLMs.""
  - ""The review process involved three stages that were automated by Covidence add-on: abstract screening, full-text screening, and extraction.""
  - ""The data charting form for extraction were designed by human experts (DS, VJ, AB, LL, and NH) and adopted into the LLM prompt to collect the following primary information:

• Author, year, title;

• Country and/or US state;

• What types of reviews were automated;

• Stage of review automated in the research project;

• LLM type used;

• Performance metrics reported by authors during each stage of the review.""
  - ""The most frequently mentioned AI model is GPT/ChatGPT, with 126 occurrences (73.3%), showing its widespread use""
  - ""Majority of reviewed publications were papers describing how LLM could be used to automate a certain phase of the review (n=146, 84.9%)""
  - ""The strength of present review is in large-scale (over 3000 abstracts screened, and 172 fulltext publications eligible for extraction) automation of different stages of review, including drafting the manuscript sections, and plot generation.""
  - ""The implications of such automation include reducing human workload and improving overall efficiency of systematic reviews.""
  - ""GPT-based LLM were the most dominant type of LLM and the one that seems to show remarkable results on the data extraction, arguably the most complex and time-consuming stage of any review.""
  - ""The use of LLMs in review automation is rapidly growing, with expected radical changes in scientific evidence synthesis.""
  - ""LLMs are likely to significantly reduce the time needed for reviews while producing similar or higher-quality data in greater quantities than manual reviews.""","  - ""(Page 24, Table 1) | Table S1. LLM Prompts used for screening and extraction. ........................................... | 25 |\n|------------------------------------------------------------------------------------------------------|----|\n| Table S2. Benchmark of abstract screening phase (N=100 abstracts). ..................................| 26 |""
  - ""<table_quotation page_num=25 table_on_page=1></table_quotation>""
  - ""(Page 26, Table 1) |                               | Sensitivity | Specificity | Pos Pred Value | Neg Pred Value | Precision | Recall |  F1  | Prevalence | Detection Rate | Detection Prevalence | Balanced Accuracy |\n|-------------------------------|-------------|-------------|----------------|----------------|-----------|--------|------|------------|----------------|----------------------|-------------------|\n| Reviewer 1 vs Consensus       | 0.77        | 0.98        | 0.97           | 0.82           | 0.97      | 0.77   | 0.86 | 0.48       | 0.37           | 0.38                 | 0.88              |""
  - ""(Page 26, Table 2) |                         | Sensitivity | Specificity | Pos Pred Value | Neg Pred Value | Precision | Recall |  F1  | Prevalence | Detection Rate | Detection Prevalence | Balanced Accuracy |\n|-------------------------|-------------|-------------|----------------|----------------|-----------|--------|------|------------|----------------|----------------------|-------------------|\n| Reviewer 1 vs Consensus |      1      |      1      |       1        |       1        |     1     |   1    |  1   |    0.76    |      0.76      |         0.76         |         1         |""
  - ""(Page 27, Table 1) | Category  | Country | Review stage automated | LLM type used | Performance metrics of LLM | Sample size | Review type automated in the study | Authors opinion on LLM | Citation to support authors opinion | Type of funding used |\n|-----------|---------|-------------------------|---------------|----------------------------|-------------|------------------------------------|------------------------|------------------------------------|----------------------|\n| Precision | 1       | 0.93                    | 0.93          | 0.8                        | 0.8         | 0.53                               | 1                      | 0.93                               | 0.73                 |""
  - ""(Page 29, Table 1) | Study            | Title                                                                                                                                  | Country/ US State | Review stage†                                                                                      | Review type†                        | LLM type†            | Performance metrics†                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n|------------------|----------------------------------------------------------------------------------------------------------------------------------------|-------------------|----------------------------------------------------------------------------------------------------|-------------------------------------|----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| Guo, 2024 [1]    | Automated Paper Screening for Clinical Reviews Using Large Language Models: Data Analysis Study                                         | Canada            | Title and abstract screening                                                                       | Systematic review, Scoping review   | GPT / ChatGPT        | GPT-4.Title and abstract screening.Accuracy=91.0; GPT-4.Title and abstract screening.F1=60.0                                                                                                                                                                                                                                                                                                                                                                               ""
  - ""(Page 54, Table 1) | Reference       | Title                                                                 | Country     | Task                                    | Type of Review       | Model         | Evaluation Metrics                                                                                                                                                                                                 | Sample Size          | Time Savings Reported | Type of Publication | Source                  | Quality | Sentiment | Conclusion                                                                                                                                                                                                                       |\n|-----------------|----------------------------------------------------------------------|-------------|-----------------------------------------|----------------------|---------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------|------------------------|---------------------|-------------------------|--------|-----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Alchekr, 2022 [168] | Supporting Systematic Literature Reviews Using Deep-Learning-Based Language Models | Germany     | Searching for publications              | Systematic review    | BERT-based    | Not mentioned / Qualitative                                                                                                                                                                                      | Not extracted/Not applicable | 174                  | Not extracted/Not applicable | Methods paper         | Unknown/unreported sources | Medium | Positive  | ""The findings indicate that using natural language-based deep-learning architectures for semi-automating the selection of primary studies can accelerate the scanning and identification process.""                             |""","- The paper focuses on the use of large language models (LLMs) in automating various stages of the scientific review process, which is a key application context.
- The target research disciplines or fields are not explicitly mentioned, but the focus on systematic reviews and meta-analyses suggests a broad application across multiple fields.
- Specific literature review tasks addressed include abstract screening, full-text screening, data extraction, and drafting manuscript sections.
- The types of academic documents processed include systematic reviews, scoping reviews, meta-analyses, and other types of reviews.
- User types and requirements are not explicitly detailed, but the automation aims to reduce human workload and improve efficiency, suggesting a focus on researchers and reviewers.
- Integration with research workflows is indicated by the use of Covidence add-on for automation and the potential for reducing human workload.
- The paper does not explicitly distinguish between commercial and academic applications, but the focus on scientific reviews suggests an academic context."
Artificial Intelligence in Food Safety: A Decade Review and Bibliometric Analysis,"Zhe Liu, Shuzhe Wang, Yudong Zhang, Yichen Feng, Jiajia Liu, Hengde Zhu",10.3390/foods12061242,https://doi.org/10.3390/foods12061242,Foods,103,2023,"The core technical architecture involves the use of machine learning and deep learning frameworks, particularly CNNs and NAS for model optimization. Transfer learning is used for improving model performance with small datasets. The study employs bibliometric methods using CiteSpace for data analysis, integrating with the Web of Science database. However, specific system architecture components, technical infrastructure, or computational resources are not detailed in the paper.","- Algorithms and models used: Machine learning (ML), deep learning (DL), convolutional neural networks (CNNs), neural architecture search (NAS), transfer learning
- Feature extraction techniques: Hyperspectral and multispectral imaging
- Data processing pipelines: Use of hyperspectral and multispectral imaging for non-destructive testing
- Training methodologies: Transfer learning for small datasets
- Preprocessing steps: Not explicitly mentioned
- Technical workflow: Bibliometric analysis using CiteSpace","- Search and retrieval algorithms: Systematic visualized review and bibliometric analysis using CiteSpace.
- Relevance ranking approaches: Cluster analysis of keywords.
- Citation analysis methods: Co-citation analysis, citation burst analysis.
- Content-based matching techniques: Science mapping including co-citation and keyword occurrence analyses.
- Metadata extraction and utilization: Keyword time-zone visualization.
- Recommendation system approaches: Not explicitly mentioned.
- Expert/authority identification methods: Author co-citation analysis, journal co-citation analysis.","- Algorithm performance issues: The need for a trade-off between model performance and computational efforts.
- Data quality or availability problems: Possibility of missing data from the WoS database; analysis limited to retrieved literature samples.
- Scalability challenges: Increased computational costs and memory requirements in CNN architecture design.
- Integration difficulties: Suggestion to expand search scope and use different databases.
- Evaluation methodology limitations: Analysis based on retrieved literature samples rather than all published articles.
- Computational resource constraints: Focus on designing more efficient networks due to computational costs and memory requirements.","- New algorithmic contributions: Convolutional neural networks (CNNs) for image recognition, neural architecture search (NAS) for designing superior CNNs.
- Creative problem-solving methods: Transfer learning for small datasets, tree boosting for machine learning.
- Hybrid or ensemble approaches: Combining CNNs with other techniques.
- Novel feature engineering techniques: Advanced CNN models like DenseNet, ResNet, VGGNet.
- Innovative evaluation metrics: Accuracy, root-mean-square error (RMSE).
- Technical workarounds: Transfer learning for small datasets.
- Original system design elements: Expert systems, non-destructive detection technologies.","- Accuracy: DCNN model showed an accuracy of 98.51%, DL model obtained an accuracy of 75.1%, proposed method obtained an accuracy of 95% for foreign object detection.
- R² values: RF algorithms had R² ranging from 0.88 to 0.96.
- RMSE: RF algorithms had RMSE ranging from 0.03 to 0.07.
- Comparison with baseline methods: Expert system outperformed traditional cost minimization model.","- Academic databases used: Web of Science (WoS) Core Collection
- Dataset size and characteristics: 1855 articles from 2012 to 2022 focused on AI in food safety
- Data collection methods: SCI Expand search in WoS with filters (time span, document type, WoS categories)
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: Food safety, precision agriculture, precision nutrition
- Specific literature review tasks addressed: Analyzing historical trajectory and identifying future trends in AI technologies in food safety
- Types of academic documents processed: Articles and reviews
- User types and requirements: Researchers, practitioners, policymakers
- Integration with research workflows: Use of CiteSpace for bibliometric analysis
- Commercial vs. academic applications: Primarily academic applications","  - ""Machine learning (ML) is currently a main branch of AI, integrating probability theory, statistics, and convex optimization to resolve the problems of computer vision, speech recognition, natural language processing, robot control, etc.""
  - ""deep learning (DL) reveals excellent performance in image recognition, speech recognition, molecule prediction, particle accelerator data analysis, brain circuit reconstruction, etc.""
  - ""CNNs extract features and recognize patterns from the input images by sliding filters across the image.""
  - ""neural architecture search (NAS) has demonstrated its potential in automatically designing CNNs that are comparable or even superior to those CNNs manually designed by human experts.""
  - ""Transfer learning is constructive, especially when the dataset size is small.""
  - ""We used keyword time-zone visualization to track the themes of AI research and depicted the hot spots in each time slice, as shown in Figure 12""
  - ""We divided the selected 1855 samples into 15 fields, such as food science and technology, environmental science, remote sensing, nutrition dietetics, etc.""
  - ""we used SCI Expand search in the Core Collection database of WoS (Web of Science).""
  - ""we set the published years of the collected documents to the period from 2012 to 2022 and limited the types of documents to articles and reviews.""
  - ""We selected the keyword as the node type in CiteSpace and set the TopN to 50 and the time slice to 1.""
  - ""The institute and country co-authorship network revealed the cooperative relationships among different countries and academic units""
  - ""The results of co-analysis by institutions and countries indicated that China had paid much attention to AI applications in food safety.""
  - ""The present character, hot spots, and future research trends of AI technologies in food safety research were determined.""
  - ""Furthermore, based on our analyses, we provide researchers, practitioners, and policymakers with the big picture of research on AI in food safety across the whole process, from precision agriculture to precision nutrition, through 28 enlightening articles.""
  - ""In this review, we used bibliometric methods to describe the development of AI in food safety, including performance analysis, science mapping, and network analysis by CiteSpace.""
  - ""Among the 1855 selected articles, China and the United States contributed the most literature, and the Chinese Academy of Sciences released the largest number of relevant articles.""
  - ""Among all the journals in this field, PLoS ONE and Computers and Electronics in Agriculture ranked first and second in terms of annual publications and co-citation frequency.""
  - ""The most frequently cited journals among the selected datasets are PLoS One, Computers and Electronics in Agriculture, Nature, Remote Sensing, and Scientific Reports.""",,"- The paper discusses the use of machine learning (ML) and deep learning (DL) as the main AI frameworks, with specific mention of convolutional neural networks (CNNs) and their architectures like DenseNet, ResNet, and VGGNet. These are used for tasks such as image recognition and pattern recognition.
- Neural architecture search (NAS) is highlighted as a method for automatically designing CNNs, indicating an advanced approach to optimizing AI models.
- Transfer learning is mentioned as a technique to improve model performance, especially when dealing with small datasets.
- The paper uses bibliometric methods and tools like CiteSpace for data analysis, which involves performance analysis, science mapping, and network analysis.
- The study relies on the Web of Science (WoS) database for literature collection, indicating integration with existing academic databases.
- The paper does not explicitly mention specific system architecture components like databases, APIs, or cloud platforms, nor does it detail the computational resources used.
- The focus is on analyzing research trends and contributions in AI applications for food safety, rather than on a specific technical system or infrastructure.","  - ""Machine learning (ML) is currently a main branch of AI, integrating probability theory, statistics, and convex optimization to resolve the problems of computer vision, speech recognition, natural language processing, robot control, etc.""
  - ""deep learning (DL) reveals excellent performance in image recognition, speech recognition, molecule prediction, particle accelerator data analysis, brain circuit reconstruction, etc.""
  - ""neural architecture search (NAS) has demonstrated its potential in automatically designing CNNs that are comparable or even superior to those CNNs manually designed by human experts.""
  - ""Transfer learning is constructive, especially when the dataset size is small.""
  - ""In the subfield of food science technology, hyperspectral and multispectral imaging were proven to be effective non-destructive detection techniques for assessing food quality objectively and accurately and may be of interest in computer vision applications for the precise prediction of bacterial loads""
  - ""In the subfield of nutrition dietetics, Zeevi et al. ( 2015) designed an algorithm that combined personal metrics with behaviour habits to customize dietary intake by predicting glycemic responses""
  - ""In the subfield of remote sensing, Mosleh et al. (2015) reviewed the fusion of optical images, microwave technologies, and AI methods for mapping areas and forecasting yield""
  - ""In the subfield of environmental sciences, Xiong et al. (2017) proposed a comprehensive approach using pixel-based classification and object-based segmentation for mapping the geographical extent of croplands, which was of great importance for field management""
  - ""Recently, machine learning has been increasingly applied to explore the rapidly increasing foodborne pathogen genome resources and their metadata. Tree boosting is a highly effective machine-learning method and has been widely used by researchers""
  - ""Journal co-citation analysis revealed the structure and distribution of knowledge by displaying the network of the co-cited journals.""
  - ""The institute and country co-authorship network revealed the cooperative relationships among different countries and academic units""
  - ""We used keyword time-zone visualization to track the themes of AI research and depicted the hot spots in each time slice,""
  - ""We divided the selected 1855 samples into 15 fields, such as food science and technology, environmental science, remote sensing, nutrition dietetics, etc.""
  - ""Then, we further screened 143 highly cited and high-quality articles by network metrics and gained seven thematic clusters.""
  - ""Finally, 28 articles were selected and studied to present the AI applications in food safety across the whole process from precision agriculture to precision nutrition,""","  - ""(Page 20, Table 1) | Field                      | Sample                                | Functionality                              | Method(s)                                      | Result(s)                                                                                                                                                  |\n|----------------------------|---------------------------------------|--------------------------------------------|-----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Molecular Breeding         | Crops (Yan et al., 2021) [108]        | Genomic prediction                         | LightGBM                                      | LightGBM exhibited superior performance of genomic selection prediction.                                                                                   |""
  - ""(Page 21, Table 1) | Field                        | Sample                                      | Functionality                        | Method(s)                          | Result(s)                                                                                                                                  |\n|------------------------------|---------------------------------------------|--------------------------------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|\n| Food Processing and Distrib-ution | Antioxidant peptide (Shen et al., 2022) [103] | Feature extraction                   | LR, LDA, SVM, KNN                  | The ML-based predictor was effective in mining the multifunctional peptides.                                                              |""
  - ""(Page 24, Table 1) Abbreviations\n|-----------|---------------------------------------------------------------|\n| AB        | adaptive boosting                                             |""
  - ""(Page 25, Table 1) | Acronym  | Description                                           |\n|----------|-------------------------------------------------------|\n| GBLUP    | genomic best linear unbiased predictor                |""","- The paper discusses various AI technologies and their applications in food safety, including machine learning (ML) and deep learning (DL).
- Specific algorithms and models mentioned include convolutional neural networks (CNNs), neural architecture search (NAS), and transfer learning.
- Feature extraction techniques are implied through the use of CNNs for image recognition and other applications.
- Data processing pipelines involve the use of hyperspectral and multispectral imaging for non-destructive testing.
- Training methodologies include the use of transfer learning for small datasets.
- Preprocessing steps are not explicitly mentioned but are likely involved in preparing data for ML and DL models.
- The technical workflow involves bibliometric analysis using CiteSpace to map the development of AI in food safety.","  - ""To discover the historical trajectory and identify future trends, we analysed the literature concerning AI technologies in food safety from 2012 to 2022 by CiteSpace.""
  - ""In this review, we used bibliometric methods to describe the development of AI in food safety, including performance analysis, science mapping, and network analysis by CiteSpace.""
  - ""Our literature review has two steps: a systematic visualised review and a bibliometric analysis.""
  - ""In contrast to the prior studies [25,26] in AI research using a natural language processing toolkit or heuristic approach, we analysed the relevant data and the visual networks by CiteSpace to explore historical origins, evolutionary trajectories, and future trends in our research.""
  - ""We divided the selected 1855 samples into 15 fields, such as food science and technology, environmental science, remote sensing, nutrition dietetics, etc. After the descriptive analysis, we deployed science mapping [30] , including co-citation and keyword occurrence analyses.""
  - ""we further screened 143 highly cited and high-quality articles by network metrics and gained seven thematic clusters.""
  - ""We used keyword time-zone visualization to track the themes of AI research and depicted the hot spots in each time slice, as shown in Figure 12""
  - ""Through cluster analysis of the keyword, we summarized the research hotspots of AI technologies in food safety.""
  - ""Journal co-citation analysis revealed the structure and distribution of knowledge by displaying the network of the co-cited journals.""
  - ""The institute and country co-authorship network revealed the cooperative relationships among different countries and academic units""
  - ""In order to explore the topological features and historical trajectory of a single theme or area, we can analyse the evolution of co-keyword networks as well as keyword cooccurrence networks [49].""
  - ""We selected the keyword as the node type in CiteSpace and set the TopN to 50 and the time slice to 1.""
  - ""Citation burst analysis [34] was used to search the representative articles with high citation growth rates.""
  - ""Reference co-citation networks represented the structure and development of a field in detail, which was composed of the nodes and connections among co-cited references [32].""
  - ""Author co-citation analysis depicted the co-citation author network generated by CiteSpace to show the interrelationships among cited authors and identify the authors with strong influence [31].""",,"- The paper uses CiteSpace for bibliometric analysis, which is a tool for visualizing and analyzing scientific literature. This suggests the use of citation analysis methods.
- The authors conducted a systematic visualized review and bibliometric analysis, indicating the use of search and retrieval algorithms to gather relevant papers.
- Science mapping, including co-citation and keyword occurrence analyses, was used to identify research hotspots and trends. This is a form of content-based matching technique.
- The paper mentions the use of keyword time-zone visualization to track themes and hotspots, which is a method for metadata extraction and utilization.
- Cluster analysis of keywords was used to summarize research hotspots, indicating a method for relevance ranking approaches.
- Journal co-citation analysis and author co-citation analysis were used to reveal knowledge structures and author influences, which are methods for expert/authority identification.
- Citation burst analysis was used to identify articles with high citation growth rates, which is a method for finding core papers.","  - ""Despite this, this approach inevitably increases computational costs and memory requirements.""
  - ""Much effort has been made to strike an excellent trade-off between model performance and computation efforts.""
  - ""The focus of CNN architecture design has shifted from designing deeper and more complex networks to designing more efficient networks with high performance, either manually or automatically.""
  - ""Although the database used in our bibliometrics research was powerful and widely used by many researchers, the possibility of missing data from the WoS cannot be ruled out.""
  - ""Moreover, all the analysis work in this paper was based on the literature samples we retrieved rather than all published articles in the field of AI.""
  - ""In addition, with the breakthrough in AI technologies, more and more fields have begun to emphasize AI applications. Some research on AI that indirectly impacts the field of food safety may not appear in the article title, abstract, and keywords.""
  - ""Therefore, to fully interpret the future applications of AI, we suggested that subsequent researchers expand the search scope and make the literature samples for analysis more comprehensive, which is helpful for gaining some insights from the literature distributed in other fields.""
  - ""Furthermore, we suggested that different databases (such as Google Scholar) be used to search for literature samples in future research.""",,"- The paper discusses computational costs and memory requirements as a challenge in designing CNN architectures, indicating a scalability challenge.
- The need for a trade-off between model performance and computational efforts suggests algorithm performance issues.
- The mention of missing data from the WoS database and the limitation of analysis to retrieved literature samples indicates data quality or availability problems.
- The suggestion to expand search scope and use different databases implies integration difficulties and limitations in evaluation methodology.
- The focus on designing more efficient networks suggests computational resource constraints.
- The paper does not explicitly mention technical bottlenecks or failure points, but the challenges in scalability and computational resources imply potential bottlenecks.","  - ""Machine learning (ML) is currently a main branch of AI, integrating probability theory, statistics, and convex optimization to resolve the problems of computer vision, speech recognition, natural language processing, robot control, etc.""
  - ""deep learning (DL) reveals excellent performance in image recognition, speech recognition, molecule prediction, particle accelerator data analysis, brain circuit reconstruction, etc.""
  - ""neural architecture search (NAS) has demonstrated its potential in automatically designing CNNs that are comparable or even superior to those CNNs manually designed by human experts.""
  - ""Transfer learning is constructive, especially when the dataset size is small.""
  - ""Tree boosting is a highly effective machine-learning method and has been widely used by researchers""
  - ""The DCNN model showed the highest performance for protein estimation.""","  - ""(Page 20, Table 1) | Field                      | Sample                                | Functionality                              | Method(s)                                      | Result(s)                                                                                                                                                  |\n|----------------------------|---------------------------------------|--------------------------------------------|-----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Molecular Breeding         | Crops (Yan et al., 2021) [108]        | Genomic prediction                         | LightGBM                                      | LightGBM exhibited superior performance of genomic selection prediction.                                                                                   |""
  - ""(Page 21, Table 1) | Field                        | Sample                                      | Functionality                        | Method(s)                          | Result(s)                                                                                                                                  |\n|------------------------------|---------------------------------------------|--------------------------------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|\n| Food Processing and Distrib-ution | Antioxidant peptide (Shen et al., 2022) [103] | Feature extraction                   | LR, LDA, SVM, KNN                  | The ML-based predictor was effective in mining the multifunctional peptides.                                                              |""","- The paper discusses various innovative solutions in AI technologies applied to food safety, including machine learning and deep learning techniques.
- New algorithmic contributions are highlighted, such as the use of convolutional neural networks (CNNs) for image recognition and neural architecture search (NAS) for designing superior CNNs.
- Creative problem-solving methods include the application of transfer learning to overcome small dataset issues and the use of tree boosting for machine learning tasks.
- Hybrid or ensemble approaches are mentioned, such as combining CNNs with other techniques for improved performance.
- Novel feature engineering techniques are implied through the use of advanced CNN models like DenseNet, ResNet, and VGGNet.
- Innovative evaluation metrics are not explicitly mentioned, but the paper discusses various performance metrics such as accuracy and root-mean-square error (RMSE).
- Technical workarounds for known problems include using transfer learning to address small dataset sizes.
- Original system design elements are evident in the development of expert systems and novel non-destructive detection technologies.","  - ""The DCNN model showed the highest performance for protein estimation.""","  - ""(Page 21, Table 1) | Field                        | Sample                                      | Functionality                        | Method(s)                          | Result(s)                                                                                                                                  |\n|------------------------------|---------------------------------------------|--------------------------------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|\n| Food Processing and Distrib-ution | Antioxidant peptide (Shen et al., 2022) [103] | Feature extraction                   | LR, LDA, SVM, KNN                  | The ML-based predictor was effective in mining the multifunctional peptides.                                                              |""
  - ""(Page 20, Table 1) | Field                      | Sample                                | Functionality                              | Method(s)                                      | Result(s)                                                                                                                                                  |\n|----------------------------|---------------------------------------|--------------------------------------------|-----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Molecular Breeding         | Crops (Yan et al., 2021) [108]        | Genomic prediction                         | LightGBM                                      | LightGBM exhibited superior performance of genomic selection prediction.                                                                                   |""","- The paper provides various performance metrics for different AI models and applications in food safety. These include accuracy, R² values, and RMSE, which are common metrics for evaluating model performance.
- The accuracy of several models is reported, such as the DCNN model showing an accuracy of 98.51% and the DL model obtaining an accuracy of 75.1%.
- The paper also mentions the performance of other models like RF, which had R² values ranging from 0.88 to 0.96 and RMSE from 0.03 to 0.07.
- The comparison with baseline methods is indicated by statements like ""The expert system outperformed the traditional cost minimization model.""
- The paper does not provide specific user satisfaction or usability results, nor does it mention system reliability and robustness measures or scalability test results.
- The tables at the end of the paper likely contain more detailed performance results, but as per the instructions, only the table references are provided without content.","  - ""To discover the historical trajectory and identify future trends, we analysed the literature concerning AI technologies in food safety from 2012 to 2022 by CiteSpace.""
  - ""Among the 1855 selected articles, China and the United States contributed the most literature, and the Chinese Academy of Sciences released the largest number of relevant articles.""
  - ""In the process of literature collection, we used SCI Expand search in the Core Collection database of WoS (Web of Science).""
  - ""A total of 2378 documents were obtained according to the literature review search process.""
  - ""Based on bibliometric analysis [28,29], when conducting literature retrieval in WoS, we set three criteria for sample screening, namely ""time span"", ""document type"", and ""WoS categories"".""
  - ""Finally, 1855 primary literature samples of research on AI in the food safety field from 2012 to 2022 and 55,282 references to these pieces of literature were generated.""
  - ""We divided the selected 1855 samples into 15 fields, such as food science and technology, environmental science, remote sensing, nutrition dietetics, etc.""
  - ""We used keyword time-zone visualization to track the themes of AI research and depicted the hot spots in each time slice, as shown in Figure 12""
  - ""We selected the keyword as the node type in CiteSpace and set the TopN to 50 and the time slice to 1.""
  - ""The institute and country co-authorship network revealed the cooperative relationships among different countries and academic units [48].""
  - ""The results of co-analysis by institutions and countries indicated that China had paid much attention to AI applications in food safety.""",,"- The paper uses the Web of Science (WoS) database for literature collection, specifically the Core Collection.
- The dataset size is 1855 selected articles, which were filtered from an initial 2378 documents based on specific criteria.
- The data collection method involved using the SCI Expand search in WoS and applying filters such as time span, document type, and WoS categories.
- The dataset is characterized by its focus on AI technologies in food safety from 2012 to 2022.
- The paper does not mention specific training/validation/test data splits or data preprocessing and cleaning approaches, as these are not typically relevant to bibliometric analyses.
- The paper uses CiteSpace for bibliometric analysis, which is a tool for visualizing and analyzing citation networks.
- The paper does not mention any external knowledge bases or ontologies used in the analysis.","  - ""Artificial Intelligence (AI) technologies have been powerful solutions used to improve food yield, quality, and nutrition, increase safety and traceability while decreasing resource consumption, and eliminate food waste.""
  - ""To discover the historical trajectory and identify future trends, we analysed the literature concerning AI technologies in food safety from 2012 to 2022 by CiteSpace.""
  - ""Among the 1855 selected articles, China and the United States contributed the most literature, and the Chinese Academy of Sciences released the largest number of relevant articles.""
  - ""Among all the journals in this field, PLoS ONE and Computers and Electronics in Agriculture ranked first and second in terms of annual publications and co-citation frequency.""
  - ""The present character, hot spots, and future research trends of AI technologies in food safety research were determined.""
  - ""Furthermore, based on our analyses, we provide researchers, practitioners, and policymakers with the big picture of research on AI in food safety across the whole process, from precision agriculture to precision nutrition, through 28 enlightening articles.""
  - ""We divided the selected 1855 samples into 15 fields, such as food science and technology, environmental science, remote sensing, nutrition dietetics, etc.""
  - ""the top five discipline categories with the highest number of publications were food science technology (410 articles), environmental sciences (280 articles), remote sensing (215 articles), nutrition dietetics (193 articles) , and imaging science information systems (179 articles).""
  - ""The institute and country co-authorship network revealed the cooperative relationships among different countries and academic units""
  - ""The results of co-analysis by institutions and countries indicated that China had paid much attention to AI applications in food safety.""
  - ""This review should be helpful for researchers and practitioners to comprehensively understand the application status of AI technologies in the food sector;""
  - ""this review may be helpful for researchers and practitioners to comprehensively understand the application status of AI technologies in the food sector.""
  - ""The research results should be helpful to researchers and practitioners in considering where to get more cooperation opportunities and where to seek more valuable information and help.""","  - ""(Page 12, Table 1) | Topic           | Keyword                  | Author(s)                   | Journal                                                                 |\n|-----------------|--------------------------|-----------------------------|-------------------------------------------------------------------------|\n| Remote sensing  | Food security            | Maimaitijiang et al., 2019 [58] | 1. *Remote Sensing of Environment*                                      |""
  - ""(Page 13, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 14, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 15, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 16, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 17, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 18, Table 1) | Topic | Keyword | Author(s)            | Journal                                                   |\n|-------|---------|----------------------|-----------------------------------------------------------|\n|       |         | Tao et al., 2020 [191] | 138. *Sensors*                                             |""
  - ""(Page 20, Table 1) | Field                      | Sample                                | Functionality                              | Method(s)                                      | Result(s)                                                                                                                                                  |\n|----------------------------|---------------------------------------|--------------------------------------------|-----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Molecular Breeding         | Crops (Yan et al., 2021) [108]        | Genomic prediction                         | LightGBM                                      | LightGBM exhibited superior performance of genomic selection prediction.                                                                                   |""
  - ""(Page 21, Table 1) | Field                        | Sample                                      | Functionality                        | Method(s)                          | Result(s)                                                                                                                                  |\n|------------------------------|---------------------------------------------|--------------------------------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|\n| Food Processing and Distrib-ution | Antioxidant peptide (Shen et al., 2022) [103] | Feature extraction                   | LR, LDA, SVM, KNN                  | The ML-based predictor was effective in mining the multifunctional peptides.                                                              |""","- The paper focuses on the application of AI in food safety, which is a specific research discipline or field.
- The literature review tasks addressed include analyzing the historical trajectory and identifying future trends in AI technologies in food safety.
- The types of academic documents processed include articles and reviews, as indicated by the selection of 1855 articles for analysis.
- The user types and requirements include researchers, practitioners, and policymakers who need to understand the application status and future trends of AI in food safety.
- The integration with research workflows is evident in the use of CiteSpace for bibliometric analysis and the identification of hotspots and future trends.
- The paper is primarily focused on academic applications, as it is a review of research literature and trends in AI technologies in food safety."
A Systematic Review on Literature-based Discovery,"M. Thilakaratne, K. Falkner, Thushari Atapattu",10.1145/3365756,https://doi.org/10.1145/3365756,ACM Computing Surveys,30,2019,"- Main AI/ML frameworks used: NLP, knowledge discovery techniques
- System architecture components: Medline/PubMed, patent databases, non-traditional data sources (Twitter, Google news), graph-based tools (Neo4j, JUNG, Gephi, NetworkX, LGL)
- Technical infrastructure: Not explicitly mentioned
- Integration with existing academic databases or platforms: Medline/PubMed, patent databases
- Overall system design approach: Focus on structured data sources, graph-based methods for association representation, machine learning for ranking associations","- Algorithms and models used: ABC model (open discovery and closed discovery), AnC model, combination of open and closed discovery models, context-based ABC model, context-assignment-based ABC model, storytelling methodologies, analogy mining, outlier detection, gaps characterisation, negative consensus analysis.
- Feature extraction techniques: UMLS and MeSH for term representation in medical domains; n-grams for other domains.
- Data processing pipelines: Filtering techniques (stop word removal, semantic category filtering, relation/predicate type filtering, hierarchical filtering, synonym mapping); ranking techniques (conventional statistical measures like token frequency, TF-IDF; non-conventional measures like average minimum weight and linking term count with AMW).
- Training methodologies: Not explicitly mentioned.
- Preprocessing steps: Filtering techniques to reduce search space.
- Technical workflow or methodology: ABC model variants; filtering and ranking techniques; evaluation techniques like replicating existing medical discoveries and time-sliced evaluation.","- Search and retrieval algorithms: Use of titles, abstracts, full-text articles, keywords, and metadata.
- Relevance ranking approaches: Filtering techniques such as stop words removal, semantic category filtering, relation/predicate type filtering, hierarchical filtering, synonym mapping, and time-based filtering.
- Citation analysis methods: Network/graph-based measures and knowledge-based measures for ranking associations.
- Content-based matching techniques: Use of UMLS and MeSH for term representation.
- Metadata extraction and utilization: Use of metadata such as author details and publisher details for additional clues in knowledge discovery.
- Recommendation system approaches: Various ranking techniques such as conventional statistical measures (e.g., TF-IDF, mutual information) and non-conventional measures (e.g., AMW, LTC-AMW).
- Expert/authority identification methods: Not explicitly mentioned in the paper.","- Algorithm performance issues: The ABC model suffers from limitations such as exponential expansion of search space and difficulty in manual interpretation.
- Data quality or availability problems: Lack of gold standard datasets and formal evaluation techniques for LBD results.
- Scalability challenges: Increasing volume of data to be analyzed over time affects computational resource constraints.
- Evaluation methodology limitations: Challenges in evaluating LBD results due to lack of standard datasets and techniques.
- Computational resource constraints: Scalability analysis needed for handling large datasets.","- New algorithmic contributions: Alternative discovery models beyond the ABC model.
- Creative problem-solving methods: Use of non-traditional data sources like Twitter and FDA drug labels.
- Hybrid or ensemble approaches: Integration of multiple data sources and techniques.
- Novel feature engineering techniques: Deeper language analysis for validation.
- Innovative evaluation metrics: Time-sliced evaluation and user-based validation.
- Technical workarounds for known problems: Addressing limitations of time-sliced evaluation.
- Original system design elements: Emphasis on simplicity in user interfaces and user interaction.","- Accuracy, precision, recall, F1-scores: Precision, recall, F-measure, and Area Under Curve (AUC) are used to evaluate LBD systems.
- Processing speed and efficiency metrics: Processing time analysis is critical for quick results; storage analysis is important for large datasets.
- Comparison with baseline methods: Evaluation techniques include replicating existing medical discoveries and time-sliced evaluation.
- User satisfaction or usability results: User-based evaluation and user-experience evaluation are essential for improving LBD systems.
- System reliability and robustness measures: Scalability is a concern due to increasing data volumes.","- Academic databases used: Medline/PubMed, PubMed Central, Science Direct, Web of Science, IEEE Xplore Digital Library, Engineering Village, ProQuest, EBSCO Host, INSPEC
- Dataset sizes and characteristics: 176 papers obtained for the review
- Data collection methods: Six keywords used to search in title, abstract, or keywords; three-stage article selection process
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: UMLS, MeSH, SemMedDB/Semantic Medline, Gene Ontology, Entrez Gene Database, HUGO/HGNC, UNIPROT, Therapeutic Target Database (TTD), LocusLink, Online Mendelian Inheritance in Man (OMIM), Drug Bank, Comparative Toxicogenomics Database (CTD), BioGRID","- Target research disciplines or fields: Medicine
- Specific literature review tasks addressed: Formulating and evaluating novel research hypotheses by detecting hidden associations
- Types of academic documents processed: Research papers (titles, abstracts, full-text), patents
- User types and requirements: Researchers
- Integration with research workflows: Assisting in hypothesis generation and research development
- Commercial vs. academic applications: Academic context","  - ""LBD is a knowledge discovery workflow that automatically detects significant, implicit knowledge associations hidden in fragmented knowledge areas by analysing existing scientific literature.""
  - ""the LBD output not only assists in formulating scientifically sensible, novel research hypotheses but also encourages the development of cross-disciplinary research.""
  - ""In this systematic review, we provide an in-depth analysis of the computational techniques used in the LBD process using a novel, up-to-date, and detailed classification.""
  - ""The popular ABC model has two variants named as open discovery and closed discovery.""
  - ""the LBD process starts with user-defined A-concept and C-concept and the output will be the intermediate B-concepts that represents the associations between the two user-defined domains.""
  - ""Medline/PubMed is extensively being used as the main data source of the LBD literature""
  - ""The importance of using the aforementioned resources in the LBD system is that they support the system's functionality not only to medical domain, but also to a wide variety of other domains.""
  - ""The algorithm used to rank the detected associations plays a vital role in LBD methodology.""
  - ""Evaluate ranking positions: Most of the studies have evaluated the ranking positions of the LBD output to verify the effectiveness of their ranking algorithm.""
  - ""The typical output of the LBD process is a ranked list of terms that denote the potential associations.""
  - ""The output component of LBD workflow, is largely neglected in the prevailing literature which emphasises the necessity of conducting user-interaction studies to assess the user experience.""","  - ""(Page 1, Table 1) ""
  - ""(Page 5, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 19, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 25, Table 1) | Evaluation Technique                                      | Category1 | Category2 |\n|-----------------------------------------------------------|-----------|-----------|\n| **Evidences-based Evaluation:**                           |           |           |""
  - ""(Page 26, Table 1) Below is a rendering of the page up to the first error.""","- The paper discusses Literature-Based Discovery (LBD) as a workflow that uses computational techniques to analyze scientific literature and detect implicit knowledge associations. This suggests a focus on natural language processing (NLP) and knowledge discovery techniques.
- The ABC model and its variants are mentioned as part of the LBD process, indicating a structured approach to identifying associations between concepts.
- The paper highlights various data sources used in LBD, including Medline/PubMed, patent databases, and non-traditional sources like Twitter and Google news. This indicates integration with existing academic databases and platforms.
- The use of databases like Medline/PubMed and patent databases suggests a reliance on structured data sources for knowledge discovery.
- The mention of tools like Neo4j, JUNG, Gephi, NetworkX, and Large Graph Layout (LGL) for network construction and visualization implies a focus on graph-based methods for representing associations.
- The paper discusses ranking algorithms and their evaluation, indicating a focus on machine learning techniques to prioritize associations.
- The output of the LBD process is typically a ranked list of terms, suggesting a structured format for presenting discovered associations.
- The paper does not explicitly mention deep learning or specific AI/ML frameworks, but the focus on NLP and knowledge discovery suggests a technical architecture that likely includes these elements.","  - ""LBD is a knowledge discovery workflow that automatically detects significant, implicit knowledge associations hidden in fragmented knowledge areas by analysing existing scientific literature.""
  - ""The popular ABC model has two variants named as open discovery and closed discovery.""
  - ""Open discovery is generally used when there is a single problem with limited knowledge about what concepts can be involved.""
  - ""closed discovery model attempts to discover novel implicit associations between the initially mentioned A-concept and C-concept (a.k.a concept bridges).""
  - ""the LBD process starts with user-defined A-concept and C-concept and the output will be the intermediate B-concepts that represents the associations between the two user-defined domains.""
  - ""Even though the prevalent ABC model have contributed in numerous ways to detect new knowledge, it is merely one of several different types of discovery models that facilitates LBD process.""
  - ""In this regard, Smalheiser (2012) points out the importance of thinking beyond the ABC formulation and experimenting alternative discovery models in the discipline.""
  - ""The data types used in the LBD literature can be categorised as follows.

Title only: Some LBD studies (Swanson & Smalheiser, 1997;Cherdioui & Boubekeur, 2013) have only considered the article title as the input of the knowledge discovery process.""
  - ""Title and Abstract: The most common data type selection in literature is using both title and abstract (Lever et al., 2017; Sebastian, Siew & Orimaye, 2017b)""
  - ""Full-text: Few studies (Lever et al., 2017;Vicente-Gomila, 2014) have considered the entire content of articles as their input type.""
  - ""Selected articles only: While most of the studies have used data retrieved from literature database search engines (e.g., Medline) for analysis, Cameron et al. Cameron et al. (2015) have only considered the reference lists of Swanson's LBD publications.""
  - ""Entire literature database: Several research studies (Lever et al., 2017;Yang et al., 2017) have considered the entire literature database as the LBD input without only limiting to articles retrieved for a given query (e.g., subset of the articles retrived for the query 'Fish oil').""
  - ""Keywords: Some research approaches have employed the keywords of the articles as the input data type (Pusala et al., 2017;Hu et al., 2010).""
  - ""Other metadata: Few studies have analysed other metadata of the research articles such as author details (Sebastian, Siew & Orimaye, 2017b) , publisher details (Sebastian, Siew & Orimaye, 2015) and reference details (Kostoff et al., 2008a) to glean additional clues for the possible links in the knowledge discovery process.""
  - ""Other traditional data types: While the majority of the studies have focused only on the analysis of research papers, some approaches have been conducted using other traditional data types such as patents (Vicente-Gomila, 2014;Maciel et al., 2011), and TREC MedTrack collection of clinical patient records (Symonds, Bruza & Sitbon, 2014), case reports (Smalheiser, Shao & Philip, 2015) as their input to the LBD process.""
  - ""Non-traditional data types: Few research studies have attempted to perform LBD using non-traditional data types such as tweets (Bhattacharya & Srinivasan, 2012), Food and Drug Administration (FDA) drug labels (Bisgin et al., 2011), Popular Medical Literature (PML) news articles (Maclean & Seltzer, 2011), web content (Gordon, Lindsay & Fan, 2002), crime incident reports (Schroeder et al., 2007) and commission reports (Jha & Jin, 2016a)""
  - ""The Data unit of analysis denotes the types of data extracted from the above-discussed data types to represent the knowledge associations.""
  - ""Since most of the LBD research performed in medicine, the most common term representation is using UMLS and MeSH (Lever et al., 2017;Preiss & Stevenson, 2017).""
  - ""LBD studies in other domains mainly consider word or word phrases (n-grams) as their term representation (Qi & Ohsawa, 2016) that have been extracted using techniques such as Part-Of-Speech (POS) tag patterns.""
  - ""Medline/PubMed is extensively being used as the main data source of the LBD literature (Lever et al., 2017).""
  - ""The patent-based LBD studies (Vicente-Gomila, 2014), have considered patent databases such as Thomson Innovation, United State Patent and Trade Mark Office (USPTO) and MAtrixware REsearch Collection (MAREC) patent document collection to retrieve the data.""
  - ""Other conventional data sources include clinical datasets (Dong et al., 2014), Gene Expression Omnibus (GEO) database (Hristovski et al., 2010), ArrayExpress (AE) database (Maver et al., 2013), Manually Annotated Target and Drug Online Resource (MATADOR) (Crichton et al., 2018), Biological General Repository for Interaction Datasets (BioGRID) (Crichton et al., 2018), PubTator (Crichton et al., 2018), Online Mendelian Inheritance in Man (OMIM) (Cohen et al., 2010) and TREC (Symonds, Bruza & Sitbon, 2014).""
  - ""Few non-English data sources such as Chinese Social Sciences Citation Index (Su & Zhou, 2009), China Biology Medicine disks (Qian, Hong & An, 2012), Chinese Medicine Library and Information System (Yao et al., 2008), Traditional Chinese Medicine Database (Gao et al., 2016) and Chinese Journal Full-text database (Yao et al., 2008) have also been utilised in LBD workflow.""
  - ""The studies that have attempted to perform LBD in a non-traditional setting have extracted data from a variety of sources such as Twitter (Bhattacharya & Srinivasan, 2012), DailyMed: FDA drug labels (Bisgin et al., 2011), Google news (Maclean & Seltzer, 2011), and World Wide Web (WWW) (Gordon, Lindsay & Fan, 2002).""
  - ""It is vital to provide a concise output to the user that is easily interpretable by only including the most promising knowledge associations.""
  - ""the search space of the knowledge discovery should be reduced by eliminating spurious, general, uninteresting, or Stop word Removal: Stop words typically denote non-topic general English terms.""
  - ""Stop word list could be either manually created, obtained from other resources, or automatically generated.""
  - ""Semantic Category Filter: This technique typically utilises the semantic type or group information provided by UMLS (Lever et al., 2017;Vlietstra et al., 2017).""
  - ""Relation/predicate Type Filter: This filtering technique mostly consider the predications assigned using SemRep (Cameron et al., 2015;Rastegar-Mojarad et al., 2015).""
  - ""Hierarchical Filter: This technique utilises the hierarchical information such as levels and relationships of terms to filter out uninformative associations (Shang et al., 2014).""
  - ""Synonym Mapping: Mapping synonyms by grouping exactly or nearly equal terms of a given term is another technique used to reduce the results (Lever et al., 2017;Baek et al., 2017).""
  - ""Several studies have utilised POS tags to restrict the search space by limiting the terms to nouns (Qi & Ohsawa, 2016), nominal phrases (Ittipanuvat et al., 2014) or verbs (Kim et al., 2016)""
  - ""Template-based Restriction: Some studies (Maver et al., 2013;Cohen et al., 2012) have reduced the search space by only extracting the associations that adhere to the imposed rules/templates.""
  - ""Time-based Filter: Smalheiser (2005) have considered the time factor of the associations to reduce the search space of results.""
  - ""Common Base Form: Deriving a common base form of the term to reduce the vocabulary space is another technique used in the literature.""
  - ""Article Retrieval Filter: Several studies (Cherdioui & Boubekeur, 2013;Ittipanuvat et al., 2014) have attempted to limit the number of articles that need to be analysed through the LBD process to reduce the search space.""
  - ""Sentence Filter: Some studies (Hossain et al., 2012;Özgür et al., 2010) have only picked specific sentences from the text to analyse.""
  - ""Network-based Filter: The network-based LBD approaches have utilised different techniques to reduce the size of the network.""
  - ""Term Restrictions: Some studies have restricted terms in word-level and character-level to reduce the vocabulary space.""
  - ""Cohesion-based filter: Given two linking terms that are most similar, Smalheiser (2005) hypothesises that the term with a more narrow focus is the most useful.""
  - ""Expert/user-based filtering: Expert/user-based filtering (Gubiani et al., 2017;Preiss & Stevenson, 2017) involves the decision of an expert/user to remove uninteresting associations.""
  - ""Term ranking/thresholding is an important component of the LBD process as it should downweigh or remove noisy associations and upweight or retain the interesting and significant knowledge associations when ordering the terms.""
  - ""Considering conventional statistical measures to rank/threshold terms is common in literature.""
  - ""Examples for conventional statistical measures used in LBD studies are; Token frequency (Gordon & Lindsay, 1996), Average token frequency (Ittipanuvat et al., 2014), Relative token frequency (Lindsay, 1999), Document/record frequency (Gordon & Lindsay, 1996), Average Document Frequency (Ittipanuvat et al., 2014), Relative Document Frequency (Thaicharoen et al., 2009), TF-IDF (Maciel et al., 2011), Mutual Information (Loglisci & Ceci, 2011 ), z-score (Yetisgen-Yildiz & Pratt, 2006) , Information Flow (Bruza et al., 2006), Information Gain (Pusala et al., 2017), Odds Ratio (Bruza et al., 2006), Log Likelihood (Bruza et al., 2006), Support (Hristovski et al., 2005), Confidence (Hristovski et al., 2003), F-value of support and confidence (Hu et al., 2010), Chi-Square (Jha & Jin, 2016b) , Kulczynski (Jha & Jin, 2016a) , Cosine (Baek et al., 2017), Equivalence Index (Stegmann & Grohmann, 2003), Coherence (Pusala et al., 2017), Conviction (Pusala et al., 2017), Klosgen (Pusala et al., 2017), Least Contradiction (Pusala et al., 2017), Linear-Correlation (Pusala et al., 2017), Loevinger (Pusala et al., 2017), Odd Multiplier (Pusala et al., 2017), Piatetsky-Shapiro (Pusala et al., 2017), Sebag-Schoenauer (Pusala et al., 2017), Zhang (Pusala et al., 2017), Jaccard Index (Yang et al., 2017), Dice Coefficient (Yang et al., 2017), and Conditional Probability (Seki & Mostafa, 2009).""
  - ""Additionally, non-conventional statistical measures such as such as Average Minimum Weight (AMW) (Yetisgen-Yildiz & Pratt, 2009), Linking Term Count with AMW (LTC-AMW) (Yetisgen-Yildiz & Pratt, 2009), Averaged Mutual Information Measure (AMIM) (Wren, 2004), Minimum Mutual Information Measure (MMIM) (Wren, 2004) have also been proposed in discipline to rank the potential associations.""
  - ""Nearest Neighbours: In this category, the score of the association is decided by analysing its nearest neighbours.""
  - ""Network/Graph-based Measures: Network/graph-based measures analyse node and edgelevel attributes to score the associations.""
  - ""Knowledge-based Measures: This category denotes the scoring measures such as MeSHbased Literature cohesiveness (Swanson, Smalheiser & Torvik, 2006), semantic type cooccurrence (Jha & Jin, 2016b) , chemDB atomic count (Ijaz, Song & Lee, 2010), and chemDB XLogP (Ijaz, Song & Lee, 2010) that involve the knowledge from structured resources to rank the associations.""
  - ""Relations-based Measures: Relations/predicate based measures (a sub-class of knowledgebased measures) analyse the relations extracted from resources such as SemRep to rank/threshold associations.""
  - ""Hierarchical Measures: This category is another sub-class of knowledge-based measures that utilise hierarchical information of taxonomies such as UMLS, and MeSH to derive the rankings.""
  - ""Cluster-based Measures: In this category, cluster similarities are measured using techniques such as Intra-cluster similarity (Cameron et al., 2015), Jaccard Index (Ittipanuvat et al., 2014), Inclusion Index (Ittipanuvat et al., 2014), Dice coefficient (Ittipanuvat et al., 2014), Cosine (Ittipanuvat et al., 2014), Cosine similarity of tf-idf (Ittipanuvat et al., 2014), and Cosine similarity of tf-lidf (Ittipanuvat et al., 2014) to derive the ranking scores of associations.""
  - ""Combined Measures: The idea of combined measures is to integrate multiple characteristics of an association to decide its potential ranking.""
  - ""The algorithm used to rank the detected associations plays a vital role in LBD methodology.""
  - ""Evaluate ranking positions: Most of the studies have evaluated the ranking positions of the LBD output to verify the effectiveness of their ranking algorithm.""
  - ""Evaluate ranking scores: Mapping the ranking scores of the detected associations with scores obtained from databases (Baek et al., 2017) or other algorithms (Pusala et al., 2017) is another evaluation technique used in the literature.""
  - ""Evaluate the interestingness of results: Cameron et al. (2015) have used association rarity to statistically evaluate the interestingness of the LBD output.""
  - ""Evaluation of quality and coherence of stories: This evaluation metric provides a novel perspective to LBD evaluation.""","  - ""(Page 1, Table 1) ""
  - ""(Page 5, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 19, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 25, Table 1) | Evaluation Technique                                      | Category1 | Category2 |\n|-----------------------------------------------------------|-----------|-----------|\n| **Evidences-based Evaluation:**                           |           |           |""
  - ""(Page 26, Table 1) Below is a rendering of the page up to the first error.""","- The paper discusses various implementation methods used in Literature-Based Discovery (LBD), focusing on computational techniques and data processing pipelines.
- The ABC model is a fundamental premise in LBD, employing a simple syllogism to identify potential knowledge associations. It has two variants: open discovery and closed discovery.
- Data types used in LBD include titles, abstracts, full-text articles, selected articles, entire literature databases, keywords, and other metadata.
- Feature extraction techniques involve using UMLS and MeSH for term representation in medical domains, while n-grams are used in other domains.
- Data processing pipelines involve filtering techniques such as stop word removal, semantic category filtering, relation/predicate type filtering, hierarchical filtering, synonym mapping, and more.
- Ranking techniques include conventional statistical measures like token frequency, TF-IDF, mutual information, and non-conventional measures like average minimum weight and linking term count with AMW.
- The paper also discusses network/graph-based measures, knowledge-based measures, relations-based measures, hierarchical measures, cluster-based measures, and combined measures for ranking associations.
- Evaluation techniques include replicating existing medical discoveries, time-sliced evaluation, manual literature search, intersection evaluation, and comparison with curated databases.
- The paper highlights the importance of user-oriented evaluations and scalability analysis in terms of processing time and storage.","  - ""LBD literature makes use of different data types as their input of the knowledge discovery process.""
  - ""Title and Abstract: The most common data type selection in literature is using both title and abstract""
  - ""Keywords: Some research approaches have employed the keywords of the articles as the input data type""
  - ""The Data unit of analysis denotes the types of data extracted from the above-discussed data types to represent the knowledge associations.""
  - ""Since most of the LBD research performed in medicine, the most common term representation is using UMLS and MeSH""
  - ""Medline/PubMed is extensively being used as the main data source of the LBD literature""
  - ""This section outlines the two major elements of the process component; filtering techniques and ranking/thresholding techniques.""
  - ""Stop words typically denote non-topic general English terms.""
  - ""Semantic Category Filter: This technique typically utilises the semantic type or group information provided by UMLS""
  - ""Relation/predicate Type Filter: This filtering technique mostly consider the predications assigned using SemRep""
  - ""Hierarchical Filter: This technique utilises the hierarchical information such as levels and relationships of terms to filter out uninformative associations""
  - ""Synonym Mapping: Mapping synonyms by grouping exactly or nearly equal terms of a given term is another technique used to reduce the results""
  - ""Term ranking/thresholding is an important component of the LBD process as it should downweigh or remove noisy associations and upweight or retain the interesting and significant knowledge associations when ordering the terms.""
  - ""Considering conventional statistical measures to rank/threshold terms is common in literature.""
  - ""Nearest Neighbours: In this category, the score of the association is decided by analysing its nearest neighbours.""
  - ""Network/Graph-based Measures: Network/graph-based measures analyse node and edgelevel attributes to score the associations.""
  - ""Relations-based Measures: Relations/predicate based measures (a sub-class of knowledgebased measures) analyse the relations extracted from resources such as SemRep to rank/threshold associations.""
  - ""Hierarchical Measures: This category is another sub-class of knowledge-based measures that utilise hierarchical information of taxonomies such as UMLS, and MeSH to derive the rankings.""
  - ""Combined Measures: The idea of combined measures is to integrate multiple characteristics of an association to decide its potential ranking.""",,"- The paper discusses various data types used in Literature-Based Discovery (LBD), including titles, abstracts, full-text articles, keywords, and metadata. These are relevant to search and retrieval algorithms.
- The use of UMLS and MeSH for term representation is a content-based matching technique.
- The paper mentions filtering techniques such as stop words removal, semantic category filtering, relation/predicate type filtering, hierarchical filtering, synonym mapping, and time-based filtering. These are relevant to relevance ranking approaches.
- The paper discusses ranking techniques using conventional statistical measures like TF-IDF and mutual information, as well as non-conventional measures like AMW and LTC-AMW. These are relevant to ranking approaches.
- The paper mentions the use of metadata such as author details and publisher details for additional clues in knowledge discovery.
- The paper discusses network/graph-based measures and knowledge-based measures for ranking associations, which are relevant to citation analysis methods.
- The paper mentions the use of databases like Medline/PubMed and patent databases for data retrieval, which is relevant to metadata extraction and utilization.
- The paper discusses various ranking techniques that can be considered as recommendation system approaches.","  - ""Even though the prevalent ABC model have contributed in numerous ways to detect new knowledge, it is merely one of several different types of discovery models that facilitates LBD process. In this regard, Smalheiser (2012) points out the importance of thinking beyond the ABC formulation and experimenting alternative discovery models in the discipline.""
  - ""Despite the simplicity and power of the ABC model, it also suffers from several limitations such as the sheer number of intermediate terms that exponentially expands the search space and producing a large number of target terms that are hard to interpret manually (Smalheiser, 2012).""
  - ""Evaluating the effectiveness of the LBD results is challenging and remains to be an open issue. The main reason for this is that the LBD process detects novel knowledge that has not been publicly published anywhere and thus needs to be proven that they are useful.""
  - ""Evaluating the LBD output is challenging and remains to be an open issue as the field lacks gold standard datasets or consistent formal evaluation techniques.""
  - ""To date, time sliced evaluation is considered as the most objective evaluation technique proposed in the field. However, this evaluation technique suffers from two major limitations; (1) The association is proven to valid if the starting and linking term co-occur in future publications (that do not co-occur in the training set). However, co-occurrence does not necessarily mean that the proposed link has been established, and (2) Rejected associations can still be valid even though they have not been published yet.""
  - ""Due to the massive influx of scientific knowledge, the volume of data that the LBD system expects to analyse increases with time. For instance, a simple search of ''dementia'' results in more than 150,000 records in PubMed alone. This highlights the importance of performing scalability analysis of the LBD systems in terms of time and storage.""
  - ""The algorithm used to rank the detected associations plays a vital role in LBD methodology. It should rank the most promising associations in the top of the list by filtering the weak or false-positive associations. Therefore, the success of the LBD process greatly depends on the effectiveness of the ranking algorithm.""",,"- The paper discusses limitations of the ABC model, which is a fundamental discovery model in LBD. These limitations include the exponential expansion of search space and difficulty in manual interpretation, indicating algorithm performance issues.
- The evaluation of LBD results is challenging due to the lack of gold standard datasets and formal evaluation techniques, which is a limitation in evaluation methodology.
- The paper highlights scalability challenges as the volume of data to be analyzed increases over time, which can impact computational resource constraints.
- The ranking algorithm's effectiveness is crucial for filtering out weak associations, indicating a technical challenge in ensuring the algorithm's performance.
- The paper mentions the need for scalability analysis in terms of time and storage, which suggests technical challenges related to handling large datasets.","  - ""In this systematic review, we provide an in-depth analysis of the computational techniques used in the LBD process using a novel, up-to-date, and detailed classification.""
  - ""Moreover, we also summarise the key milestones of the discipline through a timeline of topics.""
  - ""To provide a general overview of the discipline, the review outlines LBD validation checks, major LBD tools, application areas, domains, and generalisability of LBD methodologies.""
  - ""We also outline the insights gathered through our statistical analysis that capture the trends in LBD literature.""
  - ""To conclude, we discuss the prevailing research deficiencies in the discipline by highlighting the challenges and opportunities of future LBD research.""
  - ""The popular ABC model has two variants named as open discovery and closed discovery.""
  - ""Even though the prevalent ABC model have contributed in numerous ways to detect new knowledge, it is merely one of several different types of discovery models that facilitates LBD process.""
  - ""In this regard, Smalheiser (2012) points out the importance of thinking beyond the ABC formulation and experimenting alternative discovery models in the discipline.""
  - ""recent studies have attempted to further explore alternative discovery models that deviate from the typical ABC discovery setting.""
  - ""The importance of using the aforementioned resources in the LBD system is that they support the system's functionality not only to medical domain, but also to a wide variety of other domains.""
  - ""To date, such domain independent LBD methodologies have been rarely experimented.""
  - ""However, providing merely a ranked list may not be the best way of visualising the results due to the following two reasons; (1) ranked associations are isolated in nature and do not provide an overall picture of all the suggested associations, and (2) ranked associations do not reflect how they are linked with the start and/or target concepts to better understand the association.""
  - ""Discussed below are other visualisation techniques employed in the literature.""
  - ""Evaluating the effectiveness of the LBD results is challenging and remains to be an open issue.""
  - ""The most widely used evaluation technique is replicating Swanson's medical discoveries.""
  - ""However, relying merely on discovery replication can be restrictive and may fail to reflect the true performance of the LBD methodology.""
  - ""time sliced evaluation is considered as the most objective evaluation technique proposed in the field.""
  - ""However, this evaluation technique suffers from two major limitations; (1) The association is proven to valid if the starting and linking term co-occur in future publications (that do not co-occur in the training set). However, co-occurrence does not necessarily mean that the proposed link has been established, and (2) Rejected associations can still be valid even though they have not been published yet.""
  - ""Another interesting direction for future evaluation is to incorporate the actual end users of LBD research to validate the results which is a neglected area in the literature.""
  - ""The novel advances in HCI research will be useful in this regard""
  - ""Smalheiser and Torvik Smalheiser & Torvik (2008) emphasises the importance of simplicity in user-interfaces of LBD tools to support widening the target audience.""",,"- The paper provides a systematic review of Literature-Based Discovery (LBD) techniques, which includes an analysis of computational techniques and a detailed classification of LBD methodologies.
- It discusses the ABC model and its variants, as well as the need to explore alternative discovery models beyond ABC.
- The paper highlights the use of non-traditional data sources like Twitter and FDA drug labels, indicating innovative approaches to data collection.
- It critiques existing output visualization techniques and suggests the need for more effective visualization methods.
- The paper discusses challenges in evaluating LBD results and suggests time-sliced evaluation as a more objective method, although it notes limitations and suggests future directions for improvement.
- The emphasis on incorporating user feedback and simplicity in user interfaces suggests innovative approaches to user interaction.
- The paper mentions the need for deeper language analysis and user involvement in validation, indicating potential areas for technical innovation.","  - ""The most commonly used output of LBD systems is a ranked list of associations (Gubiani et al., 2017;Baek et al., 2017) where the top associations reflect the most probable knowledge links.""
  - ""Different information retrieval metrics have been used to obtain a quantitative understanding of the performance of the LBD methodologies as summarised in Table 4""
  - ""precision (i.e., fraction of associations obtained from the LBD process that are relevant), recall (i.e., fraction of relevant associations that are successfully retrieved), F-measure (i.e., harmonic mean of precision and recall) and Area Under Curve (AUC) (i.e., area under the Receiver Operating Characteristic (ROC) curve which falls in the range from 1 to 0.5) are the popular metrics used in the literature.""
  - ""precision at k, recall at k, 11-point average interpolated precision, and Mean Reciprocal Rank have been used in the literature.""
  - ""Processing time analysis: Less processing time is a critical characteristic of the LBD process as the users would like to quickly obtain results for their queries.""
  - ""Storage analysis: Analysing memory requirements is important when dealing with large datasets.""
  - ""The algorithm used to rank the detected associations plays a vital role in LBD methodology.""
  - ""Evaluate ranking positions: Most of the studies have evaluated the ranking positions of the LBD output to verify the effectiveness of their ranking algorithm.""
  - ""User-based evaluation: Evaluating user's ability to identify and formulating hypotheses from the output of the LBD process is an essential evaluation approach.""
  - ""User-experience evaluation: Analysing how users interact with the LBD system plays a critical role as such user behaviours provide useful clues to improve the visualisation techniques of LBD results, user-interface, and the process of knowledge discovery.""",,"- The paper discusses various performance metrics used in Literature-Based Discovery (LBD) systems, including precision, recall, F-measure, and Area Under Curve (AUC), which are common in information retrieval.
- The paper mentions the use of precision at k, recall at k, and other metrics to evaluate the ranking of associations, indicating a focus on ranking efficiency.
- Processing time and storage analysis are highlighted as important for user experience and system efficiency.
- The paper emphasizes the importance of evaluating user interaction and experience with LBD systems, suggesting a focus on usability.
- Scalability is discussed in terms of handling large volumes of data, indicating a concern for system robustness and scalability.","  - ""We used six keywords and six databases to retrieve the articles for this review. Each keyword is searched in the title, abstract or keywords depending on the search options given by the databases.""
  - ""We only included journals and conference proceedings that are in the English language in our analysis.""
  - ""We excluded other types of articles such as reviews, books, book chapters, papers reporting lessons learned, keynotes, and editorials.""
  - ""The entire article selection of this review was performed in three stages (Weidt & Silva, 2016); Stage 1: analysing only title and abstract, Stage 2: analysing introduction and conclusion, and Stage 3: read complete article and quality checklist.""
  - ""In total, we obtained 176 papers for this review (listed in https://tinyurl.com/selected-LBD-articles).""
  - ""Medline/PubMed is extensively being used as the main data source of the LBD literature (Lever et al., 2017).""
  - ""The patent-based LBD studies (Vicente-Gomila, 2014), have considered patent databases such as Thomson Innovation, United State Patent and Trade Mark Office (USPTO) and MAtrixware REsearch Collection (MAREC) patent document collection to retrieve the data.""
  - ""Other conventional data sources include clinical datasets (Dong et al., 2014), Gene Expression Omnibus (GEO) database (Hristovski et al., 2010), ArrayExpress (AE) database (Maver et al., 2013), Manually Annotated Target and Drug Online Resource (MATADOR) (Crichton et al., 2018), Biological General Repository for Interaction Datasets (BioGRID) (Crichton et al., 2018), PubTator (Crichton et al., 2018), Online Mendelian Inheritance in Man (OMIM) (Cohen et al., 2010) and TREC (Symonds, Bruza & Sitbon, 2014).""
  - ""Few non-English data sources such as Chinese Social Sciences Citation Index (Su & Zhou, 2009), China Biology Medicine disks (Qian, Hong & An, 2012), Chinese Medicine Library and Information System (Yao et al., 2008), Traditional Chinese Medicine Database (Gao et al., 2016) and Chinese Journal Full-text database (Yao et al., 2008) have also been utilised in LBD workflow.""
  - ""The studies that have attempted to perform LBD in a non-traditional setting have extracted data from a variety of sources such as Twitter (Bhattacharya & Srinivasan, 2012), DailyMed: FDA drug labels (Bisgin et al., 2011), Google news (Maclean & Seltzer, 2011), and World Wide Web (WWW) (Gordon, Lindsay & Fan, 2002).""","  - ""(Page 5, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 19, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 25, Table 1) | Evaluation Technique                                      | Category1 | Category2 |\n|-----------------------------------------------------------|-----------|-----------|\n| **Evidences-based Evaluation:**                           |           |           |""
  - ""(Page 26, Table 1) Below is a rendering of the page up to the first error.""","- The paper uses six databases to retrieve articles, which are not explicitly named in the quote but are likely academic databases such as Scopus and Web of Science.
- The paper includes only English language journals and conference proceedings, excluding other types of articles.
- The article selection process involved a three-stage analysis of title and abstract, introduction and conclusion, and a full article quality checklist.
- The total number of papers obtained for the review is 176.
- Medline/PubMed is a primary data source for LBD literature, with other sources like PubMed Central, Science Direct, and Web of Science also used.
- Patent databases like Thomson Innovation and USPTO are used for patent-based LBD studies.
- Clinical datasets, GEO database, ArrayExpress, and other biological databases are used as conventional data sources.
- Non-English data sources like Chinese Social Sciences Citation Index and Traditional Chinese Medicine Database are also utilized.
- Non-traditional data sources include Twitter, FDA drug labels, Google news, and the World Wide Web.
- The tables at the end of the paper likely provide more detailed information about the datasets and data collection methods.","  - ""LBD is a knowledge discovery workflow that automatically detects significant, implicit knowledge associations hidden in fragmented knowledge areas by analysing existing scientific literature.""
  - ""The recommended novel associations can greatly assist scientists in formulating and evaluating novel research hypotheses""
  - ""LBD was developed as a research field from the medical discoveries published by Swanson since 1986""
  - ""The most common data type selection in literature is using both title and abstract""
  - ""The importance of using the aforementioned resources in the LBD system is that they support the system's functionality not only to medical domain, but also to a wide variety of other domains.""
  - ""The typical output of the LBD process is a ranked list of terms that denote the potential associations.""
  - ""The output component of LBD workflow, is largely neglected in the prevailing literature which emphasises the necessity of conducting user-interaction studies to assess the user experience.""
  - ""The primary source of data utilised in LBD studies is research papers.""
  - ""The authors received no funding for this work.""
  - ""The authors declare there are no competing interests.""","  - ""(Page 5, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 19, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 25, Table 1) | Evaluation Technique                                      | Category1 | Category2 |\n|-----------------------------------------------------------|-----------|-----------|\n| **Evidences-based Evaluation:**                           |           |           |""
  - ""(Page 26, Table 1) Below is a rendering of the page up to the first error.""","- The paper discusses Literature-Based Discovery (LBD) as a method to identify implicit knowledge associations in scientific literature, which is a key application context in the field of literature review.
- The primary target research discipline is medicine, as indicated by the origins of LBD in medical discoveries by Swanson.
- Specific literature review tasks addressed include formulating and evaluating novel research hypotheses by detecting hidden associations.
- Types of academic documents processed include research papers, with a focus on titles and abstracts, but also full-text articles and patents.
- User types and requirements are not explicitly mentioned, but the focus on assisting scientists suggests that the primary users are researchers.
- Integration with research workflows is implied by the use of LBD to generate hypotheses and assist in research development.
- The paper does not explicitly mention commercial vs. academic applications, but the focus on research and scientific literature suggests an academic context.
- The use of non-traditional data sources like Twitter and FDA drug labels indicates a broader application context beyond traditional academic documents."
"""Open Innovation: Content, Impact and Future Research Directions""","Krithika Randhawa, Ralf Wilden, Jan Hohberger",10.5465/AMBPP.2014.13088ABSTRACT,https://doi.org/10.5465/AMBPP.2014.13088ABSTRACT,-,2,2014,Not mentioned (the abstract does not provide specific details about the technical architecture),"- Algorithms and models used: Network-based co-citation analysis, Text mining (unstructured ontological discovery)
- Feature extraction techniques: Not mentioned
- Data processing pipelines: Not mentioned
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Not mentioned","- Citation analysis methods: Network-based co-citation analysis
- Content-based matching techniques: Text mining (unstructured ontological discovery)","- Integration difficulties: The abstract suggests a need to better integrate related research streams with open innovation literature, which could imply challenges in combining different methodologies or data sources.
- Methodological limitations: The focus on the technology theme and the need for a more comprehensive perspective suggest potential limitations in current technical approaches or methodologies.","The innovative solution in this paper is the novel approach of combining network-based co-citation analysis and text mining (unstructured ontological discovery) to analyze open innovation research. This methodological innovation is used to identify themes and gaps in the literature, suggesting a structured and systematic way of analyzing research areas.",Not mentioned (the abstract does not provide performance results or metrics related to the methods used),"- Dataset size: 299 publications
- Data collection methods: Network-based co-citation analysis and text mining (unstructured ontological discovery)
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: Not mentioned
- Academic databases used: Not mentioned
- Training/validation/test data splits: Not applicable","- Target research disciplines or fields: Open innovation, technology, business models, user communities
- Specific literature review tasks addressed: Systematic review of open innovation research
- Types of academic documents processed: 299 publications
- User types and requirements: Not mentioned
- Integration with research workflows: Potential integration by identifying gaps for future research
- Commercial vs. academic applications: Not mentioned","  - ""We use a novel approach by combining two bibliometric methods of network-based co- citation analysis and text mining (unstructured ontological discovery) of 299 publications on the topic.""",,"- The abstract mentions the use of ""network-based co-citation analysis and text mining (unstructured ontological discovery)"" as part of their methodology. This suggests that the technical architecture involves bibliometric methods, which are typically used in literature reviews to analyze citations and content.
- The mention of ""text mining"" implies the use of natural language processing (NLP) techniques, which are a subset of AI/ML frameworks.
- The abstract does not provide specific details about the main AI/ML frameworks used, system architecture components, technical infrastructure, integration with existing academic databases, or the overall system design approach.
- The focus of the abstract is on the content and impact of open innovation research rather than the technical architecture of the study itself.","  - ""We use a novel approach by combining two bibliometric methods of network-based co- citation analysis and text mining (unstructured ontological discovery) of 299 publications on the topic.""",,"- The abstract mentions the use of ""network-based co-citation analysis"" and ""text mining (unstructured ontological discovery)"" as the methods employed in the study. These are the technical implementation details provided in the abstract.
- ""Network-based co-citation analysis"" suggests a method of analyzing how publications are cited together, which can involve network algorithms to identify clusters or patterns in citation networks.
- ""Text mining (unstructured ontological discovery)"" implies the use of techniques to extract meaningful information from unstructured text data, which could involve natural language processing (NLP) techniques or machine learning algorithms to discover ontological relationships.
- The abstract does not provide specific details about algorithms, feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or technical workflow beyond these general methods.","  - ""We use a novel approach by combining two bibliometric methods of network-based co- citation analysis and text mining (unstructured ontological discovery) of 299 publications on the topic.""",,"- The abstract mentions the use of ""network-based co-citation analysis"" as one of the methods employed in the paper. This is a type of citation analysis method, which is relevant to the question about paper discovery techniques.
- The abstract also mentions ""text mining (unstructured ontological discovery)"" as another method used. This is a content-based matching technique, as it involves analyzing the content of publications to discover relationships and themes.
- These methods are used to analyze 299 publications, indicating their role in finding and matching academic papers relevant to the topic of open innovation.","  - ""We use a novel approach by combining two bibliometric methods of network-based co- citation analysis and text mining (unstructured ontological discovery) of 299 publications on the topic.""
  - ""Our results show that open innovation research has drawn from several mature fields; yet it focuses only on a select number of innovation issues.""
  - ""There appears scope to better integrate several related research streams with open innovation literature.""
  - ""Our study reveals three distinct themes of open innovation research: 1) Technology, 2) Business models and value appropriation, and 3) Users and communities.""
  - ""Research-to- date has predominantly focused on the technology theme.""
  - ""We also identify several gaps in the literature that serve as directions for future research, namely: 1) Develop a more comprehensive perspective of open innovation by including more diverse levels of analysis (users, networks and communities); 2) Direct increased a...""",,"- The abstract mentions the use of ""network-based co-citation analysis and text mining (unstructured ontological discovery)"" as a novel approach. This suggests that the technical challenges might relate to the integration of these methods or the quality of data used in these analyses.
- The mention of ""scope to better integrate several related research streams"" implies potential integration difficulties or challenges in combining different research areas.
- The focus on ""technology theme"" and the need for a ""more comprehensive perspective"" suggest that there might be limitations in the current technical approaches or methodologies used in open innovation research.
- The abstract does not explicitly mention specific technical challenges such as algorithm performance issues, data quality problems, scalability challenges, or computational resource constraints. However, the need for a ""more comprehensive perspective"" and better integration of research streams could imply some technical challenges in achieving these goals.","  - ""We use a novel approach by combining two bibliometric methods of network-based co- citation analysis and text mining (unstructured ontological discovery) of 299 publications on the topic.""
  - ""We also identify several gaps in the literature that serve as directions for future research, namely: 1) Develop a more comprehensive perspective of open innovation by including more diverse levels of analysis (users, networks and communities);""
  - ""Our study reveals three distinct themes of open innovation research: 1) Technology, 2) Business models and value appropriation, and 3) Users and communities.""
  - ""Our results show that open innovation research has drawn from several mature fields; yet it focuses only on a select number of innovation issues.""
  - ""Research-to- date has predominantly focused on the technology theme.""",,"- The abstract mentions a ""novel approach"" involving the combination of ""network-based co-citation analysis and text mining (unstructured ontological discovery)"" which suggests an innovative methodological contribution.
- The focus on ""three distinct themes of open innovation research"" indicates a structured approach to categorizing research areas, which could be considered a novel way of organizing and analyzing literature.
- The identification of gaps in literature and suggestions for future research directions imply a creative problem-solving approach by highlighting areas where current research is lacking.
- The abstract does not explicitly mention new algorithmic contributions, creative problem-solving methods, hybrid or ensemble approaches, novel feature engineering techniques, innovative evaluation metrics, technical workarounds, or original system design elements. However, the use of text mining and co-citation analysis could be seen as a technical innovation in the context of literature review.","  - ""This paper presents a comprehensive and systematic review of open innovation research.""
  - ""We use a novel approach by combining two bibliometric methods of network-based co- citation analysis and text mining (unstructured ontological discovery) of 299 publications on the topic.""
  - ""Our results show that open innovation research has drawn from several mature fields; yet it focuses only on a select number of innovation issues.""
  - ""Our study reveals three distinct themes of open innovation research: 1) Technology, 2) Business models and value appropriation, and 3) Users and communities.""
  - ""Research-to- date has predominantly focused on the technology theme.""
  - ""We also identify several gaps in the literature that serve as directions for future research, namely: 1) Develop a more comprehensive perspective of open innovation by including more diverse levels of analysis (users, networks and communities); 2) Direct increased a...""",,"- The abstract discusses a systematic review of open innovation research, which involves analyzing existing literature rather than conducting new experiments or developing new systems. Therefore, it does not provide performance results in terms of accuracy, precision, recall, F1-scores, processing speed, efficiency metrics, comparison with baseline methods, user satisfaction, system reliability, or scalability test results.
- The focus of the paper is on identifying themes and gaps in open innovation research rather than evaluating the performance of a specific system or technology.
- The abstract does not mention any quantitative or qualitative performance outcomes related to the methods used (network-based co-citation analysis and text mining), nor does it discuss user satisfaction or system reliability.","  - ""We use a novel approach by combining two bibliometric methods of network-based co- citation analysis and text mining (unstructured ontological discovery) of 299 publications on the topic.""",,"- The abstract mentions the use of ""network-based co-citation analysis and text mining (unstructured ontological discovery)"" as the methods used for data analysis. This suggests that the data sources are likely publications or academic papers.
- The mention of ""299 publications on the topic"" indicates that the dataset size is 299 publications.
- The abstract does not specify which academic databases were used to collect these publications, nor does it provide details on data preprocessing, cleaning, or any external knowledge bases or ontologies used.
- There is no mention of training/validation/test data splits, as this is not typically relevant to bibliometric analysis or systematic reviews.
- The focus is on the analysis of existing literature rather than creating new datasets or using external knowledge bases.","  - ""This paper presents a comprehensive and systematic review of open innovation research.""
  - ""We use a novel approach by combining two bibliometric methods of network-based co- citation analysis and text mining (unstructured ontological discovery) of 299 publications on the topic.""
  - ""Our results show that open innovation research has drawn from several mature fields; yet it focuses only on a select number of innovation issues.""
  - ""There appears scope to better integrate several related research streams with open innovation literature.""
  - ""Our study reveals three distinct themes of open innovation research: 1) Technology, 2) Business models and value appropriation, and 3) Users and communities.""
  - ""Research-to- date has predominantly focused on the technology theme.""
  - ""We also identify several gaps in the literature that serve as directions for future research, namely: 1) Develop a more comprehensive perspective of open innovation by including more diverse levels of analysis (users, networks and communities);""",,"- The abstract discusses a systematic review of open innovation research, which is a literature review task.
- The study uses bibliometric methods and text mining on 299 publications, indicating the types of academic documents processed.
- The focus on open innovation research suggests that the target research disciplines or fields are related to innovation, technology, business models, and user communities.
- The study identifies gaps in the literature, which could serve as directions for future research, potentially integrating with research workflows.
- The abstract does not explicitly mention commercial vs. academic applications or specific user types and requirements.
- The study's focus on themes like technology, business models, and users suggests a broad application context within academic research."
Deep Learning Approaches to Author Name Disambiguation: A Survey,"Francesca Cappelli, Giovanni Colavizza, Silvio Peroni",10.1007/s00799-025-00428-6,https://doi.org/10.1007/s00799-025-00428-6,International Journal on Digital Libraries,1,2025,"- Main AI/ML frameworks used: Deep learning (DL), neural network-based models
- System architecture components: Hybrid approaches combining supervised and unsupervised learning
- Technical infrastructure: Gephi for network analysis and visualization
- Integration with existing academic databases or platforms: AMiner datasets
- Overall system design approach: Systematic literature review, bibliometric analysis","- Algorithms and models used: Supervised classification systems, unsupervised clustering methods, hybrid approaches combining both.
- Feature extraction techniques: BERT, Word2Vec, co-authorship networks, metadata attributes.
- Data processing pipelines: Normalization, dimensionality reduction using PCA.
- Training methodologies: Supervised learning with labeled data, unsupervised clustering for new authors.
- Preprocessing steps: Data normalization, PCA for dimensionality reduction.
- Technical workflow or methodology: Integration of multi-modal features, use of Graph Attention Networks and hybrid attention mechanisms.","- Search and retrieval algorithms: Keyword-based search using ""author name disambiguation"" + ""deep learning"" with time filters.
- Relevance ranking approaches: Selection of top 50 results, abstract screening, and inclusion criteria evaluation.
- Citation analysis methods: Citation network graph construction, in-degree centrality for influential articles.
- Content-based matching techniques: Supervised and unsupervised learning methods for author name disambiguation.
- Metadata extraction and utilization: Structured table for systematic evaluation of selected studies.
- Recommendation system approaches: Use of PURE Suggest tool for citation-based recommendations.
- Expert/authority identification methods: Identification of scientific communities, keyword extraction for communities.","- Algorithm performance issues: Name variations, transcription errors, and incomplete metadata affect model accuracy.
- Data quality or availability problems: Scarcity of high-quality data, manual dataset creation, small scale, and domain specificity limit model training.
- Scalability challenges: Data imbalances and scarcity of annotated datasets limit generalizability.
- Integration difficulties: Heavy reliance on AMiner dataset may not be representative of all scenarios.
- Evaluation methodology limitations: Lack of standardized benchmarks and reliance on specific datasets.
- Computational resource constraints: High computational requirements of Large Language Models (LLMs) may create barriers for smaller institutions.
- Technical bottlenecks: Need for large amounts of labeled data and potential biases in datasets.","- Hybrid deep pairwise classification
- Real-time AND frameworks like CONNA
- Co-attention-based pairwise learning
- Use of multiple Graph Attention Networks (GATs)
- Heterogeneous Graph Convolutional Networks (HGCN)
- Integration of word embeddings and knowledge graph embeddings
- Hybrid approaches combining supervised and unsupervised learning
- MORE framework integrating multiple neural models
- Graph-based methods and attention mechanisms","- Accuracy, precision, recall, F1-scores: The highest F1-score achieved is 89.7 by a hybrid approach.
- Processing speed and efficiency metrics: Not mentioned.
- Comparison with baseline methods: Not explicitly mentioned.
- User satisfaction or usability results: Not mentioned.
- System reliability and robustness measures: Not mentioned.
- Scalability test results: Not mentioned.","- Academic databases used: AMiner (including AMiner-WhoIsWho and AMiner-534K)
- Dataset sizes and characteristics: AMiner-WhoIsWho features over 1,000,000 articles; AMiner-534K is used for knowledge graph-based approaches.
- Data collection methods: Systematic literature review with keyword-based and citation-based approaches.
- Training/validation/test data splits: Mentioned for AMiner-534K.
- Data preprocessing and cleaning approaches: Not specifically detailed in the quotes.
- External knowledge bases or ontologies used: Knowledge graph extracted from AMiner.","- Target research disciplines or fields: Broad application across various academic fields
- Specific literature review tasks addressed: Author assignment (AA) and author grouping (AG)
- Types of academic documents processed: Publications with metadata such as co-authorships, affiliations, titles, and abstracts
- User types and requirements: Researchers and librarians
- Integration with research workflows: Integration into digital library systems
- Commercial vs. academic applications: Academic context","  - ""This paper provides a systematic review of state-of-the-art AND techniques based on deep learning within the timeframe 2016-2024,""
  - ""The analysis of the comparable studies reveals that hybrid approaches achieve the best performance, with the top-performing method reaching 89.7 F1-score on AMiner datasets.""
  - ""Deep learning methods have significantly impacted AND by enabling integration of structured and unstructured data.""
  - ""Hybrid approaches effectively balance supervised and unsupervised learning, demonstrating superior performance.""
  - ""The methods analysed and revised in this survey take advantage of different benchmarks in the literature. Most of them rely on datasets derived from AMiner""
  - ""The literature analysis within the selected timeframe highlights the dominant role of DL methodologies in author name disambiguation tasks.""
  - ""the use of neural network-based models has become prevalent in addressing the challenges of disambiguation in complex academic networks.""
  - ""The analysis of supervised approaches reveals a predominant focus on the AA task, with methods primarily designed as classification systems that assign publications to known author profiles.""
  - ""Unsupervised approaches do not require labelled data and typically focus on clustering or embedding methods to disambiguate names.""
  - ""Mixed AND approaches incorporate both supervised and unsupervised methods within their pipelines.""
  - ""The ideal way to compare state-of-the-art methods in DL is following these two criteria:

• Studies must evaluate their results on the same or comparable datasets. • Studies must evaluate their results with the same evaluation metrics.""
  - ""DL AND methods require large datasets but are constrained by limited labeled data due to costly manual annotation.""
  - ""the study revealed a heavy reliance on AMiner as the primary dataset, which raises concerns about the generalizability of the evaluated methods.""
  - ""The literature review was conducted following a systematic and structured approach to ensure the inclusion of relevant, high-quality academic studies addressing AND.""
  - ""We conducted a systematic literature review using Google Scholar with keywords ""author name disambiguation"" + ""deep learning"" and temporal filters.""
  - ""A structured table was created to analyse and compare the selected studies systematically.""
  - ""In addition to our primary analysis, we conducted an exploratory bibliometric analysis to gain deeper insights into the scientific landscape.""
  - ""Using the extracted data, a citation network graph was constructed, which was subsequently analyzed with Gephi, an advanced tool for network analysis and visualization.""","  - ""(Page 1, Table 1) ""
  - ""(Page 7, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 15, Table 1) Below is a rendering of the page up to the first error.""","- The paper focuses on deep learning (DL) techniques for Author Name Disambiguation (AND), indicating that DL is a primary AI/ML framework used.
- The use of neural network-based models is highlighted, suggesting these are key components of the technical architecture.
- Hybrid approaches combining supervised and unsupervised learning are emphasized, indicating a mixed system architecture.
- The reliance on AMiner datasets suggests integration with existing academic databases.
- The paper mentions the use of Gephi for network analysis and visualization, indicating a tool used in the technical infrastructure.
- The systematic literature review and bibliometric analysis suggest a structured approach to system design.
- The tables provide detailed information on the datasets and evaluation metrics used, which are crucial for understanding the technical architecture and system components.","  - ""The analysis of supervised approaches reveals a predominant focus on the AA task, with methods primarily designed as classification systems that assign publications to known author profiles.""
  - ""Most techniques employ pairwise learning paradigms, treating disambiguation as a binary classification problem to determine whether two publications belong to the same author.""
  - ""A notable trend is the integration of multi-modal features, combining semantic content (through BERT, Word2Vec), structural information (co-authorship networks), and metadata attributes (affiliations, venues) within neural architectures.""
  - ""Graph Attention Networks and hybrid attention mechanisms have emerged as popular choices for capturing complex relationships in academic networks.""
  - ""Unsupervised approaches do not require labelled data and typically focus on clustering or embedding methods to disambiguate names.""
  - ""Mixed approaches demonstrate a strategic balance between the two AND sub-tasks, typically employing supervised learning for AA to leverage available labeled data for known authors, while maintaining unsupervised clustering capabilities for AG to handle new, unregistered authors.""
  - ""The incorporation of human-in-the-loop feedback and pseudolabeling strategies further enhances these hybrid frameworks, addressing the practical challenge of limited labeled data availability.""
  - ""DL supervised techniques make use of labelled training data manually created or collected from annotated databases, usually inputted into a classifier.""",,"- The paper discusses various deep learning approaches for author name disambiguation, including supervised, unsupervised, and hybrid methods.
- Supervised methods often use labeled data and focus on classification tasks, such as assigning publications to known authors.
- Unsupervised methods use clustering or embedding techniques to disambiguate names without labeled data.
- Hybrid methods combine supervised and unsupervised techniques to leverage both labeled data and scalability.
- Feature extraction techniques include using BERT, Word2Vec, and other word embeddings to capture semantic content.
- Structural information like co-authorship networks and metadata attributes are also integrated into neural architectures.
- Graph Attention Networks and hybrid attention mechanisms are popular for capturing complex relationships.
- The paper mentions various specific models and frameworks, such as CONNA, BOND, and MORE, which use different combinations of these techniques.
- Preprocessing steps include data normalization and dimensionality reduction using PCA.
- Training methodologies involve using labeled datasets for supervised learning and unsupervised clustering for new authors.","  - ""The search for relevant studies was conducted in multiple stages using a combination of keyword-based and citation-based approaches, integrating both data collection and selection processes.""
  - ""The keywords ""author name disambiguation"" + ""deep learning"" were used to retrieve relevant studies with a time filter from 2016 to 2024.""
  - ""We selected only the top 50 results returned by the query.""
  - ""After reviewing the abstracts and assessing alignment with the inclusion criteria, 40 studies were selected for further analysis.""
  - ""The PURE Suggest tool was applied to the 40 selected studies, leveraging citation-based recommendations.""
  - ""Through abstract screening and inclusion criteria evaluation, 11 additional studies were identified and added to the dataset, increasing the total to 51 studies.""
  - ""A second iteration of PURE Suggest was performed on the updated set of 51 studies.""
  - ""A full-text review was conducted on all 52 studies.""
  - ""After the full-text assessment, a total of 28 studies were included in the final analysis.""
  - ""A structured table was created to analyse and compare the selected studies systematically.""
  - ""The literature analysis within the selected timeframe highlights the dominant role of DL methodologies in author name disambiguation tasks.""
  - ""The analysis of supervised approaches reveals a predominant focus on the AA task, with methods primarily designed as classification systems that assign publications to known author profiles.""
  - ""Unsupervised approaches do not require labelled data and typically focus on clustering or embedding methods to disambiguate names.""
  - ""Mixed approaches demonstrate a strategic balance between the two AND sub-tasks, typically employing supervised learning for AA to leverage available labeled data for known authors, while maintaining unsupervised clustering capabilities for AG to handle new, unregistered authors.""
  - ""The ideal way to compare state-of-the-art methods in DL is following these two criteria:

• Studies must evaluate their results on the same or comparable datasets. • Studies must evaluate their results with the same evaluation metrics.""
  - ""The literature review was conducted following a systematic and structured approach to ensure the inclusion of relevant, high-quality academic studies addressing AND.""
  - ""In addition to our primary analysis, we conducted an exploratory bibliometric analysis to gain deeper insights into the scientific landscape.""
  - ""Using the extracted data, a citation network graph was constructed, which was subsequently analyzed with Gephi, an advanced tool for network analysis and visualization.""
  - ""The analysis included:

• Most Influential Articles (In-Degree Centrality): To identify highly cited articles, pinpointing key contributions within the field.""
  - ""To detect distinct scientific communities within the network, highlighting clusters of closely related works.""
  - ""Keyword Extraction for Communities: Keywords for each community, providing insight into prevalent research topics and trends.""
  - ""Publication-Type Analysis: Analysis of how resources are distributed across different types of publications in the specific academic landscape.""","  - ""(Page 1, Table 1) ""
  - ""(Page 7, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 15, Table 1) Below is a rendering of the page up to the first error.""","- The paper describes a systematic literature review process that involves keyword-based and citation-based search strategies to find relevant studies. This indicates the use of search and retrieval algorithms.
- The use of the PURE Suggest tool suggests a recommendation system approach to identify additional relevant studies.
- The analysis of citation networks and the identification of influential articles using in-degree centrality indicate citation analysis methods.
- The focus on supervised and unsupervised learning methods for author name disambiguation suggests content-based matching techniques.
- The extraction and analysis of metadata from publications are implied by the structured table used for systematic evaluation.
- The identification of scientific communities and keyword extraction for these communities suggest expert/authority identification methods.","  - ""The AND task is inherently challenging due to various factors, including name variations (e.g. abbreviations, translations, pseudonyms), transcription errors, and incomplete or incorrect metadata.""
  - ""Another prominent issue is the scarcity of high-quality data. Errors in metadata, missing names, or pseudonyms create significant obstacles to constructing accurate predictive models, particularly for deep learning (DL) based approaches, which require large amounts of high-quality labelled data.""
  - ""Moreover, the lack of standard and reliable annotated datasets and shared resources for systematic evaluation limits the ability to compare methodologies and develop consolidated benchmarks directly.""
  - ""Despite the availability of labelled datasets for training supervised or semi-supervised methods, existing datasets have notable limitations. Many are manually created, which is time-intensive and prone to quality issues. Additionally, most datasets are small in scale or domain-specific (e.g. SCAD-zbMATH for mathematics), failing to capture the complexity of name ambiguities in large, real-world databases""
  - ""Biases in datasets, such as overrepresentation of certain ethnicities or insufficient handling of name variations, further limit their applicability""
  - ""The methods analysed and revised in this survey take advantage of different benchmarks in the literature. Most of them rely on datasets derived from AMiner (https://www. AMiner.org), a free online service used to index, search, and mine big scientific data designated to identify connections between researchers, conferences, and publications.""
  - ""DL AND methods require large datasets but are constrained by limited labeled data due to costly manual annotation.""
  - ""Data imbalances and the scarcity of annotated datasets further limit scalability, underscoring the pressing need for more diverse and standardized benchmarks.""
  - ""Although Large Language Models (LLMs) architectures were not included in this study, their potential use could enhance AND systems. By processing vast amounts of unstructured text, LLMs can improve accuracy in complex disambiguation scenarios, particularly for multilingual datasets and cases with limited metadata.""
  - ""However, the integration of LLMs into AND systems raises specific ethical concerns beyond those present in current DL approaches. Privacy and data governance become critical as LLMs require access to extensive textual data from publications, potentially including sensitive information about authors' research activities and collaborations.""
  - ""Computational equity emerges as a concern, as the high computational requirements of LLMs may create barriers for smaller institutions or researchers in developing countries, potentially widening the digital divide in research infrastructure.""",,"- The paper identifies several technical challenges in the context of Author Name Disambiguation (AND) using deep learning methods. These include algorithm performance issues due to data quality problems such as name variations, transcription errors, and incomplete metadata.
- Data quality and availability are significant challenges, with a scarcity of high-quality data and limitations in existing datasets such as manual creation, small scale, and domain specificity.
- Scalability is a challenge due to data imbalances and the scarcity of annotated datasets, which limits the generalizability of methods.
- Integration difficulties are highlighted by the reliance on specific datasets like AMiner, which may not be representative of all scenarios.
- Evaluation methodology limitations are noted due to the lack of standardized benchmarks and the reliance on specific datasets.
- Computational resource constraints are mentioned in the context of Large Language Models (LLMs), which require extensive computational resources and raise ethical concerns.
- Technical bottlenecks include the need for large amounts of labeled data and the potential for biases in datasets.","  - ""The analysis of the comparable studies reveals that hybrid approaches achieve the best performance, with the top-performing method reaching 89.7 F1-score on AMiner datasets.""
  - ""Hybrid approaches effectively balance supervised and unsupervised learning, demonstrating superior performance.""
  - ""Hybrid Deep Pairwise Classification: Kim et al. (2019) [23] developed a hybrid approach combining structural and global features.""
  - ""CONNA: Zhao et al. (2022) [24] introduced CONNA, a framework designed for real-time AND as new articles are added to a system.""
  - ""Co-Attention-Based Pairwise Learning: Wang et al. (2023) [25] proposed a novel approach focusing on pairwise comparisons between author names using co-attention mechanisms.""
  - ""Name Disambiguation Scheme Based on Heterogeneous Academic Sites: Choi et al., 2023 [31] propose a name disambiguation framework combining rule-based methods with DL.""
  - ""Author Name Disambiguation Using Multiple Graph Attention Networks Zhang et al. (2021) [32] explore the use of multiple Graph Attention Networks (GATs) for author name disambiguation.""
  - ""Semantic Author Name Disambiguation with Word Embeddings: Müller, MC. (2017) [33] presents a supervised AND system that combines three models: co-author similarity, content similarity (using word embeddings for title matching), and metadata similarity.""
  - ""Unsupervised Author Disambiguation Using Heterogeneous Graph Convolutional Network Embedding Qiao et al. ( 2019) [36] propose the use of Heterogeneous Graph Convolutional Networks (HGCN) to address author name disambiguation.""
  - ""Exploiting Higher Order Multi-dimensional Relationships with Self-attention for Author Name Disambiguation Pooja, Mondal, & Chandra (2022) [37] propose an unsupervised approach based on graph convolutional networks (GCN) with a dual-level attention mechanism.""
  - ""Mixed AND approaches incorporate both supervised and unsupervised methods within their pipelines.""
  - ""Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop: Zhang, Y., Zhang, F., Yao, P., & Tang, J. (2018) [38] This framework tackles large-scale author name disambiguation by combining supervised and unsupervised methods.""
  - ""Gong, Fang, Peng, et al. ( 2024) [16] propose the MORE framework that integrates multiple neural models, including OAG-BERT, SimCSE, LightGBM, and iHGAT, leveraging both supervised and unsupervised learning.""
  - ""A Graph-Based Author Name Disambiguation Method and Analysis via Information Theory: Ma, Y., Wu, Y., & Lu, C. (2020) [39] presents a hybrid approach for author name disambiguation, combining representation learning with both supervised and unsupervised techniques.""
  - ""Author Name Disambiguation via Heterogeneous Network Embedding from Structural and Semantic Perspectives Xie et al. (2022) [40] introduce a method that considers both structural and semantic perspectives to embed publications into vectors.""
  - ""Bibliographic Name Disambiguation with Graph Convolutional Network Yan, Peng, Li, Li, & Wang (2019) [41] adopt an unsupervised model utilizing two customized GCNs: one for document embeddings (Document-GCN) and one for author embeddings (Person-GCN).""
  - ""Leveraging Knowledge Graph Embeddings to Disambiguate Author Names in Scientific Data Rettig et al., (2022) [42] combines text and graph embeddings to enhance document representation and clustering.""
  - ""Mixed approaches demonstrate a strategic balance between the two AND sub-tasks, typically employing supervised learning for AA to leverage available labeled data for known authors, while maintaining unsupervised clustering capabilities for AG to handle new, unregistered authors.""",,"- The paper highlights the effectiveness of hybrid approaches in achieving superior performance in author name disambiguation, as seen in the top-performing method reaching an F1-score of 89.7 on AMiner datasets.
- Several innovative solutions are mentioned, including hybrid deep pairwise classification, real-time AND frameworks like CONNA, and co-attention-based pairwise learning.
- The use of multiple Graph Attention Networks (GATs) and Heterogeneous Graph Convolutional Networks (HGCN) are novel technical contributions.
- The integration of word embeddings and knowledge graph embeddings are innovative feature engineering techniques.
- Hybrid approaches that combine supervised and unsupervised learning are highlighted as effective solutions.
- The MORE framework, which integrates multiple neural models, is an example of an ensemble approach.
- The use of graph-based methods and attention mechanisms are original system design elements.","  - ""The analysis of the comparable studies reveals that hybrid approaches achieve the best performance, with the top-performing method reaching 89.7 F1-score on AMiner datasets.""
  - ""The results from the studies analyzed show significant variation in performance, with the highest F1-score achieved by Xie, Liu, Wang, and Jia (2022) [40], reaching an impressive 89.7, followed by Cheng, Chen, Zhang, and Tang (2024) [34] with an F1-score of 87.72.""
  - ""The top three methods in our review demonstrate distinct methodologies in addressing AND, each suited to specific challenges in the task.""
  - ""Combining supervised and unsupervised DL approaches in a hybrid setup proves effective, as it retains the advantages of using labeled data for supervision while avoiding an increase in computational complexity""
  - ""the study revealed a heavy reliance on AMiner as the primary dataset, which raises concerns about the generalizability of the evaluated methods.""
  - ""The ideal way to compare state-of-the-art methods in DL is following these two criteria:

• Studies must evaluate their results on the same or comparable datasets. • Studies must evaluate their results with the same evaluation metrics.""","  - ""(Page 7, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 15, Table 1) Below is a rendering of the page up to the first error.""","- The paper provides quantitative performance outcomes in terms of F1-scores for various deep learning methods used in author name disambiguation. The highest F1-score mentioned is 89.7, achieved by a hybrid approach.
- The paper discusses the effectiveness of hybrid approaches, which combine supervised and unsupervised learning, in achieving better performance without increasing computational complexity.
- The reliance on AMiner datasets is highlighted, which raises concerns about the generalizability of the results.
- The paper emphasizes the importance of using comparable datasets and evaluation metrics for fair comparison of different methods.
- The tables included in the paper provide detailed performance metrics for various studies, including F1-scores and dataset sizes.","  - ""The methods analysed and revised in this survey take advantage of different benchmarks in the literature. Most of them rely on datasets derived from AMiner (https://www. AMiner.org), a free online service used to index, search, and mine big scientific data designated to identify connections between researchers, conferences, and publications.""
  - ""The analysis of the comparable studies reveals that hybrid approaches achieve the best performance, with the top-performing method reaching 89.7 F1-score on AMiner datasets.""",,"QUOTES:

- ""The methods analysed and revised in this survey take advantage of different benchmarks in the literature. Most of them rely on datasets derived from AMiner (https://www. AMiner.org), a free online service used to index, search, and mine big scientific data designated to identify connections between researchers, conferences, and publications.""
- ""These benchmarks include: AMiner-WhoIsWho: the largest manually labeled AND benchmark. It features over 1,000,000 articles across three dataset versions, offering high-quality labels and extensive metadata.""
- ""AMiner-534K: a knowledge graph extracted from an AMiner benchmark. Structural triples of the knowledge graph are split into training, testing, and validation for applying representation learning methods.""
- ""The search yielded 52 documents, of which 28 were selected after full-text assessment.""
- ""The literature review was conducted following a systematic and structured approach to ensure the inclusion of relevant, high-quality academic studies addressing AND.""
- ""The search for relevant studies was conducted in multiple stages using a combination of keyword-based and citation-based approaches, integrating both data collection and selection processes.""
- ""DL supervised techniques make use of labelled training data manually created or collected from annotated databases, usually inputted into a classifier.""
- ""Unsupervised approaches do not require labelled data and typically focus on clustering or embedding methods to disambiguate names.""
- ""The analysis of supervised approaches reveals a predominant focus on the AA task, with methods primarily designed as classification systems that assign publications to known author profiles.""
- ""Unsupervised approaches do not require labelled data and typically focus on clustering or embedding methods to disambiguate names.""
- ""Mixed AND approaches incorporate both supervised and unsupervised methods within their pipelines.""
- ""A structured table was created to analyse and compare the selected studies systematically. This table included the following columns, designed to capture essential details and provide a basis for qualitative evaluation: Title, Abstract, Year, DOI, Comparability, Type of Approach, Learning Strategy, Dataset Selected, DL Structure Summary, Limitations, Evaluation Metrics.""
- <table_quotation page_num=7 table_on_page=1 />
- <table_quotation page_num=15 table_on_page=1 />

REASONING:

- The paper primarily uses datasets derived from AMiner, which is a significant source for author name disambiguation (AND) research.
- AMiner-WhoIsWho is highlighted as a large manually labeled benchmark with over 1,000,000 articles, indicating its extensive use for training and evaluation.
- AMiner-534K is another dataset mentioned, which is used for knowledge graph-based approaches and involves splitting data into training, testing, and validation sets.
- The paper describes a systematic literature review process that involved selecting 28 studies from an initial 52 documents based on full-text assessment.
- The paper discusses the use of both supervised and unsupervised learning strategies, with supervised methods relying on labeled data and unsupervised methods focusing on clustering without labeled data.
- The structured table mentioned in the paper includes details about datasets used in various studies, but specific dataset sizes and characteristics are not detailed in the quotes provided.
- The tables at the end of the paper likely contain detailed information about dataset sizes and characteristics, as well as evaluation metrics.","  - ""Author Name Disambiguation (AND) is a critical task for digital libraries aiming to link existing authors with their respective publications.""
  - ""This paper provides a systematic review of state-of-the-art AND techniques based on deep learning within the timeframe 2016-2024, filling a gap in existing literature.""
  - ""The analysis of supervised approaches reveals a predominant focus on the AA task, with methods primarily designed as classification systems that assign publications to known author profiles.""
  - ""Unsupervised approaches do not require labelled data and typically focus on clustering or embedding methods to disambiguate names.""
  - ""Mixed AND approaches incorporate both supervised and unsupervised methods within their pipelines.""
  - ""The literature review was conducted following a systematic and structured approach to ensure the inclusion of relevant, high-quality academic studies addressing AND.""
  - ""The search for relevant studies was conducted in multiple stages using a combination of keyword-based and citation-based approaches, integrating both data collection and selection processes.""
  - ""The analysis of unsupervised approaches reveals a clear predominance of methods focused on the AG task, with techniques primarily designed to cluster publications belonging to unknown or previously unregistered authors.""
  - ""DL approaches have significantly advanced Author Name Disambiguation (AND) by utilizing neural network-based models to process complex academic data.""
  - ""These methods integrate both structured metadata (e.g., co-authorships, affiliations) and unstructured text (e.g., titles, abstracts), enabling the development of robust models for both AA and AG tasks""
  - ""The ideal way to compare state-of-the-art methods in DL is following these two criteria:

• Studies must evaluate their results on the same or comparable datasets. • Studies must evaluate their results with the same evaluation metrics.""
  - ""the study revealed a heavy reliance on AMiner as the primary dataset, which raises concerns about the generalizability of the evaluated methods.""
  - ""In addressing the posed research questions, we conclude that DL has significantly advanced AND, particularly through the adoption of hybrid methods.""
  - ""Future work should prioritize the creation of comprehensive benchmarks and the development of richer datasets to ensure robust and generalizable solutions for academic name disambiguation.""","  - ""(Page 1, Table 1) ""
  - ""(Page 7, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 15, Table 1) Below is a rendering of the page up to the first error.""","- The paper focuses on Author Name Disambiguation (AND) in digital libraries, which is a critical task for accurately linking authors with their publications.
- The target research disciplines or fields are not explicitly mentioned, but the focus on digital libraries suggests a broad application across various academic fields.
- The specific literature review tasks addressed include author assignment (AA) and author grouping (AG), which are central to AND.
- The types of academic documents processed include publications with metadata such as co-authorships, affiliations, titles, and abstracts.
- User types and requirements are not explicitly detailed, but the focus on improving AND suggests that the primary users are likely researchers and librarians.
- Integration with research workflows is implied by the use of deep learning methods to process complex academic data, which could be integrated into digital library systems.
- The paper does not explicitly mention commercial vs. academic applications, but the focus on digital libraries and academic publications suggests an academic context."
The hunt for the last relevant paper: blending the best of humans and AI,"R. van de Schoot, Bruno Messina Coimbra, Tale Evenhuis, Peter Lombaers, F. Weijdema, Laurens de Bruin, Rutger Neeleman, Elizabeth M. Grandfield, M. Sijbrandij, J. Teijema, Elena Jalsovec, Michiel Bron, Sonja Winter, Jonathan de Bruin, M. van Zuiden",10.1080/20008066.2025.2546214,https://doi.org/10.1080/20008066.2025.2546214,European Journal of Psychotraumatology,1,2025,"- Main AI/ML frameworks used: NLP, large language models (LLMs), active learning
- System architecture components: OpenAlex for semantic search
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: OpenAlex
- Overall system design approach: Hybrid workflow with AI as a 'super-assistant'","- Algorithms and models used: Large language models (LLMs)
- Feature extraction techniques: Semantic search via OpenAlex
- Data processing pipelines: Not mentioned
- Training methodologies: Active learning
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Hybrid workflow combining human reviewers and AI tools","- Search and retrieval algorithms: Database searches, snowballing, full-text retrieval, semantic search via OpenAlex
- Relevance ranking approaches: Combination of human reviewers, active learning, and large language models (LLMs)
- Content-based matching techniques: Machine-aided techniques to find studies with missing keywords, unusual phrasing, or limited indexing","- Scalability challenges: The exponential growth of research literature makes it difficult to identify all relevant studies.
- Data quality or availability problems: Machine-aided techniques helped find studies with missing keywords, unusual phrasing, or limited indexing.
- Integration difficulties: A hybrid workflow is necessary, indicating that AI is not yet ready to fully replace human oversight.","- Hybrid or ensemble approaches: Merging traditional and AI-aided search and screening methods.
- Creative problem-solving methods: Employing eight search strategies including database searches, snowballing, full-text retrieval, and semantic search via OpenAlex.
- Novel feature engineering techniques: Using machine-aided techniques to find studies with missing keywords, unusual phrasing, or limited indexing.
- Innovative system design elements: Positioning AI as a ""super-assistant"" to support human reviewers.","- Quantitative results: Identified an additional 3,822 records using AI-aided methods; found 126 relevant studies.
- Qualitative results: Combination of AI tools and human screening improved coverage; AI helped find studies with missing keywords or unusual phrasing.
- No specific metrics on accuracy, precision, F1-scores, processing speed, user satisfaction, system reliability, or scalability are provided.","- Academic databases used: Not specified
- Dataset sizes and characteristics: Original 6,701 search results; additional 3,822 records identified using AI-aided methods; 126 relevant studies
- Data collection methods: Database searches, snowballing, full-text retrieval, semantic search via OpenAlex
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: OpenAlex for semantic search","- Target research disciplines or fields: Psychology, specifically post-traumatic stress symptom (PTSS) trajectories after traumatic events.
- Specific literature review tasks addressed: Identifying all relevant studies for systematic reviews and meta-analyses, particularly finding the ""last relevant paper.""
- Types of academic documents processed: Research articles and studies related to PTSS.
- User types and requirements: Researchers conducting systematic reviews and meta-analyses.
- Integration with research workflows: Combination of traditional and AI-aided search and screening methods.
- Commercial vs. academic applications: Academic.","  - ""modern AI-aided approaches have the potential to act as a powerful ‘super-assistant’ during both the searching and screening phases.""
  - ""a combined, open-source approach – merging traditional and AI-aided search and screening methods""
  - ""semantic search via OpenAlex.""
  - ""All records were screened using a combination of human reviewers, active learning, and large language models (LLMs) for quality control.""
  - ""machine-aided techniques helped find studies with missing keywords, unusual phrasing, or limited indexing.""
  - ""A transparent, hybrid workflow where AI serves as a ‘super-assistant’ can meaningfully extend the reach of systematic reviews and increase the quality of the findings,""",,"- The abstract mentions the use of ""AI-aided approaches"" and ""large language models (LLMs)"" which suggests the involvement of NLP (Natural Language Processing) techniques.
- The mention of ""semantic search via OpenAlex"" indicates the use of semantic search capabilities, likely leveraging OpenAlex as a database or API for searching.
- The abstract describes a ""combined, open-source approach"" which implies a hybrid system architecture that integrates traditional methods with AI-aided methods.
- The use of ""active learning"" suggests a machine learning component that adapts to user feedback.
- The abstract does not specify the exact technical infrastructure such as cloud platforms or computational resources, nor does it detail the integration with existing academic databases beyond OpenAlex.
- The overall system design approach is described as a ""transparent, hybrid workflow"" where AI acts as a 'super-assistant', indicating a collaborative human-AI system.","  - ""All records were screened using a combination of human reviewers, active learning, and large language models (LLMs) for quality control.""
  - ""We applied eight search strategies, including database searches, snowballing, full-text retrieval, and semantic search via OpenAlex.""
  - ""The combination of AI tools and human screening led to 126 relevant studies, with each method uncovering papers the others missed.""
  - ""A transparent, hybrid workflow where AI serves as a ‘super-assistant’ can meaningfully extend the reach of systematic reviews and increase the quality of the findings, but is not ready to replace humans fully.""
  - ""Notably, machine-aided techniques helped find studies with missing keywords, unusual phrasing, or limited indexing.""",,"- The abstract mentions the use of ""large language models (LLMs)"" for screening, which implies the use of neural networks or similar AI models for text analysis.
- The mention of ""active learning"" suggests a training methodology where the model learns from human feedback during the screening process.
- The use of ""semantic search via OpenAlex"" indicates a feature extraction technique that likely involves semantic analysis of text to identify relevant papers.
- The abstract does not provide specific details on data processing pipelines, preprocessing steps, or technical workflow beyond the general approach of combining human and AI methods.
- The focus on a ""hybrid workflow"" suggests a methodology that integrates human and AI components, but specific technical implementation details are not provided.","  - ""We applied eight search strategies, including database searches, snowballing, full-text retrieval, and semantic search via OpenAlex.""
  - ""Across all AI-assisted strategies, 10 additional studies were identified, and while the overall yield was modest, these papers were unique and relevant and would likely have been missed using traditional methods.""
  - ""Notably, machine-aided techniques helped find studies with missing keywords, unusual phrasing, or limited indexing.""
  - ""The combination of AI tools and human screening led to 126 relevant studies, with each method uncovering papers the others missed.""
  - ""All records were screened using a combination of human reviewers, active learning, and large language models (LLMs) for quality control.""",,"- The abstract mentions the use of ""eight search strategies,"" which includes ""database searches, snowballing, full-text retrieval, and semantic search via OpenAlex."" These are examples of search and retrieval algorithms.
- The use of ""human reviewers, active learning, and large language models (LLMs)"" for screening suggests a relevance ranking approach, as these methods are used to evaluate the relevance of the papers found.
- The mention of ""machine-aided techniques"" helping to find studies with ""missing keywords, unusual phrasing, or limited indexing"" implies a content-based matching technique, as these methods are used to identify relevant papers based on their content.
- The abstract does not explicitly mention citation analysis methods, metadata extraction and utilization, recommendation system approaches, or expert/authority identification methods. However, the use of ""semantic search via OpenAlex"" could imply some form of metadata utilization, as OpenAlex is a database that provides metadata about academic papers.","  - ""The exponential growth of research literature makes it increasingly difficult to identify all relevant studies for systematic reviews and meta-analyses.""
  - ""While traditional search methods are labour-intensive, modern AI-aided approaches have the potential to act as a powerful ‘super-assistant’ during both the searching and screening phases.""
  - ""The combination of AI tools and human screening led to 126 relevant studies, with each method uncovering papers the others missed.""
  - ""Notably, machine-aided techniques helped find studies with missing keywords, unusual phrasing, or limited indexing.""
  - ""Across all AI-assisted strategies, 10 additional studies were identified, and while the overall yield was modest, these papers were unique and relevant and would likely have been missed using traditional methods.""
  - ""A transparent, hybrid workflow where AI serves as a ‘super-assistant’ can meaningfully extend the reach of systematic reviews and increase the quality of the findings, but is not ready to replace humans fully.""",,"- The abstract mentions the challenge of identifying all relevant studies due to the exponential growth of research literature, which implies a scalability challenge in terms of handling the vast amount of data.
- The use of AI-aided approaches suggests that there might be algorithm performance issues or data quality problems, as these methods are used to overcome limitations in traditional search methods.
- The mention of machine-aided techniques helping find studies with missing keywords or unusual phrasing indicates potential data quality or availability problems, as these studies might not be easily discoverable through traditional means.
- The need for a hybrid workflow where AI serves as a 'super-assistant' but is not ready to replace humans fully suggests integration difficulties or limitations in the evaluation methodology, as human oversight is still necessary.
- The abstract does not explicitly mention computational resource constraints or technical bottlenecks, but the emphasis on the potential of AI-aided approaches implies that these might be areas of ongoing research and development.","  - ""This paper evaluates how a combined, open-source approach – merging traditional and AI-aided search and screening methods – can help identify all relevant literature up to the ‘last relevant paper’ for a systematic review on post-traumatic stress symptom (PTSS) trajectories after traumatic events.""
  - ""We applied eight search strategies, including database searches, snowballing, full-text retrieval, and semantic search via OpenAlex.""
  - ""All records were screened using a combination of human reviewers, active learning, and large language models (LLMs) for quality control.""
  - ""The combination of AI tools and human screening led to 126 relevant studies, with each method uncovering papers the others missed.""
  - ""Notably, machine-aided techniques helped find studies with missing keywords, unusual phrasing, or limited indexing.""
  - ""A transparent, hybrid workflow where AI serves as a ‘super-assistant’ can meaningfully extend the reach of systematic reviews and increase the quality of the findings, but is not ready to replace humans fully.""",,"- The abstract describes a ""combined, open-source approach"" that merges traditional and AI-aided methods, which is a hybrid or ensemble approach.
- The use of ""eight search strategies"" including ""database searches, snowballing, full-text retrieval, and semantic search via OpenAlex"" indicates a creative problem-solving method by employing multiple search techniques.
- The integration of ""human reviewers, active learning, and large language models (LLMs)"" for screening is another example of a hybrid approach, combining human judgment with AI tools.
- The mention of ""machine-aided techniques"" helping to find studies with missing keywords or unusual phrasing suggests a novel feature engineering technique, as it involves using AI to overcome traditional indexing limitations.
- The abstract highlights the use of AI as a ""super-assistant,"" which is an innovative system design element, as it positions AI in a supportive role rather than a replacement for human reviewers.","  - ""On top of replicating the original 6,701 search results, we identified an additional 3,822 records using AI-aided methods.""
  - ""Across all AI-assisted strategies, 10 additional studies were identified, and while the overall yield was modest, these papers were unique and relevant and would likely have been missed using traditional methods.""
  - ""Our findings demonstrate that even when returns are low, AI-aided approaches can meaningfully enhance coverage and offer a scalable path forward when combined with screening prioritisation.""
  - ""The combination of AI tools and human screening led to 126 relevant studies, with each method uncovering papers the others missed.""
  - ""Notably, machine-aided techniques helped find studies with missing keywords, unusual phrasing, or limited indexing.""",,"- The abstract provides quantitative results in terms of the number of additional records identified using AI-aided methods (3,822) and the number of relevant studies found (126).
- It highlights the effectiveness of combining AI tools with human screening, indicating a qualitative improvement in coverage.
- The mention of finding studies with missing keywords or unusual phrasing suggests an improvement in recall, as these studies might have been missed by traditional methods.
- The abstract does not provide specific metrics such as accuracy, precision, F1-scores, processing speed, or user satisfaction.
- There is no direct comparison with baseline methods or specific metrics on system reliability, robustness, or scalability.
- The abstract focuses on the qualitative benefits of using AI-aided approaches in enhancing coverage and scalability.","  - ""We applied eight search strategies, including database searches, snowballing, full-text retrieval, and semantic search via OpenAlex.""
  - ""All records were screened using a combination of human reviewers, active learning, and large language models (LLMs) for quality control.""
  - ""On top of replicating the original 6,701 search results, we identified an additional 3,822 records using AI-aided methods.""
  - ""Notably, machine-aided techniques helped find studies with missing keywords, unusual phrasing, or limited indexing.""
  - ""The combination of AI tools and human screening led to 126 relevant studies, with each method uncovering papers the others missed.""",,"- The abstract mentions the use of ""database searches"" as part of the search strategies, but it does not specify which academic databases were used (e.g., Scopus, Web of Science).
- The abstract does not provide information on dataset sizes and characteristics beyond the number of search results and relevant studies identified.
- The data collection methods are described as including ""database searches, snowballing, full-text retrieval, and semantic search via OpenAlex.""
- There is no mention of training/validation/test data splits or data preprocessing and cleaning approaches in the abstract.
- The abstract does not specify any external knowledge bases or ontologies used beyond the mention of ""OpenAlex"" for semantic search.","  - ""The exponential growth of research literature makes it increasingly difficult to identify all relevant studies for systematic reviews and meta-analyses.""
  - ""This paper evaluates how a combined, open-source approach – merging traditional and AI-aided search and screening methods – can help identify all relevant literature up to the ‘last relevant paper’ for a systematic review on post-traumatic stress symptom (PTSS) trajectories after traumatic events.""
  - ""We applied eight search strategies, including database searches, snowballing, full-text retrieval, and semantic search via OpenAlex.""
  - ""All records were screened using a combination of human reviewers, active learning, and large language models (LLMs) for quality control.""
  - ""The combination of AI tools and human screening led to 126 relevant studies, with each method uncovering papers the others missed.""
  - ""Our findings demonstrate that even when returns are low, AI-aided approaches can meaningfully enhance coverage and offer a scalable path forward when combined with screening prioritisation.""
  - ""A transparent, hybrid workflow where AI serves as a ‘super-assistant’ can meaningfully extend the reach of systematic reviews and increase the quality of the findings, but is not ready to replace humans fully.""",,"- The target research discipline or field is psychology, specifically focusing on post-traumatic stress symptom (PTSS) trajectories after traumatic events.
- The specific literature review task addressed is identifying all relevant studies for systematic reviews and meta-analyses, particularly finding the ""last relevant paper.""
- The types of academic documents processed include research articles and studies related to PTSS.
- The user types and requirements involve researchers conducting systematic reviews and meta-analyses who need to identify relevant literature efficiently.
- The integration with research workflows involves using a combination of traditional and AI-aided search and screening methods to enhance coverage and quality.
- The application is academic, as it is focused on improving the process of systematic reviews and meta-analyses in research."
Decision Support Architecture for Primary Studies Evaluation,V. Nepomuceno,-,-,-,0,2015,"- Main AI/ML frameworks used: Text mining techniques and ontologies
- System architecture components: Not mentioned
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Not mentioned
- Overall system design approach: Improving primary studies selection and quality assessment using text mining and ontologies","- Algorithms and models used: Text mining techniques
- Feature extraction techniques: Not mentioned
- Data processing pipelines: Not mentioned
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Not mentioned","- Content-based matching techniques: Text mining techniques
- Metadata extraction and utilization: Use of ontologies","- Scalability challenges: The workload required for evaluating studies can bias the results, indicating potential scalability issues.
- Integration difficulties: The use of text mining techniques and ontologies may pose integration challenges.
- Evaluation methodology limitations: Conducting experiments to evaluate the proposed architecture suggests potential challenges in evaluating its effectiveness.","The innovative solution in V. Nepomuceno (2015) is the creation of a decision support architecture that uses text mining techniques and ontologies to improve the selection and quality assessment of studies in systematic literature reviews, aiming to reduce workload and evaluation bias.",Not mentioned (the abstract does not provide specific performance results or metrics),"Not mentioned (the abstract does not specify any data sources, datasets, or knowledge resources beyond the use of text mining techniques and ontologies)","- Target research disciplines or fields: General academic fields (not specified)
- Specific literature review tasks addressed: Selection process of individual studies, quality assessment
- Types of academic documents processed: Primary studies
- User types and requirements: Researchers or participants in systematic reviews
- Integration with research workflows: Improving execution time and reducing evaluation bias
- Commercial vs. academic applications: Academic application (implied)","  - ""Improving the primary studies selection and quality assessment processes by using text mining techniques and ontologies to construct a decision support architecture.""
  - ""Creating a decision support architecture to assist participants of a systematic review in the selection process of the individual studies and quality assessment of these studies, possibly improving the execution time and reducing the evaluation bias.""
  - ""We will also conduct experiments to evaluate the proposed architecture.""",,"- The abstract mentions the use of ""text mining techniques and ontologies"" to construct a decision support architecture. This suggests that the technical architecture involves natural language processing (NLP) techniques, which are a subset of AI/ML frameworks.
- The abstract does not specify any particular deep learning frameworks or other AI/ML techniques beyond text mining and ontologies.
- There is no mention of specific system architecture components such as databases, APIs, or interfaces.
- The abstract does not provide details on the technical infrastructure, such as cloud platforms or computational resources.
- There is no information on integration with existing academic databases or platforms.
- The overall system design approach is focused on improving the selection and quality assessment processes using text mining and ontologies, but specific technical details are not provided.","  - ""Improving the primary studies selection and quality assessment processes by using text mining techniques and ontologies to construct a decision support architecture.""
  - ""We will also conduct experiments to evaluate the proposed architecture.""",,"- The abstract mentions the use of ""text mining techniques and ontologies"" to improve the selection and quality assessment processes. This suggests that these are key components of the implementation methods.
- The term ""text mining techniques"" implies the use of algorithms and models related to text analysis, but the abstract does not specify which ones (e.g., neural networks, clustering, classification).
- The mention of ""ontologies"" suggests a structured approach to organizing and integrating data, but again, specific details are not provided.
- The abstract does not provide specific details on feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or technical workflow.
- The mention of ""conducting experiments"" indicates that there will be some form of evaluation or testing, but the abstract does not specify the methodologies or technical details of these experiments.","  - ""Improving the primary studies selection and quality assessment processes by using text mining techniques and ontologies to construct a decision support architecture.""
  - ""Creating a decision support architecture to assist participants of a systematic review in the selection process of the individual studies and quality assessment of these studies, possibly improving the execution time and reducing the evaluation bias.""",,"- The abstract mentions the use of ""text mining techniques and ontologies"" to improve the selection and quality assessment of primary studies. This suggests that these techniques are part of the paper discovery process.
- Text mining techniques can be related to content-based matching techniques, as they involve analyzing the content of papers to identify relevance.
- The use of ontologies implies a structured approach to categorizing and linking concepts, which can aid in relevance ranking and metadata extraction.
- The abstract does not explicitly mention other techniques like search and retrieval algorithms, citation analysis methods, recommendation system approaches, or expert/authority identification methods.","  - ""The workload required for this process may bias the evaluation of the studies, affecting the result.""
  - ""Contribution. Improve the primary studies selection and quality assessment processes, reducing its workload, and lowering the evaluation bias in systematic literature reviews.""
  - ""We will also conduct experiments to evaluate the proposed architecture.""
  - ""Improving the primary studies selection and quality assessment processes by using text mining techniques and ontologies to construct a decision support architecture.""",,"- The abstract mentions that the workload required for evaluating studies can bias the results, which implies a challenge in managing the volume of work efficiently. This could relate to scalability challenges or computational resource constraints.
- The use of text mining techniques and ontologies suggests potential challenges in integrating these technologies or ensuring their performance is adequate for the task.
- The mention of conducting experiments to evaluate the proposed architecture implies that there may be challenges in evaluating the effectiveness of the system, which could relate to evaluation methodology limitations.
- The abstract does not explicitly mention specific technical challenges such as algorithm performance issues, data quality or availability problems, integration difficulties, or technical bottlenecks. However, these could be inferred as potential challenges in developing and implementing the decision support architecture.","  - ""Improving the primary studies selection and quality assessment processes by using text mining techniques and ontologies to construct a decision support architecture.""
  - ""Creating a decision support architecture to assist participants of a systematic review in the selection process of the individual studies and quality assessment of these studies, possibly improving the execution time and reducing the evaluation bias.""
  - ""Contribution. Improve the primary studies selection and quality assessment processes, reducing its workload, and lowering the evaluation bias in systematic literature reviews.""",,"- The abstract mentions the creation of a ""decision support architecture"" to assist in the selection and quality assessment of studies, which is a novel approach to systematic literature reviews.
- The use of ""text mining techniques and ontologies"" is highlighted as a method to improve these processes, indicating a technical innovation in how studies are evaluated.
- The aim to ""reduce the workload"" and ""lower the evaluation bias"" suggests that the architecture is designed to address known problems in systematic reviews, such as bias and time-consuming evaluation processes.
- The abstract does not specify new algorithmic contributions, creative problem-solving methods, or novel feature engineering techniques explicitly, but the use of text mining and ontologies implies a hybrid approach to study evaluation.","  - ""We will also conduct experiments to evaluate the proposed architecture.""
  - ""Contribution. Improve the primary studies selection and quality assessment processes, reducing its workload, and lowering the evaluation bias in systematic literature reviews.""
  - ""Method. Improving the primary studies selection and quality assessment processes by using text mining techniques and ontologies to construct a decision support architecture.""
  - ""Aim. Creating a decision support architecture to assist participants of a systematic review in the selection process of the individual studies and quality assessment of these studies, possibly improving the execution time and reducing the evaluation bias.""",,"- The abstract mentions the aim of creating a decision support architecture to improve the selection and quality assessment processes in systematic reviews, which implies potential performance improvements.
- The method involves using text mining techniques and ontologies, which could lead to improved accuracy and efficiency.
- The contribution section highlights the reduction of workload and evaluation bias, suggesting qualitative performance outcomes.
- The abstract states that experiments will be conducted to evaluate the proposed architecture, which implies that some form of performance results might be expected.
- However, the abstract does not provide specific quantitative or qualitative performance results such as accuracy, precision, recall, F1-scores, processing speed, user satisfaction, system reliability, or scalability test results.","  - ""Improving the primary studies selection and quality assessment processes by using text mining techniques and ontologies to construct a decision support architecture.""
  - ""Creating a decision support architecture to assist participants of a systematic review in the selection process of the individual studies and quality assessment of these studies, possibly improving the execution time and reducing the evaluation bias.""
  - ""We will also conduct experiments to evaluate the proposed architecture.""
  - ""A systematic literature review is a process in which all relevant available research about a research question is identified, evaluated, and interpreted through individual studies.""",,"- The abstract mentions the use of ""text mining techniques and ontologies"" to construct a decision support architecture. This suggests that external knowledge bases or ontologies are used in the process.
- The abstract does not specify any academic databases used, such as Scopus or Web of Science, nor does it provide details on dataset sizes, characteristics, data collection methods, or data preprocessing and cleaning approaches.
- There is no mention of training/validation/test data splits or specific data preprocessing and cleaning methods.
- The focus is on improving the selection and quality assessment processes using text mining and ontologies, but specific data sources or datasets are not detailed.","  - ""Improve the primary studies selection and quality assessment processes, reducing its workload, and lowering the evaluation bias in systematic literature reviews.""
  - ""Improving the primary studies selection and quality assessment processes by using text mining techniques and ontologies to construct a decision support architecture.""
  - ""Creating a decision support architecture to assist participants of a systematic review in the selection process of the individual studies and quality assessment of these studies, possibly improving the execution time and reducing the evaluation bias.""
  - ""A systematic literature review is a process in which all relevant available research about a research question is identified, evaluated, and interpreted through individual studies.""",,"- The abstract discusses the application context of a decision support architecture for systematic literature reviews, which is a specific task within the broader field of research.
- The target research disciplines or fields are not explicitly mentioned, but the focus on systematic literature reviews suggests a general application across various academic fields.
- The specific literature review tasks addressed include the selection process of individual studies and quality assessment, which are key components of systematic reviews.
- The types of academic documents processed are implied to be primary studies, as the architecture aims to improve their selection and quality assessment.
- The user types are likely researchers or participants involved in systematic reviews, as the architecture is designed to assist them.
- The integration with research workflows is implied by the aim to improve execution time and reduce evaluation bias, suggesting a potential integration into existing systematic review processes.
- The abstract does not specify whether the application is commercial or academic, but the context suggests an academic application given the focus on systematic literature reviews."
Intelligent transportation systems (ITS): A systematic review using a Natural Language Processing (NLP) approach,"Zulkarnain, Tsarina Dwi Putri",10.1016/j.heliyon.2021.e08615,https://doi.org/10.1016/j.heliyon.2021.e08615,Heliyon,39,2021,"- Main AI/ML frameworks used: Natural Language Processing (NLP) methods including Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram)
- System architecture components: Not mentioned
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Not mentioned
- Overall system design approach: Focus on using NLP to simplify the review process and identify research gaps","- Algorithms and models used: Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), word embedding (continuous skip-gram)
- Feature extraction techniques: Not mentioned
- Data processing pipelines: Not mentioned
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Not mentioned","- Content-based matching techniques: Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), word embedding (continuous skip-gram)
- Relevance ranking approaches: Systematic separation of unrelated documents
- Metadata extraction and utilization: NER for identifying key terms and concepts","- Scalability challenges: The growing number of journal articles complicates reviews and makes it hard to identify research gaps.
- Data quality or availability problems: Existing software relies on laborious tasks and manual reading, indicating a need for more automated and efficient methods.","- New algorithmic contributions: Not explicitly mentioned
- Creative problem-solving methods: Use of NLP methods to simplify review process
- Hybrid or ensemble approaches: Combination of NER, LDA, and word embedding
- Novel feature engineering techniques: Not explicitly mentioned
- Innovative evaluation metrics: Not explicitly mentioned
- Technical workarounds for known problems: Systematic separation of unrelated documents
- Original system design elements: Framework design for efficient systematic reviews",Not mentioned (the abstract does not provide specific performance results or metrics),"Not mentioned (the abstract does not provide specific information on data sources, dataset sizes, or data collection methods)","- Target research disciplines or fields: Intelligent Transportation Systems (ITS), multidisciplinary science
- Specific literature review tasks addressed: Systematic review of ITS
- Types of academic documents processed: Journal articles, proceedings papers
- User types and requirements: Researchers or reviewers conducting systematic reviews
- Integration with research workflows: Simplifies review process for large datasets
- Commercial vs. academic applications: Academic","  - ""The proposed framework consists of Natural Language Processing (NLP) methods i.e., Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram).""
  - ""The framework can systematically separate unrelated documents and simplify the review process for large dataset.""",,"- The abstract mentions the use of Natural Language Processing (NLP) methods as part of the technical architecture. This indicates that NLP is a main AI/ML framework used in the study.
- Specific NLP techniques mentioned include Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram). These are key components of the system architecture.
- The abstract does not mention any specific system architecture components such as databases, APIs, or interfaces, nor does it discuss technical infrastructure like cloud platforms or computational resources.
- There is no mention of integration with existing academic databases or platforms in the abstract.
- The overall system design approach seems to focus on using NLP to simplify the review process and identify research gaps, but specific details about the system design are not provided.","  - ""The proposed framework consists of Natural Language Processing (NLP) methods i.e., Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram).""",,"- The abstract mentions the use of Natural Language Processing (NLP) methods, which is a broad category of techniques used for text analysis.
- Specifically, it lists three NLP methods:
  - Named Entity Recognition (NER): This is a technique used to identify and categorize named entities in text into predefined categories such as names of people, organizations, and locations.
  - Latent Dirichlet Allocation (LDA): This is a topic modeling technique used to discover hidden topics in a large corpus of text.
  - Word embedding (continuous skip-gram): This is a technique used to represent words as vectors in a high-dimensional space, where semantically similar words are mapped to nearby points.
- These methods are part of the proposed framework for conducting systematic reviews, indicating their role in processing and analyzing the text data.
- The abstract does not provide specific details on feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or technical workflow beyond the mention of these NLP methods.","  - ""The proposed framework consists of Natural Language Processing (NLP) methods i.e., Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram).""
  - ""The framework can systematically separate unrelated documents and simplify the review process for large dataset.""
  - ""It enables this study to explore the context of research articles and their overall interpretation to determine and define the directions of knowledge growth and ITS development.""",,"- The abstract mentions the use of Natural Language Processing (NLP) methods, specifically Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram). These are content-based matching techniques used to analyze and interpret the content of research articles.
- NER is used to identify and extract named entities from text, which can help in metadata extraction and utilization by identifying key terms and concepts.
- LDA is a method for topic modeling, which can help in relevance ranking by identifying the topics or themes within a large corpus of text.
- Word embedding (continuous skip-gram) is a technique for representing words as vectors in a high-dimensional space, which can aid in content-based matching by capturing semantic relationships between words.
- The framework's ability to ""systematically separate unrelated documents"" suggests a form of relevance ranking, where documents are filtered based on their relevance to the topic of ITS.
- The abstract does not explicitly mention search and retrieval algorithms, citation analysis methods, recommendation system approaches, or expert/authority identification methods. However, the focus on NLP techniques implies a content-based approach to paper discovery.","  - ""The growing number of journal articles makes ITS reviews complicated, and research gaps can be difficult to identify.""
  - ""The existing software for systematic reviews still relies on highly laborious tasks, manual reading, and a homogeneous dataset of research articles.""
  - ""The proposed framework consists of Natural Language Processing (NLP) methods i.e., Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram).""
  - ""The framework can systematically separate unrelated documents and simplify the review process for large dataset.""",,"- The abstract mentions that the growing number of journal articles complicates reviews and makes it hard to identify research gaps. This suggests a scalability challenge, as the current methods struggle to handle large datasets efficiently.
- The existing software for systematic reviews relies on laborious tasks and manual reading, indicating a data quality or availability problem. The need for manual intervention suggests that current methods are not automated or efficient enough.
- The abstract does not explicitly mention algorithm performance issues, integration difficulties, evaluation methodology limitations, computational resource constraints, or technical bottlenecks. However, the mention of a proposed framework using NLP methods implies an attempt to address these challenges by improving the efficiency and automation of the review process.
- The ability of the framework to separate unrelated documents and simplify the review process for large datasets suggests an improvement in scalability and data quality issues.","  - ""To our knowledge, compared to prior research regarding systematic review of ITS, this study offers more thorough review.""
  - ""The framework can systematically separate unrelated documents and simplify the review process for large dataset.""
  - ""It enables this study to explore the context of research articles and their overall interpretation to determine and define the directions of knowledge growth and ITS development.""
  - ""The proposed framework consists of Natural Language Processing (NLP) methods i.e., Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram).""",,"- The abstract mentions the use of NLP methods such as NER, LDA, and word embedding, which are innovative technical approaches for systematic reviews. These methods are novel in the context of ITS reviews as they enable the exploration of article context and interpretation.
- The framework's ability to systematically separate unrelated documents and simplify the review process for large datasets is a creative problem-solving method. It addresses the issue of manual reading and laborious tasks in traditional systematic reviews.
- The use of these NLP methods can be considered a hybrid or ensemble approach, as it combines multiple techniques to achieve a comprehensive review.
- The abstract does not explicitly mention new algorithmic contributions, novel feature engineering techniques, or innovative evaluation metrics. However, the application of NLP methods to systematic reviews is an innovative solution in itself.
- The framework's design to address the complexity of ITS reviews and promote efficient systematic reviews is an original system design element.","  - ""The framework can systematically separate unrelated documents and simplify the review process for large dataset.""
  - ""The proposed framework consists of Natural Language Processing (NLP) methods i.e., Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram).""
  - ""To our knowledge, compared to prior research regarding systematic review of ITS, this study offers more thorough review.""",,"- The abstract mentions the use of NLP methods such as NER, LDA, and word embedding, which are part of the framework proposed for systematic reviews. However, it does not provide any specific performance metrics such as accuracy, precision, recall, or F1-scores for these methods.
- The abstract states that the framework can simplify the review process for large datasets, which implies some level of efficiency, but no specific processing speed or efficiency metrics are provided.
- There is no mention of comparison with baseline methods or any quantitative performance results.
- The abstract does not provide any information on user satisfaction, system reliability, robustness, or scalability test results.
- The statement that the study offers a ""more thorough review"" compared to prior research suggests a qualitative improvement but lacks specific quantitative performance results.","  - ""The existing software for systematic reviews still relies on highly laborious tasks, manual reading, and a homogeneous dataset of research articles.""
  - ""The framework can systematically separate unrelated documents and simplify the review process for large dataset.""
  - ""The proposed framework consists of Natural Language Processing (NLP) methods i.e., Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram).""",,"- The abstract mentions that existing software for systematic reviews relies on ""a homogeneous dataset of research articles,"" which suggests that the study might be using a dataset of research articles, but it does not specify the source or size of this dataset.
- The abstract does not mention any specific academic databases used (e.g., Scopus, Web of Science) or any external knowledge bases or ontologies.
- The study proposes a framework using NLP methods, but it does not provide details on data collection methods, dataset sizes, or characteristics.
- There is no information on training/validation/test data splits or data preprocessing and cleaning approaches.
- The abstract focuses on the framework's ability to simplify the review process and separate unrelated documents, but it lacks specific details about the data sources or datasets used.","  - ""Intelligent Transportation Systems (ITS) is not a new concept.""
  - ""ITS has been cited in various journal articles and proceedings papers around the world, and it has become increasingly popular.""
  - ""ITS involves multidisciplinary science.""
  - ""The growing number of journal articles makes ITS reviews complicated, and research gaps can be difficult to identify.""
  - ""The existing software for systematic reviews still relies on highly laborious tasks, manual reading, and a homogeneous dataset of research articles.""
  - ""This study proposes a framework that can address these issues, return a comprehensive systematic review of ITS, and promote efficient systematic reviews.""
  - ""The proposed framework consists of Natural Language Processing (NLP) methods i.e., Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram).""
  - ""It enables this study to explore the context of research articles and their overall interpretation to determine and define the directions of knowledge growth and ITS development.""
  - ""The framework can systematically separate unrelated documents and simplify the review process for large dataset.""
  - ""To our knowledge, compared to prior research regarding systematic review of ITS, this study offers more thorough review.""",,"- The target research discipline or field is Intelligent Transportation Systems (ITS), which is a multidisciplinary science.
- The specific literature review task addressed is the systematic review of ITS, which involves dealing with a large number of journal articles and proceedings papers.
- The types of academic documents processed are journal articles and proceedings papers.
- The framework is designed to simplify the review process for large datasets, which implies it is intended for researchers or reviewers who need to conduct systematic reviews.
- The integration with research workflows is implied by the framework's ability to systematically separate unrelated documents and simplify the review process.
- The application is academic, as it is focused on systematic reviews of research articles and proceedings papers in the ITS field.
- The framework uses NLP methods like NER, LDA, and word embedding, which are key technologies for deep research in literature review tasks."
NAVIGATING TRENDS AND TENSIONS IN DIGITAL TRANSFORMATION: A SYSTEMATIC REVIEW,Indah Arifah,10.31947/aiccon2025.v1i1.47744,https://doi.org/10.31947/aiccon2025.v1i1.47744,Proceeding of The 5th ASPIKOM  International  CommunicationConference (AICCON 2025),0,2025,Not mentioned (the paper does not provide information on the technical architecture or system components),"- Algorithms and models used: Multiple Correspondence Analysis (MCA)
- Feature extraction techniques: Keyword co-occurrence networks
- Data processing pipelines: Data collection from Scopus and Web of Science, bibliometric preprocessing, matrix construction, data reduction, network matrix creation, visualization
- Training methodologies: Not mentioned
- Preprocessing steps: Normalization of bibliographic records
- Technical workflow or methodology: Living systematic review methodology combining bibliometric and thematic analyses","- Search and retrieval algorithms: Use of Boolean operators to capture relevant keywords.
- Relevance ranking approaches: Keyword matching using Boolean operators.
- Citation analysis methods: Historical direct citation networks, bibliometric indicators (h-index, g-index, m-index).
- Content-based matching techniques: Keyword co-occurrence networks, multiple correspondence analysis (MCA).
- Metadata extraction and utilization: Focus on bibliographic records, key metadata fields (publication year, author names).
- Recommendation system approaches: Not explicitly mentioned, but implied through bibliometric indicators and citation networks.
- Expert/authority identification methods: Bibliometric indicators (h-index, g-index, m-index).","- Algorithm performance issues: Not explicitly mentioned.
- Data quality or availability problems: Reliance on indexed data from Scopus and Web of Science may introduce publication and language bias; inconsistent reporting of participant-level data.
- Scalability challenges: Not explicitly mentioned.
- Integration difficulties: Underrepresentation of ethics, data privacy, and inclusivity.
- Evaluation methodology limitations: Absence of risk of bias assessment tools; lack of metaanalysis due to methodological heterogeneity.
- Computational resource constraints: Not explicitly mentioned.
- Technical bottlenecks or failure points: Challenges in enhancing citation impact and integrating ethical principles.","- AI-driven personalization
- Sentiment analysis
- Deep learning
- Ethical design principles
- Privacy concerns","- Accuracy, precision, recall, F1-scores: Not mentioned
- Processing speed and efficiency metrics: Not mentioned
- Comparison with baseline methods: Not mentioned
- User satisfaction or usability results: Not mentioned
- System reliability and robustness measures: Not mentioned
- Scalability test results: Not mentioned
- Quantitative performance outcomes:
  - Average document age: 2.24 years
  - Mean citations per document: 10.47
  - Average citation count per document in 2025: 0.55
- Qualitative performance outcomes:
  - Increased international collaboration
  - Rise in interdisciplinary integration
  - Thematic map analysis and historical citation network provide insights into research theme development and structure","- Academic databases used: Scopus, Web of Science
- Dataset sizes and characteristics: 1015 publications across 268 sources, average document age of 2.24 years, mean citations per document of 10.47
- Data collection methods: Boolean operators to select relevant publications based on specific themes and criteria
- Data preprocessing and cleaning approaches: Normalization of bibliographic records, consolidation of synonyms
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: Digital transformation, AI personalization, sentiment analysis, digital ethics
- Specific literature review tasks addressed: Synthesizing knowledge on digital innovation, analyzing bibliometric trends, identifying dominant themes
- Types of academic documents processed: Peer-reviewed publications from Scopus and Web of Science
- User types and requirements: Ethical design and cross-cultural adaptability for diverse user groups
- Integration with research workflows: Bibliometric mapping and thematic analysis for understanding evolving academic priorities
- Commercial vs. academic applications: Primarily academic","  - ""This studies adopted a living systematic review methodology, combining bibliometric and thematic analyses peer-reviewed publications drawn from Scopus and Web of Science.""
  - ""The study emphasizes the need for frameworks that incorporate ethical design and cross-cultural adaptability.""
  - ""The research design followed a structured process comprising several stages: data collection, bibliometric preprocessing, matrix construction, data reduction, network matrix creation, and final visualization through mapping techniques.""
  - ""Data collection was performed using two prominent and credible databases-Scopus and Web of Science.""
  - ""The search strategy involved the use of Boolean operators to capture relevant permutations of keywords.""
  - ""Throughout the analysis, the Bibliometrix R package and its web interface Biblioshiny were employed for statistical and visual processing.""
  - ""VOSviewer was used to generate network diagrams and thematic maps, allowing for intuitive exploration of complex bibliometric relationships.""
  - ""the methodological framework adopted in this study reflects a comprehensive and interdisciplinary approach to digital research analysis.""
  - ""the study aspires to inform both academic inquiry and policy-making in the digital age.""",,"- The paper discusses the use of a ""living systematic review methodology,"" which involves combining bibliometric and thematic analyses. This suggests a focus on analyzing existing literature rather than developing new technical architectures.
- The study emphasizes the need for frameworks that incorporate ethical design and cross-cultural adaptability, but it does not specify any particular technical architecture or system components.
- The research design involves data collection from Scopus and Web of Science, which are academic databases, but there is no mention of integrating these databases into a specific technical architecture.
- The use of the Bibliometrix R package and VOSviewer for analysis and visualization indicates tools used for bibliometric analysis rather than components of a technical architecture.
- The paper does not mention any specific AI/ML frameworks, system architecture components, technical infrastructure, or integration with existing academic databases or platforms.","  - ""This studies adopted a living systematic review methodology, combining bibliometric and thematic analyses peer-reviewed publications drawn from Scopus and Web of Science.""
  - ""The research design followed a structured process comprising several stages: data collection, bibliometric preprocessing, matrix construction, data reduction, network matrix creation, and final visualization through mapping techniques.""
  - ""Data collection was performed using two prominent and credible databases-Scopus and Web of Science.""
  - ""Publications were included based on the following criteria: articles written in English, published between January 2020 and March 2025, and directly relevant to themes such as digital commerce, recommendation systems, sentiment analysis, and ethical computing.""
  - ""The search strategy involved the use of Boolean operators to capture relevant permutations of keywords.""
  - ""Following the initial retrieval, a rigorous data cleaning and preprocessing phase was implemented. Bibliographic records were normalized to standardize keyword entries, author names, and institutional affiliations.""
  - ""Data extraction focused on key metadata fields: publication year, document type, author(s), country of affiliation, keywords, citation count, and references.""
  - ""To analyze the collaboration structure within the field, a historical direct citation network was constructed and visualized.""
  - ""To quantify thematic development, keyword co-occurrence networks were constructed and subjected to multiple correspondence analysis (MCA).""
  - ""The thematic map analysis (Figure 4 ) organizes research themes into quadrants based on relevance and development.""
  - ""The historical citation network (Figure 9 ) shows the intellectual structure of this field.""
  - ""Throughout the analysis, the Bibliometrix R package and its web interface Biblioshiny were employed for statistical and visual processing. VOSviewer was used to generate network diagrams and thematic maps,""",,"- The paper describes a systematic review methodology that combines bibliometric and thematic analyses, which involves data collection from Scopus and Web of Science.
- The research design includes stages such as data collection, bibliometric preprocessing, matrix construction, and visualization, indicating a structured approach to data processing.
- Data collection involved using Boolean operators to search for relevant publications, and preprocessing included normalizing bibliographic records.
- The paper mentions the use of multiple correspondence analysis (MCA) for thematic development and the construction of historical direct citation networks for collaboration analysis.
- The use of Bibliometrix R package and VOSviewer for statistical and visual processing suggests specific technical tools were employed.
- The paper does not mention specific algorithms like neural networks or clustering but focuses on bibliometric and thematic analysis techniques.","  - ""This studies adopted a living systematic review methodology, combining bibliometric and thematic analyses peer-reviewed publications drawn from Scopus and Web of Science.""
  - ""The search strategy involved the use of Boolean operators to capture relevant permutations of keywords.""
  - ""Key terms included ""e-commerce"" OR ""online retail"" OR ""digital commerce"" OR ""internet shopping"" AND ""research"" OR ""study"" OR ""analysis"" OR ""investigation"" ada juga Boolean ""social media"" OR ""email marketing"" OR ""content marketing"" OR ""SEO"" AND ""strategy"" OR ""tactics"" OR ""approach"" OR ""method"".""
  - ""Following the initial retrieval, a rigorous data cleaning and preprocessing phase was implemented. Bibliographic records were normalized to standardize keyword entries, author names, and institutional affiliations.""
  - ""Data extraction focused on key metadata fields: publication year, document type, author(s), country of affiliation, keywords, citation count, and references.""
  - ""To analyze the collaboration structure within the field, a historical direct citation network was constructed and visualized.""
  - ""To quantify thematic development, keyword co-occurrence networks were constructed and subjected to multiple correspondence analysis (MCA).""
  - ""The thematic map analysis (Figure 4 ) organizes research themes into quadrants based on relevance and development.""
  - ""The historical citation network (Figure 9 ) shows the intellectual structure of this field.""
  - ""The results indicated a total of 1015 publications across 268 sources, with an average document age of 2.24 years and mean citations per document of 10.47.""
  - ""Top-performing authors and documents were identified using traditional bibliometric indicators, including h-index, g-index, m-index, and normalized citation scores.""
  - ""Throughout the analysis, the Bibliometrix R package and its web interface Biblioshiny were employed for statistical and visual processing. VOSviewer was used to generate network diagrams and thematic maps,""",,"- The paper uses a living systematic review methodology, which involves continuous updates and incorporates bibliometric and thematic analyses. This suggests a structured approach to paper discovery.
- The search strategy involves Boolean operators to capture relevant keywords, indicating a systematic search and retrieval algorithm.
- The use of Boolean operators and specific keywords suggests a relevance ranking approach based on keyword matching.
- Citation analysis is conducted through the construction of historical direct citation networks and the use of bibliometric indicators like h-index and g-index.
- Content-based matching is implied through the use of keyword co-occurrence networks and multiple correspondence analysis (MCA).
- Metadata extraction is evident from the focus on bibliographic records and key metadata fields like publication year and author names.
- Recommendation system approaches are not explicitly mentioned, but the use of bibliometric indicators and citation networks could imply a form of recommendation based on impact and collaboration.
- Expert/authority identification is facilitated through the use of bibliometric indicators like h-index and g-index to identify top-performing authors.","  - ""the bibliometric analysis is inherently reliant on the quality and completeness of indexed data from selected databases (Scopus and Web of Science), which may introduce publication and language bias.""
  - ""participant-level data were inconsistently reported in the primary studies, which restricted the ability to analyze outcomes across demographics or regional settings.""
  - ""the absence of risk of bias assessment tools, typically used in systematic reviews of clinical or experimental research, limits the interpretability of the included studies' internal validity.""
  - ""the lack of metaanalysis due to methodological heterogeneity across studies reduces the statistical precision of outcome syntheses, particularly concerning the comparative effectiveness of interventions such as recommendation system designs or sentiment analysis methods.""
  - ""the findings of this bibliometric analysis reveal a multidimensional view of the digital transformation and land consolidation research landscape between 2020 and 2025. Key trends include a decline in publication growth but a rise in collaborative and interdisciplinary practices, geographic diversification of research contributions, and a thematic shift toward computational and user-centric approaches. However, challenges remain in enhancing citation impact, integrating ethical principles, and ensuring sustained research quality.""
  - ""the underrepresentation of ethics, data privacy, and inclusivity remains a concern and warrants immediate scholarly attention.""",,"- The paper discusses several technical challenges related to data quality and availability, such as the reliance on indexed data from specific databases, which may introduce bias.
- The inconsistent reporting of participant-level data restricts analysis across demographics and regions, indicating a data quality issue.
- The absence of risk of bias assessment tools limits the interpretability of study internal validity, suggesting a methodological limitation.
- The lack of metaanalysis due to methodological heterogeneity reduces statistical precision, indicating a challenge in evaluating methodologies.
- The paper highlights challenges in enhancing citation impact and integrating ethical principles, which are related to evaluation methodology limitations.
- The underrepresentation of ethics and data privacy suggests integration difficulties and technical bottlenecks in addressing these areas.","  - ""Thematic evolution points to key areas of focus, including sentiment analysis, deep learning, and AI-driven personalization.""
  - ""The primary objective of this SLR is to synthesize and continuously update the state of scholarly knowledge on digital innovation from 2020 to 2025, with a specific emphasis on three interrelated domains: (1) technological innovation, including AI-driven personalization and recommendation systems; (2) user engagement, especially as shaped by sentiment analysis and behavioral adaptation; and (3) ethical design principles, particularly in relation to privacy concerns and algorithmic accountability.""
  - ""Dominant themes such as sentiment analysis, artificial intelligence-driven personalization, and ethical dimensions of recommender systems signal a paradigmatic shift toward user-centered design and affective computing, reflecting the field's progression from technical development to socially embedded innovation.""
  - ""The findings of this study reflects key dynamics in publication trends, citation impacts, authorship patterns, and thematic evolutions in the field of digital transformation and land consolidation research.""
  - ""The thematic evolution timeline (Figure 5 ) provides a temporal view of topic progression.

Between 2020 and 2023, themes like ""big data"" and ""data analytics"" dominated. From 2024 onward, focus shifted to ""consumer behavior,"" ""sentiment analysis,"" and ""deep learning,"" illustrating a thematic deepening into psychological and behavioral dimensions of digital transformation.""
  - ""Themes like ""social media"" and ""e-commerce"" were found to be central and well-developed, while others like ""user experience"" and ""recommender systems"" remained underdeveloped but potentially promising.""",,"- The paper discusses several innovative solutions in the context of digital transformation, particularly focusing on AI-driven personalization, sentiment analysis, and deep learning. These are highlighted as key areas of focus and thematic evolution.
- The emphasis on AI-driven personalization and recommendation systems suggests novel algorithmic contributions in these areas.
- Sentiment analysis and deep learning are mentioned as central themes, indicating their role in creative problem-solving methods and hybrid or ensemble approaches.
- The paper mentions the evolution of themes such as ""big data"" and ""data analytics"" to ""consumer behavior,"" ""sentiment analysis,"" and ""deep learning,"" which suggests a shift towards more advanced and innovative feature engineering techniques.
- The focus on ethical design principles and privacy concerns indicates innovative evaluation metrics and technical workarounds for known problems in these areas.
- The paper does not explicitly mention new algorithmic contributions, creative problem-solving methods, or novel feature engineering techniques beyond these themes. However, the emphasis on these areas suggests ongoing innovation and research.","  - ""The results indicated a total of 1015 publications across 268 sources, with an average document age of 2.24 years and mean citations per document of 10.47.""
  - ""The dataset reveals an average citation count of 10.47 per document, indicating moderate but uneven influence across the corpus.""
  - ""The year 2025 experienced the lowest average citation count (0.55), which, despite the recency bias, underscores a concerning decline in scholarly influence.""
  - ""The thematic map analysis (Figure 4 ) organizes research themes into quadrants based on relevance and development.""
  - ""The historical citation network (Figure 9 ) shows the intellectual structure of this field.""
  - ""The findings of this study reflects key dynamics in publication trends, citation impacts, authorship patterns, and thematic evolutions in the field of digital transformation and land consolidation research.""
  - ""The principal findings indicate a nuanced landscape: while there has been a contraction in publication output, the field simultaneously exhibits increased international collaboration and a marked rise in interdisciplinary integration.""
  - ""The growing influence of Asian institutions and the gradual inclusion of underrepresented regions signal an increasingly democratized research environment.""
  - ""the underrepresentation of ethics, data privacy, and inclusivity remains a concern and warrants immediate scholarly attention.""","  - ""(Page 7, Table 1) | Year   | MeanTCperArt | N  | MeanTCperYear | CitableYears |\n|--------|--------------|----|---------------|--------------|\n| (2020) | 28,84        | 37 | 4,81          | 6            |""
  - ""(Page 8, Table 1) | Author       | h_index | g_index | m_index | TC  | NP | PY_start |\n|--------------|---------|---------|---------|-----|----|----------|\n| Kumar S      | 5       | 6       | 0,833   | 175 | 6  | 2020     |""
  - ""(Page 8, Table 2) |      |       |    |       |   |\n|------|-------|----|-------|---|\n| 2021 | 20,77 | 35 | 4,15  | 5 |""","- The paper provides quantitative metrics such as the average document age and mean citations per document, which are relevant to the performance of publications in terms of scholarly influence.
- The average citation count per document is a measure of the impact of the publications, indicating moderate influence.
- The decline in average citation count in 2025 suggests a potential decrease in scholarly influence over time.
- The thematic map analysis and historical citation network provide insights into the development and structure of research themes, which can be considered qualitative performance outcomes.
- The paper discusses the increase in international collaboration and interdisciplinary integration, which are qualitative measures of performance in terms of research dynamics.
- The tables at the end of the paper likely contain specific data on publication trends and citation impacts, but without direct access to their content, we can only infer their relevance to performance metrics.","  - ""The studies were selected based on their relevance to themed as Artificial Intelligence personalization, sentiment analysis, recommendation systems, and digital ethics.""
  - ""Data collection was performed using two prominent and credible databases-Scopus and Web of Science.""
  - ""Publications were included based on the following criteria: articles written in English, published between January 2020 and March 2025, and directly relevant to themes such as digital commerce, recommendation systems, sentiment analysis, and ethical computing.""
  - ""The search strategy involved the use of Boolean operators to capture relevant permutations of keywords.""
  - ""Following the initial retrieval, a rigorous data cleaning and preprocessing phase was implemented.""
  - ""Bibliographic records were normalized to standardize keyword entries, author names, and institutional affiliations.""
  - ""Synonyms and variations in terminology were consolidated to improve coherence in the subsequent co-word and co-authorship analysis.""
  - ""Data extraction focused on key metadata fields: publication year, document type, author(s), country of affiliation, keywords, citation count, and references.""
  - ""The results indicated a total of 1015 publications across 268 sources, with an average document age of 2.24 years and mean citations per document of 10.47.""
  - ""A total of 1.015 publications from databases were systematically evaluated to assess their relevance to the research topic.""
  - ""Strict exclusion criteria were applied, including the removal of publications that were not relevant (n = 78), lacked DOI identifiers (n = 108), or were outside the research topic scope (n = 905).""
  - ""As a result, 186 documents (18.94%) were excluded from the review, while 796 documents (81.06%) were deemed eligible for further analysis.""",,"- The paper uses Scopus and Web of Science as the primary academic databases for data collection.
- The dataset consists of 1015 publications across 268 sources, with specific characteristics such as an average document age of 2.24 years and a mean citation count of 10.47 per document.
- The data collection method involved using Boolean operators to select relevant publications based on specific themes and criteria.
- The data preprocessing phase included normalizing bibliographic records and consolidating synonyms to improve coherence.
- The paper does not mention any specific training/validation/test data splits or external knowledge bases or ontologies used.","  - ""This studies adopted a living systematic review methodology, combining bibliometric and thematic analyses peer-reviewed publications drawn from Scopus and Web of Science.""
  - ""The studies were selected based on their relevance to themed as Artificial Intelligence personalization, sentiment analysis, recommendation systems, and digital ethics.""
  - ""The primary objective of this SLR is to synthesize and continuously update the state of scholarly knowledge on digital innovation from 2020 to 2025, with a specific emphasis on three interrelated domains: (1) technological innovation, including AI-driven personalization and recommendation systems; (2) user engagement, especially as shaped by sentiment analysis and behavioral adaptation; and (3) ethical design principles, particularly in relation to privacy concerns and algorithmic accountability.""
  - ""The review seeks to answer the following key questions: How has academic research evolved in response to technological and societal shifts between 2020 and 2025? What are the dominant themes, collaboration patterns, and bibliometric trends in this space? How do these trends inform the future development of responsible and user-centered digital technologies?""
  - ""The scope includes peer-reviewed publications indexed in major databases, with a focus on emerging themes such as recommender systems, sentiment analysis, and the privacy-personalization dynamic.""
  - ""The study emphasizes the need for frameworks that incorporate ethical design and cross-cultural adaptability.""
  - ""The analysis of publication output from 2020 to 2025 reveals a complex trajectory marked by an initial increase followed by a substantial decline.""
  - ""The findings of this study reflects key dynamics in publication trends, citation impacts, authorship patterns, and thematic evolutions in the field of digital transformation and land consolidation research.""
  - ""The principal findings indicate a nuanced landscape: while there has been a contraction in publication output, the field simultaneously exhibits increased international collaboration and a marked rise in interdisciplinary integration.""
  - ""Dominant themes such as sentiment analysis, artificial intelligence-driven personalization, and ethical dimensions of recommender systems signal a paradigmatic shift toward user-centered design and affective computing, reflecting the field's progression from technical development to socially embedded innovation.""
  - ""The growing influence of Asian institutions and the gradual inclusion of underrepresented regions signal an increasingly democratized research environment.""
  - ""By integrating bibliometric mapping with thematic and citation analyses, this review contributes to the existing body of knowledge by integrating bibliometric evidence with thematic and citation analysis, offering a panoramic view of evolving academic priorities.""
  - ""Future research should pursue longitudinal, cross-cultural studies that embed ethical frameworks into the development of intelligent systems.""","  - ""(Page 7, Table 1) | Year   | MeanTCperArt | N  | MeanTCperYear | CitableYears |\n|--------|--------------|----|---------------|--------------|\n| (2020) | 28,84        | 37 | 4,81          | 6            |""
  - ""(Page 8, Table 1) | Author       | h_index | g_index | m_index | TC  | NP | PY_start |\n|--------------|---------|---------|---------|-----|----|----------|\n| Kumar S      | 5       | 6       | 0,833   | 175 | 6  | 2020     |""
  - ""(Page 8, Table 2) |      |       |    |       |   |\n|------|-------|----|-------|---|\n| 2021 | 20,77 | 35 | 4,15  | 5 |""","- The paper focuses on a systematic review of digital transformation research, particularly in areas like AI personalization, sentiment analysis, and digital ethics. This indicates the target research disciplines or fields.
- The literature review tasks addressed include synthesizing knowledge on digital innovation, analyzing bibliometric trends, and identifying dominant themes.
- The types of academic documents processed are peer-reviewed publications from Scopus and Web of Science.
- The user types and requirements are not explicitly mentioned, but the focus on ethical design and cross-cultural adaptability suggests a need for frameworks that can be applied across diverse user groups.
- The integration with research workflows is implied through the use of bibliometric mapping and thematic analysis to understand evolving academic priorities.
- The application context is primarily academic, as it focuses on scholarly knowledge and research trends rather than commercial applications."
Improving Systematic Literature Review Based on Text Similarity Analysis,"Jingdong Jia, Xi Liu",10.1088/1742-6596/1069/1/012059,https://doi.org/10.1088/1742-6596/1069/1/012059,Journal of Physics: Conference Series,3,2018,"- Main AI/ML frameworks used: Text similarity analysis (string-based, corpus-based, knowledge-based)
- System architecture components: WordNet for synonyms and hypernyms, Word2Vec for word vectors
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Google Scholar
- Overall system design approach: Improving SLR through text similarity analysis for search string expansion and paper quality evaluation","- Algorithms and models used: WordNet for synonym and hypernym relations, Word2Vec for word vector generation
- Feature extraction techniques: Word2Vec for generating word vectors
- Data processing pipelines: Segmenting search strings into words, finding synonyms and hypernyms, combining them to form new search strings
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Expanding search strings using WordNet, evaluating paper quality using text similarity analysis with Word2Vec","- Search and retrieval algorithms: Expanding search strings using WordNet synonyms and hypernyms.
- Relevance ranking approaches: Calculating text similarity between research questions and paper content using Word2Vec.
- Content-based matching techniques: Using WordNet and Word2Vec for text similarity analysis.","- Subjective bias in selection process and data extraction
- Difficulty in finding all relevant articles
- Challenge of selecting and expanding suitable search strings
- Need for objective evaluation methods using text similarity analysis","- New algorithmic contributions: Use of WordNet to expand search strings based on synonyms and hypernyms.
- Creative problem-solving methods: Application of text similarity analysis to reduce subjective biases and improve paper search and selection.
- Hybrid or ensemble approaches: Combination of text similarity analysis with SLR to improve efficiency and quality.
- Novel feature engineering techniques: Use of Word2Vec to convert papers into word vector sets for calculating text similarity.
- Innovative evaluation metrics: Calculation of degree of similarity between papers and search questions for objective assessment.
- Technical workarounds for known problems: Addressing subjective biases and difficulty in finding relevant articles through text similarity analysis.","- Accuracy, precision, recall, F1-scores: Not mentioned
- Processing speed and efficiency metrics: Not mentioned
- Comparison with baseline methods: Not mentioned
- User satisfaction or usability results: Not mentioned
- System reliability and robustness measures: Not mentioned
- Scalability test results: Not mentioned","- External knowledge base: WordNet
- Data preprocessing: Word2Vec for generating word vectors
- Data source: Google Scholar
- No specific academic databases or dataset sizes mentioned","- Target research disciplines or fields: Software engineering
- Specific literature review tasks addressed: Expanding search strings using synonyms and hypernyms, assessing paper quality by calculating similarity
- Types of academic documents processed: Research papers
- User types and requirements: Researchers conducting SLRs
- Integration with research workflows: Improving the SLR process
- Commercial vs. academic applications: Academic","  - ""We propose that the synonym and hypernym relations of WordNet can be used to expand the search strings to improve the paper search process.""
  - ""In addition, through calculating the relevance of a paper and research issues according to text similarity analysis, the primary study selection can be improved.""
  - ""Text similarity is widely discussed in different fields. Its connotation is different due to the different application scenarios, so there is no unified and accepted definition.""
  - ""The methods of text similarity calculation are divided into four categories: string-based method, corpus-based method, knowledge-based method and other methods.""
  - ""The corpus-based approach uses the information obtained from the corpus to calculate text similarity. Using neural network models to generate word vectors is a method for the computation of text similarity, and the method has been studied in the field of natural language processing in recent years""
  - ""The method based on world knowledge refers to the use of a knowledge base with a normative organization system to calculate text similarity. The most commonly used ontologies are universal dictionaries, such as WordNet.""
  - ""We want to adopt an objective method to evaluate the quality of the paper efficiently. Comparing the textual similarity between the research questions and the content of the paper is a feasible method.""
  - ""In order to get word vectors, Word2Vec is a suitable tool, which takes a text corpus as input and produces the word vectors as output.""
  - ""Then, we use the Word2Vec to convert the selected paper into a word vector set, and then select each feature word in the search string as the input to calculate the mean of the distance as text similarity.""
  - ""The experiment was based on a systematic literature review paper of [9].""
  - ""We first choose ""software engineer surrounding factor"" as an example of a basic string. Then, their synonyms and hypernyms were extracted from WordNet,""
  - ""The new set of search strings can be obtained by splicing synonyms and hypernyms in the Table 1""
  - ""Finally, expanded search strings are shown in Table 2""
  - ""To verify the validity of our method, we first conducted a search by the original strings, and then searched by expanded strings in Google Scholar.""",,"- The paper discusses the use of text similarity analysis to improve the systematic literature review (SLR) process, particularly in expanding search strings and evaluating paper quality.
- The main AI/ML framework used is text similarity analysis, which is divided into four categories: string-based, corpus-based, knowledge-based, and other methods.
- The corpus-based approach uses neural network models like Word2Vec to generate word vectors, which is a key component of the system architecture.
- WordNet is used as a knowledge base for extracting synonyms and hypernyms, indicating its role in the system architecture.
- The paper does not mention specific databases, APIs, interfaces, or technical infrastructure like cloud platforms or computational resources.
- The integration with existing academic databases or platforms is implied through the use of Google Scholar for searching.
- The overall system design approach involves using text similarity analysis to improve the SLR process, specifically in search string expansion and paper quality evaluation.","  - ""We propose that the synonym and hypernym relations of WordNet can be used to expand the search strings to improve the paper search process.""
  - ""In addition, through calculating the relevance of a paper and research issues according to text similarity analysis, the primary study selection can be improved.""
  - ""Text similarity analysis can be used to expand the search strings; because a search string is a phrase including several words, the similarity analysis of phrase based on words is important to expand search strings.""
  - ""First, we get the basic search string that comes from the research questions, and segment it into words. Second, for each notional word, its synonyms and hypernyms in WordNet will be found to obtain new keywords. Next, new search strings will be gotten by combining these new keywords.""
  - ""The arithmetic of measuring semantic similarity between phrases based on WordNet has been put forward in literature, for example the study of [15].""
  - ""In order to get word vectors, Word2Vec is a suitable tool, which takes a text corpus as input and produces the word vectors as output.""
  - ""Then, we use the Word2Vec to convert the selected paper into a word vector set, and then select each feature word in the search string as the input to calculate the mean of the distance as text similarity.""
  - ""The result of this text similarity serves as a reference for the quality of the paper.""","  - ""(Page 5, Table 1) | Basic words         | Synonyms                                         | Hypernyms                              |\n|---------------------|--------------------------------------------------|----------------------------------------|\n| software engineer   | programmer, computer programmer, coder           | computer user, cracker, hacker, engineer|""
  - ""(Page 5, Table 2) | Rank | Search string                              |\n|------|--------------------------------------------|\n| 1    | software engineer encompassing factor      |""","- The paper uses WordNet to expand search strings by finding synonyms and hypernyms of the basic search string, which is a key implementation method.
- Text similarity analysis is used to calculate the relevance of papers to research issues, which involves measuring semantic similarity between phrases based on WordNet.
- Word2Vec is used to generate word vectors from text corpora, which is a feature extraction technique.
- The process involves segmenting search strings into words, finding synonyms and hypernyms, and combining them to form new search strings, which is part of the data processing pipeline.
- The paper does not mention specific training methodologies or preprocessing steps beyond using Word2Vec for word vector generation.
- The technical workflow involves expanding search strings using WordNet and evaluating paper quality using text similarity analysis with Word2Vec.","  - ""We propose that the synonym and hypernym relations of WordNet can be used to expand the search strings to improve the paper search process.""
  - ""In order to obtain suitable search strings, alternative words and synonyms are used (e.g. [7][8][9]), but synonyms are produced subjectively in these paper.""
  - ""Text similarity analysis can be used to expand the search strings; because a search string is a phrase including several words, the similarity analysis of phrase based on words is important to expand search strings.""
  - ""First, we get the basic search string that comes from the research questions, and segment it into words. Second, for each notional word, its synonyms and hypernyms in WordNet will be found to obtain new keywords. Next, new search strings will be gotten by combining these new keywords.""
  - ""Finally, by calculating the similarity between the new strings and the basic string, suitable new search strings with high similarity are selected.""
  - ""Comparing the textual similarity between the research questions and the content of the paper is a feasible method.""
  - ""In order to get word vectors, Word2Vec is a suitable tool, which takes a text corpus as input and produces the word vectors as output.""
  - ""Then, we use the Word2Vec to convert the selected paper into a word vector set, and then select each feature word in the search string as the input to calculate the mean of the distance as text similarity.""","  - ""(Page 5, Table 1) | Basic words         | Synonyms                                         | Hypernyms                              |\n|---------------------|--------------------------------------------------|----------------------------------------|\n| software engineer   | programmer, computer programmer, coder           | computer user, cracker, hacker, engineer|""
  - ""(Page 5, Table 2) | Rank | Search string                              |\n|------|--------------------------------------------|\n| 1    | software engineer encompassing factor      |""","- The paper discusses using text similarity analysis to improve the systematic literature review (SLR) process, particularly in expanding search strings and evaluating paper quality.
- The method involves using WordNet to find synonyms and hypernyms for search strings, which is a content-based matching technique.
- The paper uses Word2Vec to convert papers into word vector sets and calculate text similarity, which is a content-based matching technique.
- The process of expanding search strings and evaluating paper quality based on text similarity is a form of relevance ranking approach.
- The use of WordNet and Word2Vec for text similarity analysis is a form of content-based matching technique.","  - ""Although SLR is a commonly used method in many fields, it still has limitations. The most common limitations are the possible biases introduced in the selection process and inaccuracies in the data extraction""
  - ""In fact, selecting suitable search strings is important for this issue, and expanding search strings can find more results. However, how to expand search strings is a question.""
  - ""Another common limitation to SLR is the difficulty in finding all relevant articles.""
  - ""We want to adopt an objective method to evaluate the quality of the paper efficiently. Comparing the textual similarity between the research questions and the content of the paper is a feasible method.""
  - ""For further work, other similarity arithmetic can be used to compare the performance of improving a SLR. Additionally, other stages of a SLR can be considered to be improved.""",,"- The paper identifies several technical challenges related to Systematic Literature Review (SLR). One major challenge is the subjective bias in the selection process and data extraction, which is currently addressed through group decisions but remains subjective.
- Another challenge is the difficulty in finding all relevant articles, which is partly due to the time-consuming nature of search strategies like the ""snowball"" method.
- The paper highlights the challenge of selecting suitable search strings and expanding them to find more relevant results, which is a technical bottleneck in the SLR process.
- The authors propose using text similarity analysis to objectively evaluate paper quality, but this implies a need for further research on similarity arithmetic and its application in SLR.
- The paper does not explicitly mention algorithm performance issues, data quality or availability problems, scalability challenges, integration difficulties, evaluation methodology limitations, or computational resource constraints. However, it does focus on the challenges related to subjective bias, search string expansion, and the need for objective evaluation methods.","  - ""In this paper, in order to reduce subjective biases, we propose a different solution to improve objectively the quality of SLR, which aims to apply text similarity analysis into two stages of a SLR.""
  - ""We propose that the synonym and hypernym relations of WordNet can be used to expand the search strings to improve the paper search process.""
  - ""In addition, through calculating the relevance of a paper and research issues according to text similarity analysis, the primary study selection can be improved.""
  - ""Text similarity analysis can be used to expand the search strings; because a search string is a phrase including several words, the similarity analysis of phrase based on words is important to expand search strings.""
  - ""Comparing the textual similarity between the research questions and the content of the paper is a feasible method.""
  - ""we use the Word2Vec to convert the selected paper into a word vector set, and then select each feature word in the search string as the input to calculate the mean of the distance as text similarity.""
  - ""In this paper, through applying text similarity analysis, we put forward an objective method including two aspects to improve the SLR.""
  - ""First, in order to overcome the limitation that some related papers are not found in a SLR, we propose to expand the search strings based on synonyms and hypernyms in WordNet so as to find sufficient search results.""
  - ""In addition, in order to improve the assessment process of a paper's quality, we propose to calculate the degree of similarity between the paper and the search questions.""","  - ""(Page 5, Table 1) | Basic words         | Synonyms                                         | Hypernyms                              |\n|---------------------|--------------------------------------------------|----------------------------------------|\n| software engineer   | programmer, computer programmer, coder           | computer user, cracker, hacker, engineer|""
  - ""(Page 5, Table 2) | Rank | Search string                              |\n|------|--------------------------------------------|\n| 1    | software engineer encompassing factor      |""","- The paper introduces a novel approach by applying text similarity analysis to improve the quality of systematic literature reviews (SLRs). This is a creative problem-solving method as it addresses the subjective biases and limitations in traditional SLR methods.
- The use of WordNet to expand search strings based on synonyms and hypernyms is a new algorithmic contribution. It helps in finding more relevant papers by broadening the search scope.
- The application of Word2Vec to convert papers into word vector sets for calculating text similarity is a novel feature engineering technique. It allows for objective assessment of paper quality by comparing textual similarity with research questions.
- The method of calculating the degree of similarity between papers and search questions is an innovative evaluation metric. It provides an objective measure for selecting primary studies.
- The combination of text similarity analysis with SLR is a hybrid approach that improves the efficiency and quality of the review process.
- The paper addresses known problems such as subjective biases and difficulty in finding all relevant articles by providing technical workarounds through text similarity analysis.","  - ""In this paper, through applying text similarity analysis, we put forward an objective method including two aspects to improve the SLR.""
  - ""First, in order to overcome the limitation that some related papers are not found in a SLR, we propose to expand the search strings based on synonyms and hypernyms in WordNet so as to find sufficient search results.""
  - ""In addition, in order to improve the assessment process of a paper's quality, we propose to calculate the degree of similarity between the paper and the search questions.""
  - ""Then, we conducted an experiment to verify our method. Our results show that the application of text similarity analysis method can improve some of the steps in the SLR, thus improve the efficiency and quality of SLR.""
  - ""To verify the validity of our method, we first conducted a search by the original strings, and then searched by expanded strings in Google Scholar. The results show that some articles that did not appear in the first search appeared at a later position, and are retrieved.""","  - ""(Page 5, Table 1) | Basic words         | Synonyms                                         | Hypernyms                              |\n|---------------------|--------------------------------------------------|----------------------------------------|\n| software engineer   | programmer, computer programmer, coder           | computer user, cracker, hacker, engineer|""
  - ""(Page 5, Table 2) | Rank | Search string                              |\n|------|--------------------------------------------|\n| 1    | software engineer encompassing factor      |""","- The paper discusses the use of text similarity analysis to improve the systematic literature review (SLR) process by expanding search strings and assessing paper quality.
- The method involves using synonyms and hypernyms from WordNet to expand search strings, which helps in finding more relevant papers.
- The paper mentions that the method improves the efficiency and quality of SLR by retrieving articles that were not found with original search strings.
- The experiment conducted in the paper involved searching with original and expanded strings in Google Scholar, with results showing that expanded strings retrieved additional relevant articles.
- The tables at the end of the paper provide examples of expanded search strings and their similarity to the original strings, but they do not provide specific quantitative performance metrics like accuracy, precision, recall, or F1-scores.
- There is no mention of processing speed and efficiency metrics, user satisfaction or usability results, system reliability and robustness measures, or scalability test results in the paper.","  - ""In this paper, in order to reduce subjective biases, we propose a different solution to improve objectively the quality of SLR, which aims to apply text similarity analysis into two stages of a SLR.""
  - ""We propose that the synonym and hypernym relations of WordNet can be used to expand the search strings to improve the paper search process.""
  - ""In order to get word vectors, Word2Vec is a suitable tool, which takes a text corpus as input and produces the word vectors as output.""
  - ""The experiment was based on a systematic literature review paper of [9].""
  - ""To verify the validity of our method, we first conducted a search by the original strings, and then searched by expanded strings in Google Scholar.""
  - ""in order to overcome the limitation that some related papers are not found in a SLR, we propose to expand the search strings based on synonyms and hypernyms in WordNet so as to find sufficient search results.""
  - ""In addition, in order to improve the assessment process of a paper's quality, we propose to calculate the degree of similarity between the paper and the search questions.""","  - ""(Page 5, Table 1) | Basic words         | Synonyms                                         | Hypernyms                              |\n|---------------------|--------------------------------------------------|----------------------------------------|\n| software engineer   | programmer, computer programmer, coder           | computer user, cracker, hacker, engineer|""
  - ""(Page 5, Table 2) | Rank | Search string                              |\n|------|--------------------------------------------|\n| 1    | software engineer encompassing factor      |""","- The paper discusses the use of text similarity analysis to improve the quality of systematic literature reviews (SLRs).
- WordNet is used as an external knowledge base to expand search strings by finding synonyms and hypernyms.
- Word2Vec is used to generate word vectors from text corpora, which is a data preprocessing step.
- The experiment uses Google Scholar as a data source for searching papers.
- The paper does not mention specific academic databases like Scopus or Web of Science, nor does it provide details on dataset sizes, characteristics, or training/validation/test data splits.
- The focus is on using text similarity analysis to improve search and quality assessment, rather than detailing specific data sources or preprocessing methods beyond WordNet and Word2Vec.","  - ""Systematic literature review (SLR) is an important method for identifying, evaluating, and summarizing a specific research subject.""
  - ""In software engineering field, many systematic review studies have adopted several methods to increase the reliability, for example, group decision and pilots search.""
  - ""In this paper, in order to reduce subjective biases, we propose a different solution to improve objectively the quality of SLR, which aims to apply text similarity analysis into two stages of a SLR.""
  - ""The specific steps of the SLR may differ due to the author's writing style, but the method of SLR is roughly divided into five stages.""
  - ""Selecting primary studies is to search papers in some databases based on the search strings that are set according to the research questions.""
  - ""In this paper, through applying text similarity analysis, we put forward an objective method including two aspects to improve the SLR.""
  - ""First, in order to overcome the limitation that some related papers are not found in a SLR, we propose to expand the search strings based on synonyms and hypernyms in WordNet so as to find sufficient search results.""
  - ""In addition, in order to improve the assessment process of a paper's quality, we propose to calculate the degree of similarity between the paper and the search questions.""
  - ""The experiment was based on a systematic literature review paper of [9]. This paper examines how individual decisions in software engineering are influenced by environmental factors.""
  - ""The results show that some articles that did not appear in the first search appeared at a later position, and are retrieved.""
  - ""Our results show that the application of text similarity analysis method can improve some of the steps in the SLR, thus improve the efficiency and quality of SLR.""
  - ""For further work, other similarity arithmetic can be used to compare the performance of improving a SLR.""
  - ""Additionally, other stages of a SLR can be considered to be improved.""","  - ""(Page 5, Table 1) | Basic words         | Synonyms                                         | Hypernyms                              |\n|---------------------|--------------------------------------------------|----------------------------------------|\n| software engineer   | programmer, computer programmer, coder           | computer user, cracker, hacker, engineer|""
  - ""(Page 5, Table 2) | Rank | Search string                              |\n|------|--------------------------------------------|\n| 1    | software engineer encompassing factor      |""","- The paper focuses on improving the systematic literature review (SLR) process, which is a key application context.
- The target research discipline or field is software engineering, as indicated by the mention of software engineering field and the use of a software engineering-related paper for the experiment.
- The specific literature review tasks addressed include expanding search strings using synonyms and hypernyms to find more relevant papers and assessing paper quality by calculating similarity between papers and search questions.
- The types of academic documents processed are research papers, as the method is applied to find and evaluate papers related to software engineering.
- The user types and requirements are not explicitly mentioned, but the method is designed to reduce subjective biases and improve efficiency and quality, suggesting it is for researchers conducting SLRs.
- Integration with research workflows is implied as the method is designed to improve the SLR process, which is a part of research workflows.
- The application is academic, as it is focused on improving the quality and efficiency of SLRs in academic research."
From Professional Search to Generative Deep Research Systems: How Can Expert Oversight Improve Search Outcomes?,Samy Ateia,10.1145/3726302.3730131,https://doi.org/10.1145/3726302.3730131,Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,0,2025,"- Main AI/ML frameworks used: Generative AI (GenAI) models, deep learning, NLP
- System architecture components: Integration with OpenAI and Google ""deep research"" products, interactive interfaces for user feedback
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: CLEF BioASQ, TREC BioGen, systematic review datasets
- Overall system design approach: Integration of structured expert feedback at multiple stages of the generative search process","- Algorithms and models used: Generative AI (GenAI) models
- Feature extraction techniques: Integration of structured expert feedback
- Data processing pipelines: Evaluation on biomedical Q&A datasets (CLEF BioASQ, TREC BioGen) and systematic review datasets
- Training methodologies: Human-in-the-loop feedback approach
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Integration of expert feedback at multiple stages (query formulation, relevance judgments, information extraction)","- Search and retrieval algorithms: Generative AI (GenAI) models
- Relevance ranking approaches: Interactive Information Retrieval (IIR) literature, structured expert feedback
- Citation analysis methods: Evaluation on biomedical Q&A datasets and systematic review datasets
- Content-based matching techniques: ""deep research"" products for comprehensive query generation and synthesis
- Metadata extraction and utilization: ""deep research"" products for generating queries and synthesizing information
- Recommendation system approaches: Interactive Information Retrieval (IIR) literature
- Expert/authority identification methods: Structured expert feedback for quality assurance","- Algorithm performance issues: Misaligned results due to minimized user involvement.
- Data quality or availability problems: Unsupported claims, inaccurate citations, and overreliance on generated content.
- Integration difficulties: Lack of expert input, transparency, and user control in current generative search tools.
- Technical bottlenecks or failure points: Need for structured expert feedback to improve result quality.","- Integration of structured expert feedback into generative search systems
- Human-in-the-loop feedback approach for query formulation, relevance judgments, and information extraction
- Shift from full automation to increased expert control and quality assurance
- Evaluation on biomedical Q&A datasets",Not mentioned (the abstract does not provide specific performance results or metrics),"- Datasets: CLEF BioASQ, TREC BioGen, systematic review datasets
- No information on academic databases, dataset sizes, characteristics, data collection methods, training/validation/test data splits, data preprocessing and cleaning approaches, or external knowledge bases/ontologies","- Target research disciplines or fields: Biomedical research, systematic reviews
- Specific literature review tasks addressed: Systematic reviews, biomedical question answering
- Types of academic documents processed: Large document sets, biomedical Q&A datasets (CLEF BioASQ, TREC BioGen), systematic review datasets
- User types and requirements: Professionals or experts requiring expert input, transparency, and user control
- Integration with research workflows: Structured expert feedback for query formulation, relevance judgments, and information extraction
- Commercial vs. academic applications: Academic context (systematic reviews, biomedical research)","  - ""Generative AI (GenAI) models are increasingly employed for professional search tasks, from enterprise copilots to systematic reviews.""
  - ""Recently introduced ''deep research'' products (OpenAI, Google), marketed as automated professional search agents, promise comprehensive query generation and synthesis from large document sets.""
  - ""Interactive Information Retrieval (IIR) literature has long emphasized the value of user interactions and iterative search refinement.""
  - ""The rise of generative search introduces innovative methods of accessing information through natural language interactions, which show promise in addressing complex information needs.""
  - ""Given these challenges, this work explores systematically integrating structured expert feedback at multiple stages of a generative search process, specifically query formulation, relevance judgments, and information extraction.""
  - ""Building on our earlier experiments that assessed the performance of GenAI models in a fully automated setting for biomedical question answering and introduced an interactive system for refining generated queries, we now seek to evaluate how a structured, human-in-the-loop feedback approach might improve retrieval and task effectiveness over fully automated pipelines.""
  - ""By evaluating our proposed framework on biomedical Q&A datasets such as CLEF BioASQ and TREC BioGen as well as systematic review datasets, we want to highlight the value of human guidance for professional generative search.""",,"- The abstract mentions the use of ""Generative AI (GenAI) models"" for professional search tasks, indicating that deep learning and NLP are likely main AI/ML frameworks used.
- The mention of ""deep research"" products from OpenAI and Google suggests integration with these platforms, which are known for their NLP capabilities.
- The emphasis on ""Interactive Information Retrieval (IIR)"" and ""natural language interactions"" implies a system architecture that includes interfaces for user interaction and feedback.
- The integration of ""structured expert feedback"" at multiple stages suggests a system component that involves human-in-the-loop feedback mechanisms.
- The evaluation on ""biomedical Q&A datasets such as CLEF BioASQ and TREC BioGen as well as systematic review datasets"" indicates integration with existing academic databases or platforms.
- The abstract does not provide specific details on technical infrastructure like cloud platforms or computational resources, nor does it specify the overall system design approach beyond the integration of expert feedback.","  - ""Generative AI (GenAI) models are increasingly employed for professional search tasks, from enterprise copilots to systematic reviews.""
  - ""Recently introduced ''deep research'' products (OpenAI, Google), marketed as automated professional search agents, promise comprehensive query generation and synthesis from large document sets.""
  - ""Interactive Information Retrieval (IIR) literature has long emphasized the value of user interactions and iterative search refinement.""
  - ""The rise of generative search introduces innovative methods of accessing information through natural language interactions, which show promise in addressing complex information needs.""
  - ""Given these challenges, this work explores systematically integrating structured expert feedback at multiple stages of a generative search process, specifically query formulation, relevance judgments, and information extraction.""
  - ""Building on our earlier experiments that assessed the performance of GenAI models in a fully automated setting for biomedical question answering and introduced an interactive system for refining generated queries, we now seek to evaluate how a structured, human-in-the-loop feedback approach might improve retrieval and task effectiveness over fully automated pipelines.""
  - ""By evaluating our proposed framework on biomedical Q&A datasets such as CLEF BioASQ and TREC BioGen as well as systematic review datasets, we want to highlight the value of human guidance for professional generative search.""",,"- The abstract mentions the use of ""Generative AI (GenAI) models"" for professional search tasks, indicating that these models are part of the implementation.
- The mention of ""deep research"" products suggests the use of advanced AI technologies, likely involving neural networks or similar architectures.
- The emphasis on ""Interactive Information Retrieval (IIR)"" and ""iterative search refinement"" implies a focus on user interaction and feedback loops, which could involve classification or clustering algorithms to refine search results.
- The integration of ""structured expert feedback"" at multiple stages suggests a human-in-the-loop approach, which may involve feature extraction techniques to incorporate expert input into the search process.
- The evaluation on ""biomedical Q&A datasets such as CLEF BioASQ and TREC BioGen as well as systematic review datasets"" indicates that these datasets are used for training and testing the proposed framework.
- The abstract does not provide specific technical details such as specific algorithms, feature extraction techniques, or preprocessing steps, but it implies a focus on integrating human feedback into generative search systems.","  - ""Generative AI (GenAI) models are increasingly employed for professional search tasks, from enterprise copilots to systematic reviews.""
  - ""Recently introduced ''deep research'' products (OpenAI, Google), marketed as automated professional search agents, promise comprehensive query generation and synthesis from large document sets.""
  - ""Professional search, especially in domains such as biomedical research and systematic reviews, necessitates expert input, transparency, and user control, requirements often not met by current generative search tools.""
  - ""Interactive Information Retrieval (IIR) literature has long emphasized the value of user interactions and iterative search refinement.""
  - ""The rise of generative search introduces innovative methods of accessing information through natural language interactions, which show promise in addressing complex information needs.""
  - ""professional search tasks differ substantially from general consumer searches, involving transparent criteria for relevance judgments and systematic data extraction processes.""
  - ""Given these challenges, this work explores systematically integrating structured expert feedback at multiple stages of a generative search process, specifically query formulation, relevance judgments, and information extraction.""
  - ""We aim to shift the focus from full automation toward increased expert control and quality assurance.""
  - ""Building on our earlier experiments that assessed the performance of GenAI models in a fully automated setting for biomedical question answering and introduced an interactive system for refining generated queries, we now seek to evaluate how a structured, human-in-the-loop feedback approach might improve retrieval and task effectiveness over fully automated pipelines.""",,"- The abstract discusses the use of Generative AI (GenAI) models for professional search tasks, which implies the use of search and retrieval algorithms.
- The mention of ""deep research"" products suggests the use of content-based matching techniques and metadata extraction and utilization to generate comprehensive queries and synthesize information.
- The emphasis on expert input and transparency indicates the importance of expert/authority identification methods in ensuring the quality of search outcomes.
- The reference to Interactive Information Retrieval (IIR) literature highlights the role of user interactions and iterative search refinement, which can be related to relevance ranking approaches and recommendation system approaches.
- The focus on integrating structured expert feedback at multiple stages of the search process suggests a method for improving relevance judgments and information extraction, which are critical for finding and matching academic papers.
- The mention of evaluating the framework on biomedical Q&A datasets and systematic review datasets implies the use of citation analysis methods to assess the reliability and suitability of the search outcomes.","  - ""However, this drive toward automation risks minimizing user involvement, potentially leading to misaligned results and inappropriate reliance on AI-generated outputs.""
  - ""Professional search, especially in domains such as biomedical research and systematic reviews, necessitates expert input, transparency, and user control, requirements often not met by current generative search tools.""
  - ""Given these challenges, this work explores systematically integrating structured expert feedback at multiple stages of a generative search process, specifically query formulation, relevance judgments, and information extraction.""
  - ""Studies highlight issues such as unsupported claims, inaccurate citations, and overreliance on generated content, questioning the reliability and suitability of current generative search systems for high-stakes professional use.""
  - ""Despite early evidence suggesting increased productivity among knowledge workers using GenAI tools. these systems also exhibit substantial shortcomings.""",,"- The abstract mentions that the drive towards automation in generative AI models risks minimizing user involvement, which can lead to misaligned results and inappropriate reliance on AI-generated outputs. This suggests a technical challenge related to algorithm performance issues, as the models may not accurately align with user needs without sufficient human input.
- The need for expert input, transparency, and user control in professional search tasks is highlighted, indicating that current generative search tools lack these essential features. This points to integration difficulties and technical bottlenecks in incorporating human feedback into the systems.
- The abstract notes substantial shortcomings in current generative search systems, including unsupported claims, inaccurate citations, and overreliance on generated content. These issues are related to data quality or availability problems, as the systems may not provide reliable or accurate information.
- The focus on integrating structured expert feedback at multiple stages of the generative search process suggests that current systems lack effective evaluation methodologies and computational resource constraints, as they require human intervention to improve result quality.","  - ""Given these challenges, this work explores systematically integrating structured expert feedback at multiple stages of a generative search process, specifically query formulation, relevance judgments, and information extraction.""
  - ""The rise of generative search introduces innovative methods of accessing information through natural language interactions, which show promise in addressing complex information needs.""
  - ""Building on our earlier experiments that assessed the performance of GenAI models in a fully automated setting for biomedical question answering and introduced an interactive system for refining generated queries, we now seek to evaluate how a structured, human-in-the-loop feedback approach might improve retrieval and task effectiveness over fully automated pipelines.""
  - ""Interactive Information Retrieval (IIR) literature has long emphasized the value of user interactions and iterative search refinement.""
  - ""Recently introduced ''deep research'' products (OpenAI, Google), marketed as automated professional search agents, promise comprehensive query generation and synthesis from large document sets.""
  - ""We aim to shift the focus from full automation toward increased expert control and quality assurance.""",,"- The abstract discusses the integration of structured expert feedback into generative search systems, which is a novel approach to improve result quality and task performance. This is an innovative solution as it shifts the focus from full automation to increased expert control.
- The mention of ""systematically integrating structured expert feedback at multiple stages"" suggests a new algorithmic contribution by incorporating human feedback into the search process.
- The use of ""human-in-the-loop feedback approach"" is a creative problem-solving method that addresses the shortcomings of fully automated systems by involving human expertise.
- The abstract also mentions the evaluation of this framework on biomedical Q&A datasets, which could involve novel feature engineering techniques or innovative evaluation metrics to assess the effectiveness of the proposed system.
- The emphasis on ""increased expert control and quality assurance"" indicates a hybrid approach that combines AI with human oversight, which is an innovative system design element.","  - ""Despite early evidence suggesting increased productivity among knowledge workers using GenAI tools. these systems also exhibit substantial shortcomings.""
  - ""Studies highlight issues such as unsupported claims, inaccurate citations, and overreliance on generated content, questioning the reliability and suitability of current generative search systems for high-stakes professional use.""
  - ""Given these challenges, this work explores systematically integrating structured expert feedback at multiple stages of a generative search process, specifically query formulation, relevance judgments, and information extraction.""
  - ""Our key research questions are:RQ1: How can expert feedback be systematically integrated into GenAI-driven retrieval systems to improve result quality? RQ2: How does a feedback-driven generative search system compare to fully automated approaches in terms of retrieval effectiveness and task performance? RQ3: Does the integration of human expert feedback improve professional search outcomes compared to GenAI-simulated expert feedback?""
  - ""Building on our earlier experiments that assessed the performance of GenAI models in a fully automated setting for biomedical question answering and introduced an interactive system for refining generated queries, we now seek to evaluate how a structured, human-in-the-loop feedback approach might improve retrieval and task effectiveness over fully automated pipelines.""
  - ""By evaluating our proposed framework on biomedical Q&A datasets such as CLEF BioASQ and TREC BioGen as well as systematic review datasets, we want to highlight the value of human guidance for professional generative search.""",,"- The abstract discusses the shortcomings of current generative search systems, such as unsupported claims and inaccurate citations, which implies issues with accuracy and reliability.
- The research aims to integrate expert feedback to improve result quality, suggesting a focus on improving accuracy and precision.
- The abstract mentions evaluating the proposed framework on specific datasets, which could involve metrics like accuracy, precision, recall, and F1-scores, but these are not explicitly mentioned.
- There is no mention of processing speed, efficiency metrics, user satisfaction, system reliability, or scalability test results in the abstract.
- The abstract does not provide specific quantitative or qualitative performance outcomes, as it focuses on the research questions and objectives rather than results.","  - ""By evaluating our proposed framework on biomedical Q&A datasets such as CLEF BioASQ and TREC BioGen as well as systematic review datasets, we want to highlight the value of human guidance for professional generative search.""
  - ""we now seek to evaluate how a structured, human-in-the-loop feedback approach might improve retrieval and task effectiveness over fully automated pipelines.""",,"- The abstract mentions the use of ""biomedical Q&A datasets such as CLEF BioASQ and TREC BioGen"" as well as ""systematic review datasets."" These are the data sources used in the study.
- There is no mention of academic databases like Scopus or Web of Science, nor is there any information on dataset sizes, characteristics, data collection methods, training/validation/test data splits, or data preprocessing and cleaning approaches.
- The abstract does not mention any external knowledge bases or ontologies used in the study.
- The focus is on evaluating a framework using specific datasets, but detailed information about these datasets or other data sources is not provided.","  - ""Generative AI (GenAI) models are increasingly employed for professional search tasks, from enterprise copilots to systematic reviews.""
  - ""Professional search, especially in domains such as biomedical research and systematic reviews, necessitates expert input, transparency, and user control, requirements often not met by current generative search tools.""
  - ""The rise of generative search introduces innovative methods of accessing information through natural language interactions, which show promise in addressing complex information needs.""
  - ""professional search tasks differ substantially from general consumer searches, involving transparent criteria for relevance judgments and systematic data extraction processes.""
  - ""Studies highlight issues such as unsupported claims, inaccurate citations, and overreliance on generated content, questioning the reliability and suitability of current generative search systems for high-stakes professional use.""
  - ""Given these challenges, this work explores systematically integrating structured expert feedback at multiple stages of a generative search process, specifically query formulation, relevance judgments, and information extraction.""
  - ""Building on our earlier experiments that assessed the performance of GenAI models in a fully automated setting for biomedical question answering and introduced an interactive system for refining generated queries, we now seek to evaluate how a structured, human-in-the-loop feedback approach might improve retrieval and task effectiveness over fully automated pipelines.""
  - ""By evaluating our proposed framework on biomedical Q&A datasets such as CLEF BioASQ and TREC BioGen as well as systematic review datasets, we want to highlight the value of human guidance for professional generative search.""",,"- The abstract mentions the use of GenAI models for ""professional search tasks,"" which includes ""systematic reviews"" and ""biomedical research,"" indicating the target research disciplines or fields.
- The focus on ""systematic reviews"" and ""biomedical question answering"" suggests specific literature review tasks addressed.
- The abstract refers to processing ""large document sets"" and ""biomedical Q&A datasets such as CLEF BioASQ and TREC BioGen as well as systematic review datasets,"" indicating the types of academic documents processed.
- The need for ""expert input, transparency, and user control"" implies that the user types are professionals or experts in these fields, with requirements for reliable and accurate search outcomes.
- The integration of ""structured expert feedback"" into the search process suggests integration with research workflows, particularly in refining queries and ensuring relevance.
- The abstract does not explicitly mention commercial vs. academic applications, but the focus on systematic reviews and biomedical research suggests an academic context."
Towards semantic-driven boolean query formalization for biomedical systematic literature reviews,"Mohammadreza Pourreza, F. Ensan",10.1016/j.ijmedinf.2022.104928,https://doi.org/10.1016/j.ijmedinf.2022.104928,Int. J. Medical Informatics,5,2022,"- Main AI/ML frameworks used: Fine-tuning of pre-trained language models (deep learning, NLP)
- System architecture components: Integration with PubMed for dataset construction; use of UMLS for query expansion
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: PubMed, UMLS
- Overall system design approach: Automatic query generation and refinement","- Algorithms and models used: Pre-trained language models, Agglomerative clustering, Affinity Propagation, K-Means
- Feature extraction techniques: Generating high-level key-phrases and their dense embeddings
- Data processing pipelines: Construction and publication of a dataset consisting of PubMed articles' abstracts and keywords
- Training methodologies: Fine-tuning pre-trained language models
- Preprocessing steps: Integration of concepts from the Unified Medical Language System (UMLS) for query expansion and embedding generation","- Search and retrieval algorithms: Automatic query generation
- Content-based matching techniques: Use of pre-trained language models for generating high-level key-phrases and their dense embeddings
- Metadata extraction and utilization: Construction of a dataset with PubMed articles' abstracts and keywords; use of UMLS concepts for query expansion
- Relevance ranking approaches: Application of clustering methods (Agglomerative clustering, Affinity Propagation, K-Means)","- Algorithm performance issues: Existing models have low precision and recall.
- Data quality or availability problems: Not explicitly mentioned.
- Scalability challenges: Manual query construction is costly and time-consuming.
- Integration difficulties: Future work involves integrating with existing methodologies.
- Evaluation methodology limitations: Future work involves exploring iterative training processes.
- Computational resource constraints: Not explicitly mentioned.
- Technical bottlenecks or failure points: Not explicitly mentioned.","- New algorithmic contribution: Automatic query generation approach
- Creative problem-solving method: Use of pre-trained language models for key-phrase and dense embedding generation
- Novel feature engineering technique: Construction and publication of a large-scale dataset for fine-tuning language models
- Hybrid approach: Integration of UMLS concepts for query expansion and embedding generation
- Ensemble approach: Testing of different clustering methods (Agglomerative clustering, Affinity Propagation, and K-Means)
- Innovative evaluation metric: Outperforming existing state-of-the-art models in precision, recall, and F-measures","- Precision: 0.0821 (compared with 0.005)
- Recall: 0.9676 (compared with 0.878)
- F-measures: 0.2898 (compared with 0.0356 in F3 measure)
- Comparison with baseline methods: Proposed methods outperform existing state-of-the-art models.
- User satisfaction or usability results: Not mentioned
- System reliability and robustness measures: Not mentioned
- Scalability test results: Not mentioned","- Academic databases used: PubMed
- Dataset sizes and characteristics: Almost one million PubMed articles' abstracts and their keywords
- External knowledge bases or ontologies used: Unified Medical Language System (UMLS)","- Target research disciplines or fields: Biomedical
- Specific literature review tasks addressed: Study identification for systematic reviews
- Types of academic documents processed: PubMed articles' abstracts and keywords
- User types and requirements: Researchers conducting systematic reviews; reducing time and labor cost
- Integration with research workflows: Initial search query refinement by human reviewers
- Commercial vs. academic applications: Academic","  - ""The objective of this paper is to present an automatic query generation approach to reduce the time and labor cost of manual biomedical study identification.""
  - ""We use and fine-tune pre-trained language models for generating high-level key-phrases and their dense embeddings.""
  - ""We constructed and published a dataset consists of almost one million PubMed articles' abstracts and their keywords for fine-tuning pre-trained language models.""
  - ""We also use concepts that are represented in the Unified Medical Language System, UMLS, for query expansion and embedding generation.""
  - ""We exploit and test different clustering methods, namely Agglomerative clustering, Affinity Propagation, and K-Means, over the generated embeddings to form query clauses.""
  - ""The proposed model in this paper can be utilized to form an effective initial search query that can be further refined and updated by human reviewers for achieving the desired performance.""",,"- The main AI/ML framework used is the fine-tuning of pre-trained language models, which is a deep learning approach.
- The system architecture involves generating high-level key-phrases and their dense embeddings, indicating a focus on natural language processing (NLP).
- The dataset constructed from PubMed articles' abstracts and keywords suggests integration with existing academic databases, specifically PubMed.
- The use of concepts from the Unified Medical Language System (UMLS) indicates integration with existing medical language systems.
- The system design approach involves automatic query generation and refinement, which is a key component of the technical architecture.
- The abstract does not mention specific technical infrastructure such as cloud platforms or computational resources, nor does it detail specific system architecture components like databases or APIs beyond the integration with PubMed and UMLS.","  - ""We use and fine-tune pre-trained language models for generating high-level key-phrases and their dense embeddings.""
  - ""We constructed and published a dataset consists of almost one million PubMed articles' abstracts and their keywords for fine-tuning pre-trained language models.""
  - ""We also use concepts that are represented in the Unified Medical Language System, UMLS, for query expansion and embedding generation.""
  - ""We exploit and test different clustering methods, namely Agglomerative clustering, Affinity Propagation, and K-Means, over the generated embeddings to form query clauses.""
  - ""The objective of this paper is to present an automatic query generation approach to reduce the time and labor cost of manual biomedical study identification.""",,"- The abstract mentions the use of ""pre-trained language models"" for generating key-phrases and their dense embeddings, which indicates a feature extraction technique.
- The construction and publication of a dataset consisting of PubMed articles' abstracts and keywords suggest a data processing pipeline.
- The use of ""concepts that are represented in the Unified Medical Language System, UMLS, for query expansion and embedding generation"" implies a preprocessing step involving the integration of external knowledge systems.
- The mention of ""different clustering methods, namely Agglomerative clustering, Affinity Propagation, and K-Means"" indicates the use of clustering algorithms for forming query clauses.
- The fine-tuning of pre-trained language models suggests a training methodology.","  - ""The objective of this paper is to present an automatic query generation approach to reduce the time and labor cost of manual biomedical study identification.""
  - ""We also use concepts that are represented in the Unified Medical Language System, UMLS, for query expansion and embedding generation.""
  - ""We exploit and test different clustering methods, namely Agglomerative clustering, Affinity Propagation, and K-Means, over the generated embeddings to form query clauses.""
  - ""We use and fine-tune pre-trained language models for generating high-level key-phrases and their dense embeddings.""
  - ""We constructed and published a dataset consists of almost one million PubMed articles' abstracts and their keywords for fine-tuning pre-trained language models.""",,"- The paper focuses on ""automatic query generation"" which is a technique for finding and matching academic papers by generating effective search queries.
- The use of ""pre-trained language models"" for generating ""high-level key-phrases and their dense embeddings"" is a content-based matching technique, as it involves analyzing the content of papers to create relevant search terms.
- The construction of a dataset with ""almost one million PubMed articles' abstracts and their keywords"" is a form of metadata extraction and utilization, as it involves using metadata (keywords) to improve search queries.
- The use of ""concepts that are represented in the Unified Medical Language System, UMLS, for query expansion and embedding generation"" is a form of metadata extraction and utilization, as it involves using standardized medical concepts to enhance search queries.
- The application of ""different clustering methods, namely Agglomerative clustering, Affinity Propagation, and K-Means"" is a relevance ranking approach, as it involves grouping similar papers together based on their content.","  - ""Our proposed methods outperform existing state-of-the-art automatic query generation models across Precision (0.0821 compared with 0.005), Recall (0.9676 compared with 0.878), and F-measures (0.2898 compared with 0.0356 in F3 measure).""
  - ""For future work, we would like to explore the application of the presented query formalization methods in existing study identification methodologies and techniques, especially those that iteratively train machine learning models based on the domain experts' feedback on the relevancy of the retrieved studies.""
  - ""The objective of this paper is to present an automatic query generation approach to reduce the time and labor cost of manual biomedical study identification.""
  - ""Manual construction of queries, where a user submit a search query for which a biomedical search system such as PubMed would identify the most relevant documents, has been recognized as a very costly step in conducting systematic reviews.""",,"- The abstract mentions that manual construction of queries is a costly step, which implies a technical challenge related to efficiency and scalability.
- The objective of the paper is to reduce the time and labor cost, suggesting that current methods are not efficient or scalable.
- The comparison of performance metrics indicates that existing models have performance issues, such as low precision and recall, which are technical challenges.
- The future work section hints at integration difficulties with existing methodologies and techniques, as well as potential limitations in evaluation methodologies since they plan to explore applications in iterative training processes.","  - ""The objective of this paper is to present an automatic query generation approach to reduce the time and labor cost of manual biomedical study identification.""
  - ""We use and fine-tune pre-trained language models for generating high-level key-phrases and their dense embeddings.""
  - ""We constructed and published a dataset consists of almost one million PubMed articles' abstracts and their keywords for fine-tuning pre-trained language models.""
  - ""We also use concepts that are represented in the Unified Medical Language System, UMLS, for query expansion and embedding generation.""
  - ""We exploit and test different clustering methods, namely Agglomerative clustering, Affinity Propagation, and K-Means, over the generated embeddings to form query clauses.""
  - ""Our proposed methods outperform existing state-of-the-art automatic query generation models across Precision (0.0821 compared with 0.005), Recall (0.9676 compared with 0.878), and F-measures (0.2898 compared with 0.0356 in F3 measure).""
  - ""The proposed model in this paper can be utilized to form an effective initial search query that can be further refined and updated by human reviewers for achieving the desired performance.""",,"- The abstract presents an ""automatic query generation approach"" as a novel solution to reduce the time and labor cost associated with manual study identification. This is a new algorithmic contribution.
- The use of ""pre-trained language models"" for generating key-phrases and their dense embeddings is a creative problem-solving method, as it leverages existing models to improve query generation.
- The construction and publication of a dataset consisting of almost one million PubMed articles' abstracts and their keywords is a novel feature engineering technique, as it provides a large-scale dataset for fine-tuning language models.
- The integration of concepts from the Unified Medical Language System (UMLS) for query expansion and embedding generation is a hybrid approach, combining linguistic and medical knowledge.
- The testing of different clustering methods (Agglomerative clustering, Affinity Propagation, and K-Means) is an ensemble approach, as it evaluates multiple methods to form query clauses.
- The proposed methods outperform existing state-of-the-art models, indicating an innovative evaluation metric or technical workaround for known problems in query generation.","  - ""Our proposed methods outperform existing state-of-the-art automatic query generation models across Precision (0.0821 compared with 0.005), Recall (0.9676 compared with 0.878), and F-measures (0.2898 compared with 0.0356 in F3 measure).""
  - ""The proposed model in this paper can be utilized to form an effective initial search query that can be further refined and updated by human reviewers for achieving the desired performance.""
  - ""In addition, some of the proposed methods can even outperform the performance of the manually crafted queries in some specific measures.""
  - ""The objective of this paper is to present an automatic query generation approach to reduce the time and labor cost of manual biomedical study identification.""",,"- The abstract provides quantitative performance outcomes in terms of precision, recall, and F-measures, which are key metrics for evaluating the effectiveness of the proposed automatic query generation approach.
- The comparison with baseline methods is explicitly mentioned, as the proposed methods outperform existing state-of-the-art models in these metrics.
- The abstract does not mention processing speed and efficiency metrics, user satisfaction or usability results, system reliability and robustness measures, or scalability test results.
- The focus is on the performance of the query generation model in terms of precision, recall, and F-measures, which are directly relevant to the task of identifying and retrieving relevant documents for systematic reviews.","  - ""We also use concepts that are represented in the Unified Medical Language System, UMLS, for query expansion and embedding generation.""
  - ""We constructed and published a dataset consists of almost one million PubMed articles' abstracts and their keywords for fine-tuning pre-trained language models.""
  - ""The evaluation benchmark is the widely adopted CLEF 2018 Technology Assisted Reviews (TAR) collection, with 72 systematic reviews on Diagnosis Test Accuracy.""",,"- The abstract mentions the use of the ""CLEF 2018 Technology Assisted Reviews (TAR) collection"" as an evaluation benchmark. This indicates that this collection is a data source used in the study.
- A dataset of ""almost one million PubMed articles' abstracts and their keywords"" was constructed and published for fine-tuning pre-trained language models. This is a significant data source used in the study.
- The abstract also mentions the use of the ""Unified Medical Language System, UMLS"" for query expansion and embedding generation. This is an external knowledge base or ontology used in the study.
- There is no mention of other academic databases like Scopus or Web of Science, nor is there information on data collection methods, training/validation/test data splits, or data preprocessing and cleaning approaches.","  - ""Study identification refers to formalizing an effective search over biomedical databases for retrieving all eligible evidence for a systematic review.""
  - ""The objective of this paper is to present an automatic query generation approach to reduce the time and labor cost of manual biomedical study identification.""
  - ""For future work, we would like to explore the application of the presented query formalization methods in existing study identification methodologies and techniques, especially those that iteratively train machine learning models based on the domain experts' feedback on the relevancy of the retrieved studies.""
  - ""The proposed model in this paper can be utilized to form an effective initial search query that can be further refined and updated by human reviewers for achieving the desired performance.""
  - ""We constructed and published a dataset consists of almost one million PubMed articles' abstracts and their keywords for fine-tuning pre-trained language models.""
  - ""The evaluation benchmark is the widely adopted CLEF 2018 Technology Assisted Reviews (TAR) collection, with 72 systematic reviews on Diagnosis Test Accuracy.""",,"- The target research discipline or field is clearly biomedical, as indicated by the focus on ""biomedical databases"" and ""biomedical study identification.""
- The specific literature review task addressed is ""study identification,"" which involves formalizing effective searches for systematic reviews.
- The types of academic documents processed are PubMed articles' abstracts and their keywords, as part of the dataset used for fine-tuning language models.
- The user types and requirements include reducing the time and labor cost of manual query construction, which suggests the system is designed for researchers conducting systematic reviews.
- Integration with research workflows is implied by the potential for the model to be refined and updated by human reviewers, indicating a collaborative process.
- The application is academic, as it is focused on improving systematic review processes in biomedical research."
"Unleashing the Power of AI. A Systematic Review of Cutting-Edge Techniques in AI-Enhanced Scientometrics, Webometrics, and Bibliometrics","H. R. Saeidnia, Elaheh Hosseini, Shadi Abdoli, M. Ausloos",10.1108/LHT-10-2023-0514,https://doi.org/10.1108/LHT-10-2023-0514,Library hi tech,14,2024,"The paper discusses the use of AI techniques such as NLP, machine learning, and deep learning for enhancing scientometrics, webometrics, and bibliometrics. It integrates with existing academic databases like ProQuest, EBSCO, IEEE Explore, Web of Science, and Scopus. However, specific details on system architecture components, technical infrastructure, or overall system design approach are not mentioned.","- Algorithms and models used: Natural Language Processing (NLP), Machine Learning, Deep Learning
- Feature extraction techniques: NLP for extracting key information from scientific papers
- Data processing pipelines: Machine learning and deep learning for web data collection and analysis
- Training methodologies: Not explicitly mentioned, but implied through the use of recommender systems
- Preprocessing steps: Not explicitly mentioned, but likely involved in data collection and analysis
- Technical workflow or methodology: Web crawling, link analysis, content analysis, recommender systems","- Search and retrieval algorithms: Comprehensive search strategy across databases like ProQuest, IEEE Explore, EBSCO, Web of Science, and Scopus.
- Relevance ranking approaches: Use of keywords, variations, and synonyms to maximize coverage.
- Content-based matching techniques: Screening of titles and abstracts by independent reviewers.
- Metadata extraction and utilization: Development and application of a coding framework.
- Recommendation system approaches: Implied through the structured approach to identifying key papers.","- Lack of comprehensive understanding of cutting-edge AI techniques in scientometrics, webometrics, and bibliometrics (knowledge gaps and integration difficulties)
- Absence of a comprehensive overview and analysis of current AI-enhanced techniques (evaluation methodology limitations)
- Ethical considerations: data privacy, bias, transparency (algorithm performance issues and computational resource constraints)
- Need for regular monitoring and evaluation of AI systems to address biases and ethical concerns (computational resource constraints and technical bottlenecks)
- Exclusion of certain articles due to not searching in Google Scholar (data quality or availability problems)","- New algorithmic contributions: AI-based metrics for research impact prediction and knowledge mapping.
- Creative problem-solving methods: Automation of data collection, citation analysis, and author disambiguation.
- Hybrid or ensemble approaches: Integration of machine learning and natural language processing for web content analysis and text mining.
- Novel feature engineering techniques: Extraction of valuable information from scientific publications and web content.
- Innovative evaluation metrics: AI-based metrics considering factors beyond traditional citation counts.
- Technical workarounds for known problems: AI-assisted detection of scientific misconduct and plagiarism.
- Original system design elements: AI-powered recommender systems for discovering relevant scientific resources.","Not mentioned (the paper does not provide specific performance results such as accuracy, precision, recall, F1-scores, processing speed, comparison with baseline methods, user satisfaction, system reliability, or scalability test results)","- Academic databases used: ProQuest, IEEE Explore, EBSCO, Web of Science, Scopus
- Dataset size: 61 relevant articles
- Data collection methods: Comprehensive search strategy across databases
- Data collection period: January 1, 2000, to September 2022
- Study selection process: Two-step process involving title and abstract screening, followed by full-text analysis
- Quality assessment: CASP Systematic Review Checklist
- Coding framework: Used for systematic examination of research methodology, AI applications, metrics, ethical considerations, and future implications
- No specific data preprocessing or cleaning approaches mentioned
- No training/validation/test data splits mentioned
- No external knowledge bases or ontologies mentioned","- Target research disciplines or fields: Scientometrics, webometrics, and bibliometrics
- Specific literature review tasks addressed: Analyzing large volumes of scientific publications, extracting valuable information, mapping scientific knowledge landscapes, identifying emerging research trends
- Types of academic documents processed: Scientific publications, citations, web-based data
- User types and requirements: Researchers needing to analyze scholarly communication, identify research trends, evaluate publication impact
- Integration with research workflows: Automation of data collection and analysis to streamline research processes
- Commercial vs. academic applications: Primarily academic context","  - ""By conducting a systematic literature review, our aim is to explore the potential of AI in revolutionizing the methods used to measure and analyze scholarly communication, identify emerging research trends and evaluate the impact of scientific publications.""
  - ""First, by leveraging natural language processing (NLP) algorithms, machine learning techniques, and deep learning approaches, AI can extract key information from scientific papers from a scientometric perspective to gain a comprehensive understanding of research trends, collaborations, and impact within specific domains""
  - ""Next, in terms of webometrics, AI algorithms can collect data from various online sources through web scraping, including web pages, blogs, forums, and social media posts. Machine learning, data mining algorithms, and deep learning (DL) techniques can extract data and patterns to help researchers understand and predict online users' behaviors, and digital impact""
  - ""We have conducted searches in several databases including ProQuest (LISTA & IBSS), EBSCO (LISTA), IEEE Explore, Web of Science, and Scopus to identify relevant studies.""
  - ""The search strategy involved using keywords that were relevant to our research topic, including variations and synonyms to maximize coverage.""
  - ""Our study has involved conducting a thorough review of the existing literature to explore the various aspects and several indicators related to the use of AI-enhanced in Scientometrics, Webometrics, and Bibliometrics.""
  - ""The coding framework consisted of several key categories and criteria that guided the analysis process.""
  - ""The coding process allowed for a systematic examination of the research methodology, AI applications, metrics, ethical considerations, and future implications discussed in each article.""
  - ""The identification phase of our study involved conducting searches on various databases, such as ProQuest (LISTA & IBSS), EBSCO (LISTA), IEEE Explore, Web of Science, and Scopus.""
  - ""The study selection process consisted of two steps to identify articles that met our inclusion criteria.""
  - ""The quality assessment of the included reviews was conducted by two researchers (HR.S. and E.H.) using the CASP Systematic Review Checklist (Appendix 1).""",,"- The paper primarily focuses on the application of AI techniques such as natural language processing (NLP), machine learning, and deep learning to enhance scientometrics, webometrics, and bibliometrics. These techniques are used to extract key information from scientific papers and online sources, indicating the use of advanced AI/ML frameworks.
- The systematic literature review methodology suggests a structured approach to data collection and analysis, but specific system architecture components like databases, APIs, or interfaces are not explicitly detailed in the text.
- The paper mentions the use of several academic databases (ProQuest, EBSCO, IEEE Explore, Web of Science, and Scopus) for data collection, indicating integration with existing academic platforms.
- There is no explicit mention of technical infrastructure such as cloud platforms or computational resources in the provided text.
- The overall system design approach seems to be focused on leveraging AI to automate and enhance the analysis of scholarly communication, but specific technical architecture details are not provided.","  - ""By conducting a systematic literature review, our aim is to explore the potential of AI in revolutionizing the methods used to measure and analyze scholarly communication, identify emerging research trends and evaluate the impact of scientific publications.""
  - ""First, by leveraging natural language processing (NLP) algorithms, machine learning techniques, and deep learning approaches, AI can extract key information from scientific papers from a scientometric perspective to gain a comprehensive understanding of research trends, collaborations, and impact within specific domains""
  - ""Next, in terms of webometrics, AI algorithms can collect data from various online sources through web scraping, including web pages, blogs, forums, and social media posts. Machine learning, data mining algorithms, and deep learning (DL) techniques can extract data and patterns to help researchers understand and predict online users' behaviors, and digital impact""
  - ""AI algorithms can analyze large volumes of scientific publications and extract valuable information, such as author and co-author names, affiliations, keywords, and citations""
  - ""AI techniques can be employed to predict the impact of scientific research based on various factors, such as author reputation, journal quality, and citation patterns""
  - ""AI can map the scientific knowledge landscape by analyzing the relationships between different scientific papers, keywords, and concepts""
  - ""AI can provide several specific benefits including Web Crawling and Data Collection, Web Link Analysis, Web Content Analysis, Social Media Analysis, Web Impact Analysis, and Recommender Systems""
  - ""AI techniques, such as natural language processing and machine learning, can be employed to analyze the content of webpages and scientific publications available online""
  - ""AI can analyze social media platforms, such as Twitter, to understand the online discussions, trends, and interactions related to scientific research""
  - ""AI-powered recommender systems can assist researchers in discovering relevant scientific websites, online resources, and research collaborations""
  - ""AI algorithms can automatically collect bibliographic data from various sources, such as online databases, academic libraries, and digital repositories""
  - ""AI can analyze citation networks to identify influential papers, authors, and journals""
  - ""AI techniques can be employed to disambiguate authors with similar names, a common issue in bibliometrics""
  - ""AI algorithms can provide insights into research productivity, citation patterns, and research impact over time, assisting researchers and institutions in assessing research fame or performance.""
  - ""AI techniques, including natural language processing, can be utilized to analyze the textual content of research publications""","  - ""(Page 10, Table 1) | References                                                                 | Findings              | Main Points                                                                                           |\n|---------------------------------------------------------------------------|-----------------------|-------------------------------------------------------------------------------------------------------|\n| Donthu, Kumar et al. 2021, Rietz 2021, Caputo and Kargina 2022, Saeidnia, Kozak et al. 2023, Soleymani, Saeidnia et al. 2023 | *Publication Analysis* | AI can enhance the accuracy and efficiency of data collection and analysis in scientometrics.          |""
  - ""(Page 11, Table 1) | References                                                                 | Focus Area                      | AI Contribution                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------|---------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2019, Fazeli-Varzaneh, Ghorbi et al. 2021, Nicholson, Mordaunt et al. 2021, Caputo and Kargina 2022, Zhao and Feng 2022 | Analysis                        | patterns and analyze the impact of scientific publications.                                                                                                                                                     |""
  - ""(Page 14, Table 1) | References                                                                 | Findings                          | Main Points                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Serrano 2018, Brewer, Westlake et al. 2021, Khder 2021, Alaid, Roa’a et al. 2022 | *Web Crawling and Data Collection* | AI can automate the web crawling process, extracting data from websites and improving the efficiency of data collection for webometrics.                                                                    |""
  - ""(Page 16, Table 1) | References                                                                                                                                                                                                 | Findings                      | Main Points                                                                                                           |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|-----------------------------------------------------------------------------------------------------------------------|\n| Cox and Mazumdar, Donthu, Kumar et al. 2021, Rietz 2021, Caputo and Kargina 2022, Saeidnia, Kozak et al. 2023, Soleymani, Saeidnia et al. 2023                                                            | Automated Data Collection     | AI can enhance the accuracy and efficiency of data collection and analysis in bibliometrics.                          |""
  - ""(Page 17, Table 1) | References                                                                                           | Category                    | Description                                                                                                           |\n|------------------------------------------------------------------------------------------------------|-----------------------------|-----------------------------------------------------------------------------------------------------------------------|\n| Santamaría 2021, Rehs 2021, Abramo and D’Angelo 2023                                                 | *Disambiguation*            | disambiguate author names, improving the reliability of bibliometric analyses.                                        |""","- The paper discusses the use of AI in scientometrics, webometrics, and bibliometrics, focusing on techniques like natural language processing (NLP), machine learning, and deep learning.
- NLP is used to extract key information from scientific papers, indicating a feature extraction technique.
- Machine learning and deep learning are applied in webometrics for data collection and analysis, suggesting a data processing pipeline.
- AI algorithms are used for web crawling, link analysis, and content analysis, which are part of the technical workflow.
- The paper mentions the use of recommender systems, which likely involve training methodologies and preprocessing steps.
- The tables at the end of the paper likely contain specific references to studies that have implemented these techniques, but without content, we can infer they support the mentioned methods.","  - ""By conducting a systematic literature review, our aim is to explore the potential of AI in revolutionizing the methods used to measure and analyze scholarly communication, identify emerging research trends and evaluate the impact of scientific publications.""
  - ""we implemented a comprehensive search strategy across reputable databases such as ProQuest, IEEE Explore, EBSCO, Web of Science and Scopus.""
  - ""Our search encompassed articles published from January 1, 2000, to September 2022, resulting in a thorough review of 61 relevant articles.Findings(1)""
  - ""The study selection process consisted of two steps to identify articles that met our inclusion criteria. Initially, two independent reviewers (HR.S. and E.H.) screened the titles and abstracts of the identified articles to determine their relevance to our research question and inclusion criteria.""
  - ""The quality assessment of the included reviews was conducted by two researchers (HR.S. and E.H.) using the CASP Systematic Review Checklist (Appendix 1).""
  - ""a coding framework was developed and applied. The coding framework consisted of several key categories and criteria that guided the analysis process.""
  - ""The coding process allowed for a systematic examination of the research methodology, AI applications, metrics, ethical considerations, and future implications discussed in each article.""
  - ""The identification phase of our study involved conducting searches on various databases, such as ProQuest (LISTA & IBSS), EBSCO (LISTA), IEEE Explore, Web of Science, and Scopus.""
  - ""The search strategy involved using keywords that were relevant to our research topic, including variations and synonyms to maximize coverage.""
  - ""The study selection process consisted of two steps to identify articles that met our inclusion criteria.""
  - ""a coding framework was developed and applied.""",,"- The paper describes a systematic literature review process, which involves a comprehensive search strategy across multiple reputable databases such as ProQuest, IEEE Explore, EBSCO, Web of Science, and Scopus. This indicates the use of search and retrieval algorithms to find relevant academic papers.
- The search strategy includes the use of keywords, variations, and synonyms to maximize coverage, which suggests relevance ranking approaches to ensure the retrieval of pertinent articles.
- The study selection process involves screening titles and abstracts by independent reviewers to determine relevance, which aligns with content-based matching techniques.
- The paper mentions the use of a coding framework to systematically examine research methodologies, AI applications, and other criteria, which implies metadata extraction and utilization.
- The systematic review process and the use of a coding framework suggest a structured approach to identifying key papers, which could be related to recommendation system approaches.
- The paper does not explicitly mention citation analysis methods, expert/authority identification methods, or specific recommendation system approaches, but the systematic review process inherently involves some level of citation analysis to evaluate the impact of scientific publications.","  - ""While AI has shown great promise in improving the efficiency and accuracy of scientometric, webometric, and bibliometric analyses, there remains a lack of comprehensive understanding of the cutting-edge techniques and advancements in this rapidly evolving field.""
  - ""the problem at hand is the absence of a comprehensive overview and analysis of the current state-of-the-art AI-enhanced techniques in scientometrics, webometrics, and bibliometrics.""
  - ""This knowledge gap inhibits researchers and practitioners from fully capitalizing on the potential benefits and advancements offered by AI in these domains.""
  - ""By conducting a systematic review, we aim to address this gap and provide a comprehensive understanding of the state-of-the-art AI techniques, their applications, and their impact on the field of informetrics.""
  - ""the use of AI in these areas also raises important ethical considerations that must be carefully addressed.""
  - ""Data privacy and security, bias, and fairness, transparency and explainability, accountability and responsibility, informed consent, impact on employment and society, and continuous monitoring and evaluation are among the key ethical considerations that should be taken into account.""
  - ""Regular monitoring and evaluation of AI systems should be conducted to assess their performance, identify any biases or ethical concerns, and make necessary improvements.""
  - ""Addressing these ethical considerations requires a multidisciplinary approach involving researchers, policymakers, ethicists, and stakeholders from various fields.""
  - ""By not searching in Google Scholar, we aimed to minimize the number of overlapping studies. However, it is important to note that this highly technical approach may have resulted in overlooking certain articles, which could regretfully lead to our study excluding relevant information.""",,"- The paper highlights a lack of comprehensive understanding of cutting-edge AI techniques in scientometrics, webometrics, and bibliometrics, which is a technical challenge related to knowledge gaps and integration difficulties.
- The absence of a comprehensive overview and analysis of current AI-enhanced techniques is a technical challenge related to evaluation methodology limitations.
- Ethical considerations such as data privacy, bias, and transparency are technical challenges related to algorithm performance issues and computational resource constraints.
- The need for regular monitoring and evaluation of AI systems to address biases and ethical concerns is a technical challenge related to computational resource constraints and technical bottlenecks.
- The exclusion of certain articles due to not searching in Google Scholar is a technical challenge related to data quality or availability problems.","  - ""the application of AI yields various distinct advantages, such as conducting analyses of publications, citations, research impact prediction, collaboration, research trend analysis and knowledge mapping, in a more objective and reliable framework.""
  - ""AI algorithms are able to enhance web crawling and data collection, web link analysis, web content analysis, social media analysis, web impact analysis and recommender systems.""
  - ""automation of data collection, analysis of citations, disambiguation of authors, analysis of co-authorship networks, assessment of research impact, text mining and recommender systems are considered as the potential of AI integration in the field of bibliometrics.Originality/valueThis""
  - ""AI algorithms can analyze large volumes of scientific publications and extract valuable information, such as author and co-author names, affiliations, keywords, and citations""
  - ""AI techniques can be employed to predict the impact of scientific research based on various factors, such as author reputation, journal quality, and citation patterns""
  - ""AI can map the scientific knowledge landscape by analyzing the relationships between different scientific papers, keywords, and concepts""
  - ""AI techniques, such as natural language processing and machine learning, can be employed to analyze the content of webpages and scientific publications available online""
  - ""AI can analyze social media platforms, such as Twitter, to understand the online discussions, trends, and interactions related to scientific research""
  - ""AI-powered recommender systems can assist researchers in discovering relevant scientific websites, online resources, and research collaborations""
  - ""AI algorithms can automatically collect bibliographic data from various sources, such as online databases, academic libraries, and digital repositories""
  - ""AI techniques can be employed to disambiguate authors with similar names, a common issue in bibliometrics""
  - ""AI techniques can enhance research evaluation in bibliometrics. By considering various factors beyond traditional citation counts, such as social media mentions, downloads, and media coverage, AI algorithms can provide more comprehensive metrics for evaluating research impact.""","  - ""(Page 10, Table 1) | References                                                                 | Findings              | Main Points                                                                                           |\n|---------------------------------------------------------------------------|-----------------------|-------------------------------------------------------------------------------------------------------|\n| Donthu, Kumar et al. 2021, Rietz 2021, Caputo and Kargina 2022, Saeidnia, Kozak et al. 2023, Soleymani, Saeidnia et al. 2023 | *Publication Analysis* | AI can enhance the accuracy and efficiency of data collection and analysis in scientometrics.          |""
  - ""(Page 11, Table 1) | References                                                                 | Focus Area                      | AI Contribution                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------|---------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2019, Fazeli-Varzaneh, Ghorbi et al. 2021, Nicholson, Mordaunt et al. 2021, Caputo and Kargina 2022, Zhao and Feng 2022 | Analysis                        | patterns and analyze the impact of scientific publications.                                                                                                                                                     |""
  - ""(Page 14, Table 1) | References                                                                 | Findings                          | Main Points                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Serrano 2018, Brewer, Westlake et al. 2021, Khder 2021, Alaid, Roa’a et al. 2022 | *Web Crawling and Data Collection* | AI can automate the web crawling process, extracting data from websites and improving the efficiency of data collection for webometrics.                                                                    |""
  - ""(Page 16, Table 1) | References                                                                                                                                                                                                 | Findings                      | Main Points                                                                                                           |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|-----------------------------------------------------------------------------------------------------------------------|\n| Cox and Mazumdar, Donthu, Kumar et al. 2021, Rietz 2021, Caputo and Kargina 2022, Saeidnia, Kozak et al. 2023, Soleymani, Saeidnia et al. 2023                                                            | Automated Data Collection     | AI can enhance the accuracy and efficiency of data collection and analysis in bibliometrics.                          |""
  - ""(Page 17, Table 1) | References                                                                                           | Category                    | Description                                                                                                           |\n|------------------------------------------------------------------------------------------------------|-----------------------------|-----------------------------------------------------------------------------------------------------------------------|\n| Santamaría 2021, Rehs 2021, Abramo and D’Angelo 2023                                                 | *Disambiguation*            | disambiguate author names, improving the reliability of bibliometric analyses.                                        |""","- The paper discusses various innovative solutions in AI-enhanced scientometrics, webometrics, and bibliometrics. These include new algorithmic contributions such as AI-based metrics for research impact prediction and knowledge mapping.
- Creative problem-solving methods are highlighted through the use of AI for automating data collection, citation analysis, and author disambiguation.
- Hybrid or ensemble approaches are implied through the integration of AI techniques like machine learning and natural language processing for web content analysis and text mining.
- Novel feature engineering techniques are suggested by the use of AI to extract valuable information from scientific publications and web content.
- Innovative evaluation metrics are proposed through AI-based metrics that consider factors beyond traditional citation counts for research impact assessment.
- Technical workarounds for known problems are addressed through AI-assisted detection of scientific misconduct and plagiarism.
- Original system design elements are evident in the use of AI-powered recommender systems for discovering relevant scientific resources.","  - ""The main point is that AI algorithms can analyze large volumes of scientific publications and extract valuable information, such as author and co-author names, affiliations, keywords, and citations [21,22].""
  - ""AI algorithms can effectively identify citation patterns, analyze the impact of scientific publications, and predict research trends.""
  - ""AI techniques showcased in the studies can analyze scientific literature to identify emerging research areas and patterns of scientific collaborations""
  - ""AI-based metrics can provide more comprehensive and accurate measures of research impact""
  - ""AI algorithms can automate the process of gathering web data, such as web links, page content, and user behavior.""
  - ""AI algorithms can analyze web content to extract relevant information and identify trends in webometrics""
  - ""AI algorithms can provide insights into the online visibility, dissemination, and engagement of scientific publications, authors, and research institutions.""
  - ""AI-powered recommender systems can assist researchers in discovering relevant scientific websites, online resources, and research collaborations""
  - ""AI algorithms can automatically collect bibliographic data from various sources, such as online databases, academic libraries, and digital repositories""
  - ""AI algorithms can provide more comprehensive metrics for evaluating research impact.""
  - ""AI techniques can enhance research evaluation in bibliometrics.""","  - ""(Page 10, Table 1) | References                                                                 | Findings              | Main Points                                                                                           |\n|---------------------------------------------------------------------------|-----------------------|-------------------------------------------------------------------------------------------------------|\n| Donthu, Kumar et al. 2021, Rietz 2021, Caputo and Kargina 2022, Saeidnia, Kozak et al. 2023, Soleymani, Saeidnia et al. 2023 | *Publication Analysis* | AI can enhance the accuracy and efficiency of data collection and analysis in scientometrics.          |""
  - ""(Page 11, Table 1) | References                                                                 | Focus Area                      | AI Contribution                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------|---------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2019, Fazeli-Varzaneh, Ghorbi et al. 2021, Nicholson, Mordaunt et al. 2021, Caputo and Kargina 2022, Zhao and Feng 2022 | Analysis                        | patterns and analyze the impact of scientific publications.                                                                                                                                                     |""
  - ""(Page 14, Table 1) | References                                                                 | Findings                          | Main Points                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Serrano 2018, Brewer, Westlake et al. 2021, Khder 2021, Alaid, Roa’a et al. 2022 | *Web Crawling and Data Collection* | AI can automate the web crawling process, extracting data from websites and improving the efficiency of data collection for webometrics.                                                                    |""
  - ""(Page 16, Table 1) | References                                                                                                                                                                                                 | Findings                      | Main Points                                                                                                           |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|-----------------------------------------------------------------------------------------------------------------------|\n| Cox and Mazumdar, Donthu, Kumar et al. 2021, Rietz 2021, Caputo and Kargina 2022, Saeidnia, Kozak et al. 2023, Soleymani, Saeidnia et al. 2023                                                            | Automated Data Collection     | AI can enhance the accuracy and efficiency of data collection and analysis in bibliometrics.                          |""
  - ""(Page 17, Table 1) | References                                                                                           | Category                    | Description                                                                                                           |\n|------------------------------------------------------------------------------------------------------|-----------------------------|-----------------------------------------------------------------------------------------------------------------------|\n| Santamaría 2021, Rehs 2021, Abramo and D’Angelo 2023                                                 | *Disambiguation*            | disambiguate author names, improving the reliability of bibliometric analyses.                                        |""","- The paper discusses the use of AI in scientometrics, webometrics, and bibliometrics, highlighting its ability to analyze large volumes of data, extract valuable information, and provide insights into research trends and impact.
- It mentions the accuracy and efficiency of AI algorithms in data collection and analysis, but does not provide specific quantitative metrics such as accuracy, precision, recall, or F1-scores.
- The paper does not mention processing speed or efficiency metrics, nor does it compare AI performance with baseline methods.
- There is no mention of user satisfaction or usability results, nor are there any system reliability and robustness measures discussed.
- The paper does not include scalability test results or any specific performance metrics related to the technical implementation or challenges in the context of deep research or literature review.","  - ""we implemented a comprehensive search strategy across reputable databases such as ProQuest, IEEE Explore, EBSCO, Web of Science and Scopus.""
  - ""Our search encompassed articles published from January 1, 2000, to September 2022, resulting in a thorough review of 61 relevant articles.Findings(1)""
  - ""The identification phase of our study involved conducting searches on various databases, such as ProQuest (LISTA & IBSS), EBSCO (LISTA), IEEE Explore, Web of Science, and Scopus.""
  - ""The search was limited to articles published between January 1, 2000, and September 2022, in order to encompass the most recent literature related to our research objectives.""
  - ""The study selection process consisted of two steps to identify articles that met our inclusion criteria.""
  - ""The quality assessment of the included reviews was conducted by two researchers (HR.S. and E.H.) using the CASP Systematic Review Checklist (Appendix 1).""
  - ""The coding framework consisted of several key categories and criteria that guided the analysis process.""
  - ""The coding process allowed for a systematic examination of the research methodology, AI applications, metrics, ethical considerations, and future implications discussed in each article.""
  - ""The identification phase of our study involved conducting searches on various databases, such as ProQuest (LISTA & IBSS), EBSCO (LISTA), IEEE Explore, Web of Science, and Scopus. From these databases, a total of 1827 articles were initially found. After removing duplicate articles (962), we were left with 865 articles for further evaluation.""
  - ""By carefully assessing the exclusion criteria based on specific criteria, we excluded 356 articles, resulting in 509 articles for a more thorough analysis of their titles and abstracts.""
  - ""From a full-text analysis, we identified 61 of 121 articles that were relevant to our study's focus on AI in Scientometrics, Webometrics, and Bibliometrics.""",,"- The paper mentions the use of several academic databases for data collection, including ProQuest, IEEE Explore, EBSCO, Web of Science, and Scopus.
- The dataset size is indicated by the number of articles reviewed, which is 61 relevant articles.
- The data collection method involved a comprehensive search strategy across these databases, with a specific time frame from January 1, 2000, to September 2022.
- The study selection process involved two steps: screening titles and abstracts, and then full-text analysis to identify relevant articles.
- The quality assessment was conducted using the CASP Systematic Review Checklist.
- The coding framework was used to systematically examine various aspects of the articles, but there is no mention of specific data preprocessing or cleaning approaches.
- There is no mention of training/validation/test data splits or external knowledge bases or ontologies used.","  - ""The study aims to analyze the synergy of artificial intelligence (AI), with scientometrics, webometrics and bibliometrics to unlock and to emphasize the potential of the applications and benefits of AI algorithms in these fields.Design/methodology/approachBy""
  - ""By conducting a systematic literature review, our aim is to explore the potential of AI in revolutionizing the methods used to measure and analyze scholarly communication, identify emerging research trends and evaluate the impact of scientific publications.""
  - ""The main point is that AI algorithms can analyze large volumes of scientific publications and extract valuable information, such as author and co-author names, affiliations, keywords, and citations""
  - ""AI can map the scientific knowledge landscape by analyzing the relationships between different scientific papers, keywords, and concepts""
  - ""In webometrics, AI can provide several specific benefits including Web Crawling and Data Collection, Web Link Analysis, Web Content Analysis, Social Media Analysis, Web Impact Analysis, and Recommender Systems""
  - ""In bibliometrics, AI can provide several specific benefits including Automated Data Collection, Citation Analysis, Author Disambiguation, Co-authorship Analysis, Research Impact Analysis, Text Mining, and Recommender Systems""
  - ""AI can analyze social media platforms, such as Twitter, to understand the online discussions, trends, and interactions related to scientific research""
  - ""AI-powered recommender systems can assist researchers in discovering relevant scientific websites, online resources, and research collaborations""
  - ""AI algorithms can automatically collect bibliographic data from various sources, such as online databases, academic libraries, and digital repositories""
  - ""AI can enhance bibliometrics by improving publication analysis, citation analysis, author disambiguation, predictive models, collaboration analysis, and research evaluation.""
  - ""AI presents an efficient and scalable approach to scientometrics, webometrics, and bibliometrics, enabling researchers to extract meaningful insights from vast and diverse sources of scientific information.""","  - ""(Page 10, Table 1) | References                                                                 | Findings              | Main Points                                                                                           |\n|---------------------------------------------------------------------------|-----------------------|-------------------------------------------------------------------------------------------------------|\n| Donthu, Kumar et al. 2021, Rietz 2021, Caputo and Kargina 2022, Saeidnia, Kozak et al. 2023, Soleymani, Saeidnia et al. 2023 | *Publication Analysis* | AI can enhance the accuracy and efficiency of data collection and analysis in scientometrics.          |""
  - ""(Page 11, Table 1) | References                                                                 | Focus Area                      | AI Contribution                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------|---------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2019, Fazeli-Varzaneh, Ghorbi et al. 2021, Nicholson, Mordaunt et al. 2021, Caputo and Kargina 2022, Zhao and Feng 2022 | Analysis                        | patterns and analyze the impact of scientific publications.                                                                                                                                                     |""
  - ""(Page 14, Table 1) | References                                                                 | Findings                          | Main Points                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Serrano 2018, Brewer, Westlake et al. 2021, Khder 2021, Alaid, Roa’a et al. 2022 | *Web Crawling and Data Collection* | AI can automate the web crawling process, extracting data from websites and improving the efficiency of data collection for webometrics.                                                                    |""
  - ""(Page 16, Table 1) | References                                                                                                                                                                                                 | Findings                      | Main Points                                                                                                           |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|-----------------------------------------------------------------------------------------------------------------------|\n| Cox and Mazumdar, Donthu, Kumar et al. 2021, Rietz 2021, Caputo and Kargina 2022, Saeidnia, Kozak et al. 2023, Soleymani, Saeidnia et al. 2023                                                            | Automated Data Collection     | AI can enhance the accuracy and efficiency of data collection and analysis in bibliometrics.                          |""
  - ""(Page 17, Table 1) | References                                                                                           | Category                    | Description                                                                                                           |\n|------------------------------------------------------------------------------------------------------|-----------------------------|-----------------------------------------------------------------------------------------------------------------------|\n| Santamaría 2021, Rehs 2021, Abramo and D’Angelo 2023                                                 | *Disambiguation*            | disambiguate author names, improving the reliability of bibliometric analyses.                                        |""","- The paper focuses on the application of AI in scientometrics, webometrics, and bibliometrics, which are the target research disciplines or fields.
- The specific literature review tasks addressed include analyzing large volumes of scientific publications, extracting valuable information, mapping scientific knowledge landscapes, and identifying emerging research trends.
- The types of academic documents processed include scientific publications, citations, and web-based data.
- The user types and requirements include researchers who need to analyze scholarly communication, identify research trends, and evaluate publication impact.
- Integration with research workflows is implied through the automation of data collection and analysis, which can streamline research processes.
- The paper does not explicitly distinguish between commercial and academic applications, but the focus on research disciplines suggests an academic context."
Identifying Key Issues in Integration of Autonomous Ships in Container Ports: A Machine-Learning-Based Systematic Literature Review,"Enna Hirata, Annette Skovsted Hansen",10.3390/logistics8010023,https://doi.org/10.3390/logistics8010023,Logistics,6,2024,"- Main AI/ML frameworks used: BERTopic (built on BERT) for NLP
- System architecture components: PDF to text conversion, tokenization, lemmatization, stop word removal
- Technical infrastructure: Pre-processing for BERTopic training, UMAP for dimensionality reduction, HDBSCAN for clustering
- Integration with existing academic databases: Scopus and Web of Science
- Overall system design approach: Machine learning-based systematic literature review using BERTopic","The implementation methods include using BERTopic for topic modeling, which involves tokenization, lemmatization, and stop word removal as preprocessing steps. The workflow includes transforming text into word embeddings, using UMAP for dimensionality reduction, HDBSCAN for clustering, and C-TF-IDF for feature extraction. Fine-tuning is done using MMR to refine topics. The process leverages NLP techniques and machine learning models to analyze a large corpus of literature.","- Search and retrieval algorithms: Machine learning method to extract main topics from over 2000 journal publications.
- Relevance ranking approaches: Use of HDBSCAN and C-TF-IDF for clustering and topic modeling.
- Citation analysis methods: Not explicitly mentioned, but implied through topic modeling.
- Content-based matching techniques: Use of BERTopic and NLP techniques for content analysis.
- Metadata extraction and utilization: Conversion of PDFs to text, tokenization, lemmatization, and stop word removal.
- Recommendation system approaches: Diversity score and number of keywords to return.
- Expert/authority identification methods: Not explicitly mentioned, but implied through topic relevance.","- Regulatory frameworks need to be established for safe operation.
- Cybersecurity measures are required to protect against hacking.
- Data governance procedures are necessary for data management.
- Technological challenges include developing AUVs and UAVs.
- Safety protocols and testing of wave prediction systems are needed.
- Integration difficulties with logistics and supply networks.
- Data quality or availability problems due to language limitations.
- Algorithm performance issues in categorization compared to human review.","- New algorithmic contributions: Use of BERTopic for topic modeling, novel method for training BERTopic using a matrix-based corpus.
- Creative problem-solving methods: Application of UMAP for dimensionality reduction, HDBSCAN for clustering.
- Hybrid or ensemble approaches: Combination of machine learning techniques with traditional methods.
- Novel feature engineering techniques: Modification of TF-IDF formula to create class-based TF-IDF.
- Innovative evaluation metrics: Use of maximal marginal relevance (MMR) technique.
- Technical workarounds for known problems: Use of UMAP and HDBSCAN to handle complex data structures.
- Original system design elements: Machine learning clustering to focus on broader themes such as technology, cyber-physical systems, and regulation.",Not mentioned (the paper does not provide specific quantitative performance metrics or detailed performance results),"- Academic databases used: Scopus, Web of Science
- Dataset size: Initially 2610 papers, filtered down to 2023 papers
- Data collection method: Keyword search using ""autonomous ship"" and ""port"" or ""shipping""
- Data preprocessing: Tokenization, lemmatization, removal of stop words
- Machine learning model: BERTopic built on BERT
- No specific training/validation/test data splits or external knowledge bases mentioned","- Target research disciplines or fields: Container shipping and port management
- Specific literature review tasks addressed: Extracting and summarizing main topics related to autonomous ships
- Types of academic documents processed: Journal publications indexed in WoS and Scopus
- User types and requirements: Researchers and industry stakeholders interested in autonomous shipping
- Integration with research workflows: Use of machine learning techniques to analyze large datasets
- Commercial vs. academic applications: Primarily academic","  - ""A machine learning method is used to extract the main topics from more than 2000 journal publications indexed in WoS and Scopus.""
  - ""The data are converted from PDF to text format and analyzed using machine-learning-based natural language processing (NLP) techniques, including BERTopic [4]""
  - ""Using machine learning, BERTopic is a topic modeling method. It is built on BERT, presented by Devlin et al. [34] in 2018, and is used as a fine-tuning strategy.""
  - ""The first step is called tokenization. Tokenization treats each word or punctuation mark in a sentence as a standalone unit.""
  - ""The second step is called lemmatization, which identifies the root form, or lemma, of each token.""
  - ""The third step is the removal of stop words, frequently used words in a language, such as pronouns, determiners, and conjunctions.""
  - ""The analysis process is presented in the following manner, as shown in Figure 2""
  - ""The study employs a dataset obtained from the Scopus and Web of Science scientific paper databases.""
  - ""To create the dataset, we conducted a search using the keywords ""autonomous ship"" and ""port"" or ""autonomous ship"" and ""shipping"" on 24 September 2023.""
  - ""the data are pre-processed in a way to prepare them for the training of the BERTopic model.""
  - ""Using machine learning, BERTopic is a topic modeling method.""
  - ""In this study, a novel method for training BERTopic using a matrix-based corpus is presented.""
  - ""The procedure of topic modeling consists of five steps, which are described in the following subsections.""
  - ""The first step of the model focuses on the transformation of the text corpus into numerical representations, which are called word embeddings.""
  - ""Uniform manifold approximation and projection (UMAP) is a dimensionality reduction technique.""
  - ""HDBSCAN, short for hierarchical density-based spatial clustering of applications with noise, is a clustering method""
  - ""C-TF-IDF stands for class-based term frequency-inverse document frequency.""
  - ""The modification of the traditional TF-IDF formula combines all documents related to the same topic into a single collective document.""
  - ""The experiment's results were obtained using the parameters outlined in Table 4""",,"- The paper uses a machine learning method to extract main topics from over 2000 journal publications, indicating the use of AI/ML frameworks.
- BERTopic, a topic modeling method built on BERT, is used for natural language processing (NLP), which is a key AI/ML framework.
- The system architecture involves converting PDFs to text format and analyzing them using NLP techniques, suggesting a focus on text processing.
- The data is sourced from Scopus and Web of Science databases, indicating integration with existing academic databases.
- The technical infrastructure involves pre-processing data for BERTopic training, using techniques like tokenization, lemmatization, and stop word removal.
- The use of UMAP for dimensionality reduction and HDBSCAN for clustering suggests a focus on data visualization and clustering.
- The fine-tuning phase uses the MMR technique, indicating a refinement process in the AI/ML model.
- The study does not explicitly mention cloud platforms or specific computational resources, but it does involve large-scale data processing.","  - ""A machine learning method is used to extract the main topics from more than 2000 journal publications indexed in WoS and Scopus.""
  - ""The data are converted from PDF to text format and analyzed using machine-learning-based natural language processing (NLP) techniques, including BERTopic [4]""
  - ""The analysis process is presented in the following manner, as shown in Figure 2""
  - ""The first step is called tokenization. Tokenization treats each word or punctuation mark in a sentence as a standalone unit.""
  - ""The second step is called lemmatization, which identifies the root form, or lemma, of each token.""
  - ""The third step is the removal of stop words, frequently used words in a language, such as pronouns, determiners, and conjunctions.""
  - ""Using machine learning, BERTopic is a topic modeling method. It is built on BERT, presented by Devlin et al. [34] in 2018, and is used as a fine-tuning strategy.""
  - ""In the fine-tuning phase, we use the maximal marginal relevance (MMR) [39] technique.""
  - ""The experiment's results were obtained using the parameters outlined in Table 4""
  - ""The first step of the model focuses on the transformation of the text corpus into numerical representations, which are called word embeddings.""
  - ""Uniform manifold approximation and projection (UMAP) is a dimensionality reduction technique.""
  - ""HDBSCAN, short for hierarchical density-based spatial clustering of applications with noise, is a clustering method""
  - ""C-TF-IDF stands for class-based term frequency-inverse document frequency.""
  - ""The modification of the traditional TF-IDF formula combines all documents related to the same topic into a single collective document.""
  - ""The TF-IDF equation multiplies a word's TF (term frequency) by its IDF (inverse document frequency),""
  - ""In machine learning, fine-tuning is a process of further training an existing model that has already been trained on a large dataset,""
  - ""The procedure of topic modeling consists of five steps, which are described in the following subsections.""
  - ""The first step is called tokenization.""
  - ""The second step is called lemmatization,""
  - ""The third step is the removal of stop words,""
  - ""Using machine learning, BERTopic is a topic modeling method.""
  - ""In 2020, Grootendorst [36] proposed a method that combines transformer models with traditional TF-IDF classification to create understandable and expressive clusters.""",,"- The paper uses a machine learning method to extract topics from over 2000 journal publications, specifically employing natural language processing (NLP) techniques.
- BERTopic, a topic modeling method built on BERT, is used for analyzing the text data. This involves fine-tuning strategies to leverage pre-trained models for specific tasks.
- The data processing pipeline includes converting PDFs to text, followed by tokenization, lemmatization, and removal of stop words to prepare the data for analysis.
- The methodology involves transforming text into numerical representations using word embeddings, which are essential for machine learning models to process text data.
- Dimensionality reduction is performed using Uniform Manifold Approximation and Projection (UMAP), which helps in visualizing and preserving the structure of high-dimensional data.
- Clustering is achieved through HDBSCAN, a density-based clustering algorithm that identifies clusters based on data density.
- The C-TF-IDF technique is used to highlight important words within clusters, modifying the traditional TF-IDF approach to focus on topic-specific attributes.
- Fine-tuning involves using Maximal Marginal Relevance (MMR) to refine topic representations and reduce redundancy.
- The technical workflow includes these preprocessing steps and the application of machine learning models to extract and analyze topics from the literature.","  - ""A machine learning method is used to extract the main topics from more than 2000 journal publications indexed in WoS and Scopus.""
  - ""The data are converted from PDF to text format and analyzed using machine-learning-based natural language processing (NLP) techniques, including BERTopic""
  - ""The study employs a dataset obtained from the Scopus and Web of Science scientific paper databases.""
  - ""we conducted a search using the keywords ""autonomous ship"" and ""port"" or ""autonomous ship"" and ""shipping"" on 24 September 2023.""
  - ""The analysis process is presented in the following manner, as shown in Figure 2""
  - ""The first step is called tokenization. Tokenization treats each word or punctuation mark in a sentence as a standalone unit.""
  - ""The second step is called lemmatization, which identifies the root form, or lemma, of each token.""
  - ""The third step is the removal of stop words, frequently used words in a language, such as pronouns, determiners, and conjunctions.""
  - ""Using machine learning, BERTopic is a topic modeling method.""
  - ""The BERTopic framework leverages HDBSCAN's ability to identify clusters that do not conform to conventional geometric shapes by focusing on regions of higher data density compared to the surrounding space.""
  - ""C-TF-IDF stands for class-based term frequency-inverse document frequency.""
  - ""The modification of the traditional TF-IDF formula combines all documents related to the same topic into a single collective document.""
  - ""The experiment's results were obtained using the parameters outlined in Table 4""
  - ""The diversity score falls between 0 and 1, where 0 indicates minimal diversity, and 1 represents maximum diversity.""
  - ""This parameter defines how many keywords or key phrases should be returned. 10""
  - ""The output is presented in Section 5.""
  - ""The data presented in this study are available on request from the corresponding author.""","  - ""(Page 10, Table 1) | Name             | Description                                                                                                                                                                                                                                                                                                                                 | Value                                                                                                                                                                  |\n|------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| embedding model  | The initial BERTopic model applied for fine-tuning.                                                                                                                                                                                                                                                                                         | all-MiniLM-L6-v2                                                                                                                                                       |""
  - ""(Page 10, Table 2) | Topic | Count | Name                                | Topic Label                          |\n|-------|-------|-------------------------------------|--------------------------------------|\n| -1    | 365   | -1_ship_control_fig_model           | -1_ship_control_fig                  |""","- The paper uses a machine learning method to extract main topics from a large dataset of journal publications, indicating the use of search and retrieval algorithms.
- The use of BERTopic and NLP techniques suggests content-based matching techniques for identifying relevant papers.
- The conversion of PDFs to text and the use of tokenization, lemmatization, and stop word removal indicate metadata extraction and utilization.
- The employment of HDBSCAN and C-TF-IDF suggests relevance ranking approaches and citation analysis methods.
- The use of a three-stage process for corpus construction and the parameters outlined in Table 4 indicate a structured approach to paper discovery.
- The mention of diversity score and the number of keywords to return suggests a recommendation system approach.
- The paper does not explicitly mention expert/authority identification methods, but the use of machine learning and topic modeling implies some level of authority identification through topic relevance.","  - ""The main benefits of autonomous ships in container ports can be summarized in three areas: (1) increased operational efficiency, which can lead to faster turnaround times and reduced congestion; (2) improved safety and reduced risk of accidents through advanced collision avoidance systems; (3) reduction of greenhouse gas emissions through operation on cleaner energy sources.""
  - ""Our study sheds light on critical issues such as technological challenges, cybersecurity vulnerabilities, data governance procedures, and the structural regulations necessary for a regulatory framework.""
  - ""Firstly, we argue for increased support for research and development of autonomous underwater vehicles (AUVs) and unmanned aerial vehicles (UAVs), rigorous safety protocols, mandatory testing of wave prediction systems, and the pursuit of global technology standards.""
  - ""Secondly, it is important to strengthen logistics and supply networks that are tailored to autonomous ships. Additionally, a comprehensive data management strategy should be created, and strict cybersecurity safeguards should be enforced.""
  - ""thirdly, we suggest implementing initiatives to mitigate the environmental footprint of autonomous ships, encouraging the formation of international agreements to assess broader social impacts, and refining legal frameworks to ensure clear accountability in the event of accidents and attacks involving these ships.""
  - ""The machine learning model's clusters focus on broader themes such as technology, cyber-physical systems, and regulation.""
  - ""the machine learning model's approach to categorization differs from human categorization, resulting in a different grouping of the literature.""
  - ""The limitation of this paper is that it focuses on the integration of autonomous ships with ports and only includes studies published in English.""",,"- The paper discusses several technical challenges related to the integration of autonomous ships into container ports. These include the need for regulatory frameworks, cybersecurity measures, and data governance procedures.
- The paper highlights the importance of technological advancements such as AUVs and UAVs, which suggests a focus on improving these technologies to overcome current limitations.
- The need for rigorous safety protocols and mandatory testing of wave prediction systems indicates challenges in ensuring safety and reliability.
- The paper mentions the importance of strengthening logistics and supply networks, which implies integration difficulties with existing systems.
- The limitation of the paper focusing only on English publications suggests a potential data quality or availability problem in terms of language diversity.
- The comparison between machine learning and human categorization suggests potential algorithm performance issues or limitations in evaluation methodology.","  - ""This paper takes a systematic review approach to extract and summarize the main topics related to autonomous ships in the fields of container shipping and port management.""
  - ""A machine learning method is used to extract the main topics from more than 2000 journal publications indexed in WoS and Scopus.""
  - ""The data are converted from PDF to text format and analyzed using machine-learning-based natural language processing (NLP) techniques, including BERTopic""
  - ""Topic modeling is a machine learning technique that belongs to the category of unsupervised learning.""
  - ""In this study, a novel method for training BERTopic using a matrix-based corpus is presented.""
  - ""The procedure of topic modeling consists of five steps, which are described in the following subsections.""
  - ""The first step of the model focuses on the transformation of the text corpus into numerical representations, which are called word embeddings.""
  - ""Uniform manifold approximation and projection (UMAP) is a dimensionality reduction technique.""
  - ""HDBSCAN, short for hierarchical density-based spatial clustering of applications with noise, is a clustering method""
  - ""C-TF-IDF stands for class-based term frequency-inverse document frequency.""
  - ""The modification of the traditional TF-IDF formula combines all documents related to the same topic into a single collective document.""
  - ""The experiment's results were obtained using the parameters outlined in Table 4""
  - ""The machine learning model's clusters focus on broader themes such as technology, cyber-physical systems, and regulation.""",,"- The paper uses a machine learning method to extract main topics from a large number of publications, which is a novel approach in the context of systematic literature reviews.
- The use of BERTopic, a machine learning technique, is highlighted as a key method for analyzing the text data.
- The paper describes a novel method for training BERTopic using a matrix-based corpus, which is an innovative solution in the field of NLP.
- The application of UMAP for dimensionality reduction and HDBSCAN for clustering are technical workarounds for handling complex data structures.
- The modification of the traditional TF-IDF formula to create a class-based version is a novel feature engineering technique.
- The use of MMR in the fine-tuning phase is an innovative evaluation metric to reduce word redundancy.
- The paper's focus on broader themes such as technology, cyber-physical systems, and regulation through machine learning clustering is an original system design element.","  - ""Our study sheds light on critical issues such as technological challenges, cybersecurity vulnerabilities, data governance procedures, and the structural regulations necessary for a regulatory framework.""
  - ""The research findings highlight key issues related to technology, cybersecurity, data governance, regulations, and legal frameworks, providing a different perspective compared to human manual reviews of papers.""
  - ""Our research makes two contributions. First, it addresses the technical, regulatory, and legal frameworks required to govern the operation of autonomous ships in container ports. Second, it provides valuable insights into the evolving landscape of autonomous ships and their integration into container ports.""
  - ""The machine learning model's clusters focus on broader themes such as technology, cyber-physical systems, and regulation.""
  - ""the machine learning model's approach to categorization differs from human categorization, resulting in a different grouping of the literature.""
  - ""Our model achieved a C-v coherence score of 0.65, indicating its relatively high reliability.""
  - ""The research findings highlight key issues in technology, cybersecurity, data governance, regulation, and legal frameworks.""
  - ""The significance of this research is twofold. Firstly, it addresses the important technical, legal, and regulatory measures required for the management of autonomous ships in container ports. Second, it provides a critical perspective on the ever-evolving field of autonomous shipping.""
  - ""The limitation of this paper is that it focuses on the integration of autonomous ships with ports and only includes studies published in English.""",,"- The paper discusses the use of a machine learning model to extract and summarize main topics related to autonomous ships in container ports, but it does not provide specific quantitative performance metrics such as accuracy, precision, recall, or F1-scores.
- The paper mentions that the machine learning model provides a different perspective compared to human manual reviews, but it does not compare its performance to baseline methods.
- The paper does not mention processing speed or efficiency metrics, nor does it discuss user satisfaction or usability results.
- The paper does not provide system reliability and robustness measures beyond mentioning a C-v coherence score of 0.65, which indicates the model's reliability in terms of topic coherence.
- The paper does not include scalability test results or any specific quantitative performance outcomes.","  - ""The study employs a dataset obtained from the Scopus and Web of Science scientific paper databases.""
  - ""To create the dataset, we conducted a search using the keywords ""autonomous ship"" and ""port"" or ""autonomous ship"" and ""shipping"" on 24 September 2023.""
  - ""During the first stage, where the initial search is performed, 2610 papers were found. After filtering out irrelevant papers in the second stage, we obtained a total of 2023 papers for analysis.""
  - ""In the third stage, the text mining technique is applied to extract information from the papers in PDF format and convert it into structured data in JSON format.""
  - ""the data are pre-processed in a way to prepare them for the training of the BERTopic model. This pre-processing consists of three steps and employs the Python package gensim, which was chosen for its open source status, speed, and ability to handle large datasets.""
  - ""The first step is called tokenization. Tokenization treats each word or punctuation mark in a sentence as a standalone unit.""
  - ""The second step is called lemmatization, which identifies the root form, or lemma, of each token.""
  - ""The third step is the removal of stop words, frequently used words in a language, such as pronouns, determiners, and conjunctions.""
  - ""The analysis process is presented in the following manner, as shown in Figure 2 . Details of the input and machine learning model are discussed in Sections 4.2 and 4.3.""",,"- The paper uses Scopus and Web of Science as academic databases to collect data.
- The dataset size is initially 2610 papers, which is then filtered down to 2023 papers for analysis.
- The data collection method involves searching with specific keywords related to autonomous ships and ports.
- The data is preprocessed using tokenization, lemmatization, and removal of stop words to prepare it for machine learning analysis.
- The BERTopic model is used for topic modeling, which is built on BERT and involves fine-tuning.
- The paper does not mention specific training/validation/test data splits or external knowledge bases or ontologies used.","  - ""This paper takes a systematic review approach to extract and summarize the main topics related to autonomous ships in the fields of container shipping and port management.""
  - ""The research findings highlight key issues related to technology, cybersecurity, data governance, regulations, and legal frameworks, providing a different perspective compared to human manual reviews of papers.""
  - ""Our study sheds light on critical issues such as technological challenges, cybersecurity vulnerabilities, data governance procedures, and the structural regulations necessary for a regulatory framework.""
  - ""The study employs a dataset obtained from the Scopus and Web of Science scientific paper databases.""
  - ""The analysis process is presented in the following manner, as shown in Figure 2""
  - ""The research findings highlight key issues in technology, cybersecurity, data governance, regulation, and legal frameworks.""
  - ""The research results contribute to informed decision making and strategic planning, which are essential for the successful deployment of autonomous ship technology in the maritime industry.""
  - ""The data presented in this study are available on request from the corresponding author.""
  - ""The machine learning model's clusters focus on broader themes such as technology, cyber-physical systems, and regulation.""
  - ""the machine learning model's approach to categorization differs from human categorization, resulting in a different grouping of the literature.""",,"- The paper is focused on the integration of autonomous ships in container ports, which is the primary application context.
- The target research disciplines or fields include container shipping and port management.
- The specific literature review tasks addressed involve extracting and summarizing main topics related to autonomous ships.
- The types of academic documents processed are journal publications indexed in WoS and Scopus.
- The user types and requirements are not explicitly mentioned but can be inferred as researchers and industry stakeholders interested in autonomous shipping.
- The integration with research workflows is evident through the use of machine learning techniques to analyze large datasets.
- The application is primarily academic, as it involves a systematic review of literature to inform decision-making in the maritime industry."
Towards a multi-level framework for supporting systematic review — A pilot study,"Dingcheng Li, Zhen Wang, F. Shen, M. Murad, Hongfang Liu",10.1109/BIBM.2014.6999266,https://doi.org/10.1109/BIBM.2014.6999266,IEEE International Conference on Bioinformatics and Biomedicine,7,2014,"Not mentioned (the abstract does not provide specific details on the technical architecture, system components, or technical infrastructure)","- Algorithms and models used: Latent Dirichlet Allocations (LDAs) for topic analysis
- Feature extraction techniques: Not mentioned
- Data processing pipelines: Not mentioned
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Multi-level framework with ranking, topic analysis, and network analysis","- Relevance ranking approaches: Ranking with multiple metrics to assist the screening process.
- Content-based matching techniques: Topic analysis using Latent Dirichlet Allocations (LDAs) for discovering distributed semantics.","- Scalability challenges: Manual article screening is time-consuming.
- Algorithm performance issues: Use of a ""crude relevance ranking approach"" suggests room for improvement.","- New algorithmic contributions: Multi-level SR supporting framework with ranking using multiple metrics.
- Creative problem-solving methods: Reducing manual screening burdens by 25% while maintaining high sensitivity.
- Hybrid or ensemble approaches: Integration of multiple levels of analysis (ranking, topic analysis, network analysis).
- Novel feature engineering techniques: Topic analysis using Latent Dirichlet Allocations (LDAs).
- Innovative evaluation metrics: Sensitivity above 80% in case studies.
- Technical workarounds for known problems: Lowering screening burdens while maintaining validity.
- Original system design elements: Predicate relation network for semantic summarization.","- Sensitivity: above 80%
- Screening burden reduction: lowered to 25%
- Topic analysis consistency: high consistency among domain experts","Not mentioned (the abstract does not provide specific information on data sources, dataset sizes, data collection methods, data splits, or external knowledge bases)","- Target research disciplines or fields: Medicine
- Specific literature review tasks addressed: Manual article screening
- Types of academic documents processed: Articles
- User types and requirements: Researchers, librarians involved in systematic reviews
- Integration with research workflows: Assists screening process, reduces screening burdens
- Commercial vs. academic applications: Likely academic","  - ""Topic analysis based on Latent Dirichlet Allocations (LDAs) showed high consistency among the domain experts' selection of articles.""
  - ""The sensitivities in two case studies for relevance ranking reached above 80%, while the screening burdens were lowered to 25% based on a crude relevance ranking approach.""
  - ""we propose a multi-level SR supporting framework including three levels: Level 1 - ranking with multiple metrics aiming to assist the screening process by increasing the efficiency without compromising the validity; Level 2 - topic analysis for discovering distributed semantics; and Level 3 - network analysis on relation extracted for comprehensive semantic summarization.""
  - ""The predicate relation network summarized important predicate relations between medical concepts.""
  - ""A time-consuming step in conducting systematic review (SR) is the manual article screening from a list of potentially relevant articles retrieved by librarians.""",,"- The abstract mentions a ""multi-level SR supporting framework"" which suggests a structured approach to systematic review, but it does not explicitly detail the technical architecture or system components.
- The use of ""multiple metrics"" for ranking and ""Latent Dirichlet Allocations (LDAs)"" for topic analysis indicates the use of NLP techniques, but it does not specify any deep learning frameworks or other AI/ML frameworks.
- There is no mention of system architecture components such as databases, APIs, or interfaces.
- The abstract does not provide information on technical infrastructure like cloud platforms or computational resources.
- There is no mention of integration with existing academic databases or platforms.
- The overall system design approach is described as a multi-level framework, but the abstract lacks specific details on the technical architecture.","  - ""Topic analysis based on Latent Dirichlet Allocations (LDAs) showed high consistency among the domain experts' selection of articles.""
  - ""we propose a multi-level SR supporting framework including three levels: Level 1 - ranking with multiple metrics aiming to assist the screening process by increasing the efficiency without compromising the validity; Level 2 - topic analysis for discovering distributed semantics; and Level 3 - network analysis on relation extracted for comprehensive semantic summarization.""
  - ""The sensitivities in two case studies for relevance ranking reached above 80%, while the screening burdens were lowered to 25% based on a crude relevance ranking approach.""
  - ""The predicate relation network summarized important predicate relations between medical concepts.""
  - ""A time-consuming step in conducting systematic review (SR) is the manual article screening from a list of potentially relevant articles retrieved by librarians.""",,"- The abstract mentions a ""multi-level SR supporting framework"" which implies a structured approach to systematic review, but it does not specify the exact algorithms or models used beyond ""ranking with multiple metrics"" and ""Latent Dirichlet Allocations (LDAs)"" for topic analysis.
- The use of ""Latent Dirichlet Allocations (LDAs)"" is a specific technical implementation detail related to topic analysis, which is a form of unsupervised machine learning used for discovering topics in large collections of text.
- The abstract does not provide details on feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or the technical workflow beyond the general framework and the use of LDA.
- The mention of ""ranking with multiple metrics"" suggests some form of classification or ranking algorithm, but the specific algorithms or models are not detailed.
- The abstract does not mention neural networks, clustering, or other specific machine learning techniques beyond LDA.","  - ""we propose a multi-level SR supporting framework including three levels: Level 1 - ranking with multiple metrics aiming to assist the screening process by increasing the efficiency without compromising the validity; Level 2 - topic analysis for discovering distributed semantics; and Level 3 - network analysis on relation extracted for comprehensive semantic summarization.""
  - ""The sensitivities in two case studies for relevance ranking reached above 80%, while the screening burdens were lowered to 25% based on a crude relevance ranking approach.""
  - ""A time-consuming step in conducting systematic review (SR) is the manual article screening from a list of potentially relevant articles retrieved by librarians.""
  - ""The predicate relation network summarized important predicate relations between medical concepts.""
  - ""Topic analysis based on Latent Dirichlet Allocations (LDAs) showed high consistency among the domain experts' selection of articles.""",,"- The abstract mentions a ""multi-level SR supporting framework"" which includes three levels: ranking with multiple metrics, topic analysis, and network analysis. This suggests that the paper discusses methods for finding and matching academic papers.
- Level 1 involves ""ranking with multiple metrics,"" which is a relevance ranking approach. This is a technique used to find and match papers by ranking them based on relevance.
- Level 2 involves ""topic analysis for discovering distributed semantics,"" which is a content-based matching technique. This uses Latent Dirichlet Allocations (LDAs) to analyze topics, indicating a method for identifying relevant papers based on their content.
- Level 3 involves ""network analysis on relation extracted for comprehensive semantic summarization,"" which could be related to citation analysis methods or metadata extraction and utilization, as it involves analyzing relationships between concepts.
- The abstract does not explicitly mention search and retrieval algorithms, citation analysis methods, metadata extraction and utilization, recommendation system approaches, or expert/authority identification methods. However, the focus on relevance ranking and topic analysis provides insight into the paper discovery techniques discussed.","  - ""A time-consuming step in conducting systematic review (SR) is the manual article screening from a list of potentially relevant articles retrieved by librarians.""
  - ""Topic analysis based on Latent Dirichlet Allocations (LDAs) showed high consistency among the domain experts' selection of articles.""
  - ""The sensitivities in two case studies for relevance ranking reached above 80%, while the screening burdens were lowered to 25% based on a crude relevance ranking approach.""
  - ""The predicate relation network summarized important predicate relations between medical concepts.""",,"- The abstract mentions that manual article screening is a time-consuming step, which implies a technical challenge related to efficiency and scalability. This suggests a scalability challenge in terms of processing large volumes of articles manually.
- The mention of ""crude relevance ranking approach"" and the achievement of sensitivities above 80% indicates a potential algorithm performance issue, as the term ""crude"" suggests room for improvement in the ranking algorithm.
- The use of Latent Dirichlet Allocations (LDAs) for topic analysis and the predicate relation network for summarization indicates a focus on data quality and availability, as these methods are used to improve the consistency and relevance of article selection.
- The abstract does not explicitly mention data quality or availability problems, integration difficulties, evaluation methodology limitations, computational resource constraints, or technical bottlenecks or failure points. However, the focus on improving efficiency and relevance suggests underlying challenges in these areas.","  - ""we propose a multi-level SR supporting framework including three levels: Level 1 - ranking with multiple metrics aiming to assist the screening process by increasing the efficiency without compromising the validity; Level 2 - topic analysis for discovering distributed semantics; and Level 3 - network analysis on relation extracted for comprehensive semantic summarization.""
  - ""A time-consuming step in conducting systematic review (SR) is the manual article screening from a list of potentially relevant articles retrieved by librarians.""
  - ""The sensitivities in two case studies for relevance ranking reached above 80%, while the screening burdens were lowered to 25% based on a crude relevance ranking approach.""
  - ""The predicate relation network summarized important predicate relations between medical concepts.""
  - ""Topic analysis based on Latent Dirichlet Allocations (LDAs) showed high consistency among the domain experts' selection of articles.""",,"- The abstract describes a ""multi-level SR supporting framework"" which is a novel approach to support systematic reviews. This framework is innovative as it integrates multiple levels of analysis to improve the efficiency and validity of the review process.
- Level 1 involves ""ranking with multiple metrics,"" which is a new algorithmic contribution aimed at increasing efficiency without compromising validity. This is a creative problem-solving method as it addresses the time-consuming nature of manual article screening.
- Level 2 uses ""topic analysis for discovering distributed semantics"" based on Latent Dirichlet Allocations (LDAs), which is a novel feature engineering technique. This method shows high consistency with domain experts' selections, indicating its effectiveness.
- Level 3 involves ""network analysis on relation extracted for comprehensive semantic summarization,"" which is an original system design element. It provides a technical workaround for summarizing complex predicate relations between medical concepts.
- The framework's ability to lower screening burdens to 25% while maintaining high sensitivity is a technical workaround for known problems in systematic review processes.","  - ""The sensitivities in two case studies for relevance ranking reached above 80%, while the screening burdens were lowered to 25% based on a crude relevance ranking approach.""
  - ""The predicate relation network summarized important predicate relations between medical concepts.""
  - ""Topic analysis based on Latent Dirichlet Allocations (LDAs) showed high consistency among the domain experts' selection of articles.""",,"- The abstract provides some quantitative performance results, specifically mentioning that the sensitivities in two case studies for relevance ranking reached above 80%. This is a measure of recall, indicating how well the system identifies relevant articles.
- The screening burdens were lowered to 25%, which suggests an improvement in processing speed and efficiency metrics. This implies that the system is more efficient than manual screening.
- The abstract does not provide specific metrics like accuracy, precision, F1-scores, or direct comparisons with baseline methods. It does not mention user satisfaction, system reliability, robustness, or scalability test results.
- The mention of ""high consistency among the domain experts' selection of articles"" suggests a qualitative performance outcome related to the reliability of the topic analysis.
- The abstract does not provide detailed quantitative metrics or comparisons with baseline methods, nor does it mention user satisfaction or scalability test results.","  - ""The sensitivities in two case studies for relevance ranking reached above 80%, while the screening burdens were lowered to 25% based on a crude relevance ranking approach.""
  - ""Topic analysis based on Latent Dirichlet Allocations (LDAs) showed high consistency among the domain experts' selection of articles.""
  - ""The predicate relation network summarized important predicate relations between medical concepts.""
  - ""A time-consuming step in conducting systematic review (SR) is the manual article screening from a list of potentially relevant articles retrieved by librarians.""",,"- The abstract mentions that articles are ""retrieved by librarians,"" which suggests that the data sources are likely academic databases, but it does not specify which ones (e.g., Scopus, Web of Science).
- There is no mention of dataset sizes or characteristics, data collection methods, training/validation/test data splits, or data preprocessing and cleaning approaches.
- The abstract does not mention any external knowledge bases or ontologies used.
- The use of Latent Dirichlet Allocations (LDAs) for topic analysis and the predicate relation network for summarizing medical concepts suggests some form of data analysis, but it does not provide details on the data sources or preprocessing.","  - ""As part of the forefront of evidence-based medicine, a systematic review (SR) identifies, appraises, and synthesizes all the available literature relevant to a question of interest in a transparent and systematic way.""
  - ""A time-consuming step in conducting systematic review (SR) is the manual article screening from a list of potentially relevant articles retrieved by librarians.""
  - ""In this study, we propose a multi-level SR supporting framework including three levels: Level 1 - ranking with multiple metrics aiming to assist the screening process by increasing the efficiency without compromising the validity; Level 2 - topic analysis for discovering distributed semantics; and Level 3 - network analysis on relation extracted for comprehensive semantic summarization.""
  - ""The sensitivities in two case studies for relevance ranking reached above 80%, while the screening burdens were lowered to 25% based on a crude relevance ranking approach.""
  - ""Topic analysis based on Latent Dirichlet Allocations (LDAs) showed high consistency among the domain experts' selection of articles.""
  - ""The predicate relation network summarized important predicate relations between medical concepts.""",,"- The abstract mentions that the study is part of ""evidence-based medicine,"" indicating that the target research discipline or field is medicine.
- The specific literature review task addressed is the manual article screening process, which is a critical step in systematic reviews.
- The types of academic documents processed are likely articles, as the study involves screening and ranking articles.
- The user types and requirements are implied to be researchers or librarians involved in systematic reviews, as they are the ones who would benefit from increased efficiency and validity in the screening process.
- The integration with research workflows is suggested by the aim to assist the screening process and reduce screening burdens, indicating that the framework is designed to be integrated into existing systematic review workflows.
- The abstract does not explicitly mention whether the application is commercial or academic, but given the context of systematic reviews and evidence-based medicine, it is likely academic."
"The RapiD and SecuRe AI enhAnced DiaGnosis, Precision Medicine and Patient EmpOwerment Centered Decision Support System for Coronavirus PaNdemics Initial graph based upon existing knowledge",-,-,-,-,0,2021,"- Main AI/ML frameworks used: Knowledge Graph (KG)
- System architecture components: Knowledge graph database/repository
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Google Scholar, PubMed
- Overall system design approach: Focus on creating a knowledge graph for decision-making and drug development","- Algorithms and models used: Knowledge graph (KG)
- Feature extraction techniques: Not mentioned
- Data processing pipelines: Not mentioned
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Not mentioned","- Content-based matching techniques: Use of a knowledge graph (KG) to link multiple pieces of information across documents.
- Search and retrieval algorithms: Google Scholar is used for quick searches due to its superiority over PubMed in this regard.
- Metadata extraction and utilization: Not explicitly mentioned.
- Recommendation system approaches: Not explicitly mentioned.
- Expert/authority identification methods: Not explicitly mentioned.
- Citation analysis methods: Not explicitly mentioned.
- Relevance ranking approaches: Not explicitly mentioned.","- Data quality or availability problems: Searching through COVID-19 research literature is a formidable task.
- Algorithm performance issues: Simple keyword searches are insufficient for linking multiple pieces of information.
- Evaluation methodology limitations: Google Scholar's subjective human judgment affects repeatability and reliability.","- New algorithmic contributions: Use of a knowledge graph (KG) to link information across documents.
- Creative problem-solving methods: Facilitating rapid drug development through the knowledge graph.
- Technical workarounds for known problems: Using Google Scholar to access grey literature.
- Original system design elements: Knowledge graph for decision making and explainability.",Not mentioned (the abstract does not provide specific performance results or metrics),"- Academic databases used: Google Scholar
- Dataset sizes and characteristics: Not mentioned
- Data collection methods: Not mentioned
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: COVID-19 research, therapeutic development
- Specific literature review tasks addressed: Systematic review of existing knowledge for actionable clinical insights
- Types of academic documents processed: COVID-19 research literature from Google Scholar and PubMed
- User types and requirements: Researchers and clinicians seeking to improve patient care
- Integration with research workflows: Facilitating rapid drug development and decision-making
- Commercial vs. academic applications: Academic","  - ""an innovative AI technology called a knowledge graph (KG) could prove to be effective.""
  - ""A knowledge graph produced by systematic review of existing knowledge and the landscape of new therapeutic development that can be used to support decision making and the explainability of the analysis.""
  - ""This work demonstrates that a KG can facilitate rapid drug development.""
  - ""The superiority of Google Scholar over PubMed with respect to the ability to retrieve relevant articles using a quick search has been studied before.""
  - ""The advantages of PubMed over Google Scholar, which mainly stem from PubMed using human curation, are irrelevant for this review, because the sources identified by Google Scholar are perused by us before inclusion in the results.""",,"- The abstract mentions the use of a ""knowledge graph (KG)"" as an innovative AI technology, which suggests that the main AI/ML framework used is related to knowledge graph technology.
- The abstract does not specify any particular deep learning or NLP frameworks, but the use of a knowledge graph implies some form of semantic processing or network analysis.
- The system architecture components are not explicitly detailed, but the mention of a knowledge graph suggests a database or repository for storing and linking information.
- There is no mention of specific APIs, interfaces, or technical infrastructure such as cloud platforms or computational resources.
- The abstract discusses the integration with existing academic databases or platforms, specifically mentioning Google Scholar and PubMed, but does not detail how these are integrated into the system.
- The overall system design approach seems to focus on creating a knowledge graph to facilitate decision-making and drug development, but specific technical details are not provided.","  - ""an innovative AI technology called a knowledge graph (KG) could prove to be effective.""
  - ""A knowledge graph produced by systematic review of existing knowledge and the landscape of new therapeutic development that can be used to support decision making and the explainability of the analysis.""
  - ""This work demonstrates that a KG can facilitate rapid drug development.""
  - ""using Google Scholar allows access to a larger number of sources (sometimes referred to as ‘grey literature’).""
  - ""The superiority of Google Scholar over PubMed with respect to the ability to retrieve relevant articles using a quick search has been studied before.""",,"- The abstract mentions the use of a ""knowledge graph (KG)"" as an innovative AI technology, which is a key implementation method. Knowledge graphs are used for organizing and linking information across different sources.
- The abstract does not specify any particular algorithms or models like neural networks, clustering, or classification. It focuses on the concept of a knowledge graph rather than specific technical details.
- There is no mention of feature extraction techniques, data processing pipelines, training methodologies, or preprocessing steps in the abstract.
- The abstract does not provide a detailed technical workflow or methodology beyond the use of a knowledge graph.
- The use of Google Scholar is mentioned as a tool for retrieving relevant articles, but this is more about data collection rather than a specific implementation method.","  - ""Searching through the COVID-19 research literature to gain actionable clinical insight is a formidable task, even for experts.""
  - ""The usefulness of this corpus in terms of improving patient care is tied to the ability to see the big picture that emerges when the studies are seen in conjunction rather than in isolation.""
  - ""When the answer to a search query requires linking together multiple pieces of information across documents, simple keyword searches are insufficient.""
  - ""To answer such complex information needs, an innovative AI technology called a knowledge graph (KG) could prove to be effective.""
  - ""The superiority of Google Scholar over PubMed with respect to the ability to retrieve relevant articles using a quick search has been studied before.""
  - ""The advantages of PubMed over Google Scholar, which mainly stem from PubMed using human curation, are irrelevant for this review, because the sources identified by Google Scholar are perused by us before inclusion in the results.""
  - ""It would be unacceptable to use Google Scholar for a systematic review because the process must be repeatable, and human judgement used for quality evaluation is subjective (and thus not repeatable).""
  - ""However, for an exploratory review, this does not pose a problem, and using Google Scholar allows access to a larger number of sources (sometimes referred to as ‘grey literature’).""
  - ""An up-to-date comparison of these different search approaches from the perspective of a librarian can be found elsewhere.""",,"- The abstract discusses the challenge of searching through COVID-19 research literature, indicating the need for advanced techniques beyond simple keyword searches.
- It mentions the use of a knowledge graph (KG) as an innovative AI technology to link multiple pieces of information across documents, which is a content-based matching technique.
- The abstract compares Google Scholar and PubMed in terms of retrieving relevant articles, suggesting that Google Scholar is superior for quick searches but lacks the repeatability and human curation of PubMed.
- The use of Google Scholar for exploratory reviews is highlighted as beneficial for accessing a larger number of sources, including grey literature.
- The abstract does not explicitly mention other techniques like search and retrieval algorithms, relevance ranking approaches, citation analysis methods, metadata extraction and utilization, recommendation system approaches, or expert/authority identification methods.","  - ""Searching through the COVID-19 research literature to gain actionable clinical insight is a formidable task, even for experts.""
  - ""However, for an exploratory review, this does not pose a problem, and using Google Scholar allows access to a larger number of sources (sometimes referred to as ‘grey literature’).""
  - ""It would be unacceptable to use Google Scholar for a systematic review because the process must be repeatable, and human judgement used for quality evaluation is subjective (and thus not repeatable).""
  - ""When the answer to a search query requires linking together multiple pieces of information across documents, simple keyword searches are insufficient.""",,"- The abstract mentions that searching through COVID-19 research literature is a ""formidable task,"" which suggests a technical challenge related to data quality or availability problems. The sheer volume and complexity of the literature make it difficult to extract actionable insights.
- The limitation of simple keyword searches indicates an algorithm performance issue, as these searches are insufficient for linking multiple pieces of information across documents. This suggests a need for more sophisticated search algorithms.
- The mention of Google Scholar's limitations for systematic reviews due to the subjective nature of human judgment highlights an evaluation methodology limitation. This is a technical challenge because it affects the repeatability and reliability of the review process.
- The abstract does not explicitly mention scalability challenges, integration difficulties, computational resource constraints, or technical bottlenecks. However, the context implies that these could be potential challenges in developing and utilizing a knowledge graph for such a vast and complex dataset.","  - ""an innovative AI technology called a knowledge graph (KG) could prove to be effective.""
  - ""A knowledge graph produced by systematic review of existing knowledge and the landscape of new therapeutic development that can be used to support decision making and the explainability of the analysis.""
  - ""using Google Scholar allows access to a larger number of sources (sometimes referred to as ‘grey literature’).""
  - ""This work demonstrates that a KG can facilitate rapid drug development.""
  - ""The superiority of Google Scholar over PubMed with respect to the ability to retrieve relevant articles using a quick search has been studied before.""",,"- The abstract mentions the use of a ""knowledge graph (KG)"" as an innovative AI technology. This suggests a novel technical approach in organizing and linking information across multiple documents, which is a key feature in literature review and deep research.
- The use of a knowledge graph is highlighted as a method to support decision making and explainability, indicating its role in creative problem-solving and facilitating rapid drug development.
- The abstract also discusses the use of Google Scholar for accessing a broader range of sources, including grey literature, which can be considered a technical workaround for accessing more diverse information.
- The mention of facilitating rapid drug development through the knowledge graph suggests an original system design element focused on accelerating research processes.","  - ""A knowledge graph produced by systematic review of existing knowledge and the landscape of new therapeutic development that can be used to support decision making and the explainability of the analysis.""
  - ""The usefulness of this corpus in terms of improving patient care is tied to the ability to see the big picture that emerges when the studies are seen in conjunction rather than in isolation.""
  - ""A trial of baricitinib plus remdesivir has already been conducted and was superior to remdesivir alone in reducing recovery time and accelerating improvement in clinical status.""
  - ""This work demonstrates that a KG can facilitate rapid drug development.""
  - ""To answer such complex information needs, an innovative AI technology called a knowledge graph (KG) could prove to be effective.""",,"- The abstract discusses the use of a knowledge graph (KG) for systematic review and decision support, which implies a focus on information retrieval and analysis.
- The mention of ""facilitate rapid drug development"" suggests that the system is efficient in processing and analyzing data, but no specific metrics like accuracy, precision, or recall are provided.
- The abstract does not provide any quantitative performance metrics such as accuracy, precision, recall, F1-scores, or processing speed and efficiency metrics.
- There is no mention of comparison with baseline methods, user satisfaction, system reliability, robustness, or scalability test results.
- The abstract focuses more on the conceptual and practical application of the knowledge graph rather than providing specific performance results.","  - ""Searching through the COVID-19 research literature to gain actionable clinical insight is a formidable task, even for experts.""
  - ""The usefulness of this corpus in terms of improving patient care is tied to the ability to see the big picture that emerges when the studies are seen in conjunction rather than in isolation.""
  - ""The superiority of Google Scholar over PubMed with respect to the ability to retrieve relevant articles using a quick search has been studied before.""
  - ""The advantages of PubMed over Google Scholar, which mainly stem from PubMed using human curation, are irrelevant for this review, because the sources identified by Google Scholar are perused by us before inclusion in the results.""
  - ""It would be unacceptable to use Google Scholar for a systematic review because the process must be repeatable, and human judgement used for quality evaluation is subjective (and thus not repeatable).""
  - ""However, for an exploratory review, this does not pose a problem, and using Google Scholar allows access to a larger number of sources (sometimes referred to as ‘grey literature’).""
  - ""An up-to-date comparison of these different search approaches from the perspective of a librarian can be found elsewhere.""",,"- The abstract mentions the use of Google Scholar as a data source for retrieving relevant articles, particularly for an exploratory review.
- It highlights the advantage of Google Scholar in accessing a larger number of sources, including 'grey literature', which is not typically indexed in traditional academic databases like PubMed.
- The abstract does not specify any other academic databases such as Scopus or Web of Science.
- There is no mention of dataset sizes, characteristics, data collection methods, training/validation/test data splits, or data preprocessing and cleaning approaches.
- The abstract does not mention any external knowledge bases or ontologies used.","  - ""A knowledge graph produced by systematic review of existing knowledge and the landscape of new therapeutic development that can be used to support decision making and the explainability of the analysis.""
  - ""Searching through the COVID-19 research literature to gain actionable clinical insight is a formidable task, even for experts.""
  - ""The usefulness of this corpus in terms of improving patient care is tied to the ability to see the big picture that emerges when the studies are seen in conjunction rather than in isolation.""
  - ""To answer such complex information needs, an innovative AI technology called a knowledge graph (KG) could prove to be effective.""
  - ""The superiority of Google Scholar over PubMed with respect to the ability to retrieve relevant articles using a quick search has been studied before.""
  - ""The advantages of PubMed over Google Scholar, which mainly stem from PubMed using human curation, are irrelevant for this review, because the sources identified by Google Scholar are perused by us before inclusion in the results.""
  - ""This work demonstrates that a KG can facilitate rapid drug development.""
  - ""A trial of baricitinib plus remdesivir has already been conducted and was superior to remdesivir alone in reducing recovery time and accelerating improvement in clinical status.""",,"- The application context involves the use of a knowledge graph (KG) to support decision-making and analysis in the field of COVID-19 research, particularly in therapeutic development.
- The target research discipline is COVID-19 research, focusing on new therapeutic developments and clinical insights.
- The specific literature review task addressed is the systematic review of existing knowledge to gain actionable clinical insights.
- The types of academic documents processed include research literature on COVID-19, which may include articles from Google Scholar and PubMed.
- The user types and requirements likely include researchers and clinicians seeking to improve patient care by understanding the broader context of COVID-19 research.
- Integration with research workflows is implied by the use of a knowledge graph to facilitate rapid drug development and decision-making.
- The application is academic, as it involves research and development in the context of COVID-19 treatment."
Evolving Strategies in Machine Learning: A Systematic Review of Concept Drift Detection,"Gurgen Hovakimyan, J. Bravo",10.3390/info15120786,https://doi.org/10.3390/info15120786,Information,4,2024,"- Main AI/ML frameworks used: T5 (Text-to-Text Transfer Transformer) for natural language processing.
- System architecture components: Integration with IEEE and Science Direct databases through APIs and Python SDK.
- Technical infrastructure: Not explicitly mentioned, but implies use of computational resources for data processing and analysis.
- Integration with existing academic databases or platforms: IEEE and Science Direct.
- Overall system design approach: Structured review process with stages of identification, screening, eligibility, and inclusion; emphasis on methodological clarity, empirical validation, reproducibility, and practical relevance.","- Algorithms and models used: Drift detection mechanisms (DDMs), window-based mechanisms (WBMs), unsupervised and semi-supervised methods (USSMs), ensemble methods (EMs), neural networks (NNs)
- Feature extraction techniques: Clustering, density estimation
- Data processing pipelines: Window-based mechanisms for real-time drift detection
- Training methodologies: Ensemble methods, neural networks
- Preprocessing steps: Not explicitly detailed
- Technical workflow: Categorization of studies based on methods, evaluation of strengths and weaknesses","- Search and retrieval algorithms: Utilized APIs and Python SDK from IEEE and Science Direct.
- Relevance ranking approaches: Established strict inclusion and exclusion criteria.
- Citation analysis methods: Conducted citation analysis to identify influential articles.
- Content-based matching techniques: Used the T5 model for natural language processing.
- Metadata extraction and utilization: Evaluated quality and relevance of studies.","- Algorithm performance issues: Handling imbalanced data, computational efficiency, and application to non-tabular data.
- Data quality or availability problems: Limited focus on unsupervised methods, reliance on synthetic datasets.
- Scalability challenges: Computational intensity of neural networks, need for fast and scalable algorithms.
- Integration difficulties: Application to non-tabular data, integration with real-world complexities.
- Evaluation methodology limitations: Lack of standardized protocols for evaluation.
- Computational resource constraints: High computational demands of neural networks.
- Technical bottlenecks or failure points: Catastrophic forgetting in neural networks.","- Use of the T5 model for screening and data extraction.
- Hybrid approaches combining lightweight methods with adaptive techniques.
- Recommendations for hybrid frameworks, parallel computing, and hardware acceleration.
- Development of regression-specific drift detection metrics.
- Application of unsupervised feature extraction techniques and self-supervised learning.
- Establishment of a unified evaluation framework.","- Accuracy: High accuracy provided by neural networks and ensemble methods, but with high computational cost.
- Processing Speed and Efficiency Metrics: Emphasis on training time, prediction time, throughput, and latency.
- Comparison with Baseline Methods: Neural networks and ensemble methods offer high accuracy but are computationally expensive.
- User Satisfaction or Usability Results: Not mentioned.
- System Reliability and Robustness Measures: Not mentioned.
- Scalability Test Results: Not mentioned.","- Academic databases used: IEEE, Science Direct
- Dataset sizes and characteristics: Not explicitly mentioned
- Data collection methods: Utilized APIs and Python SDKs; snowballing through references
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: Machine learning and data science
- Specific literature review tasks addressed: Categorizing and evaluating existing concept drift detection methods
- Types of academic documents processed: Studies from various databases related to concept drift detection
- User types and requirements: Researchers and practitioners in machine learning and data science
- Integration with research workflows: Guiding future research and providing a resource for understanding concept drift mitigation strategies
- Commercial vs. academic applications: Primarily academic applications","  - ""This review employs an innovative method integrating the advanced natural language processing model T5 (Text-to-Text Transfer Transformer) to enhance the accuracy and efficiency of screening and data extraction processes.""
  - ""The initial research was conducted on two primary databases, IEEE and Science Direct, utilizing their inherent Application Programming Interfaces (APIs) and Python SDK.""
  - ""The methodological process was structured within four crucial stages: the identification, screening, eligibility, and inclusion of studies.""
  - ""The included studies varied in study design, drift types, data types, and methods.""
  - ""The studies were categorized based on the methods used for detecting concept drift.""
  - ""The integration of these theoretical frameworks into concept drift detection methods enhances their adaptability and effectiveness:""
  - ""The primary goal of this literature review is to illuminate the landscape of existing methods employed in detecting concept drift.""
  - ""This review comprehensively examines various concept drift detection methods developed over the last two decades.""
  - ""We also discuss the datasets, and their characteristics, widely used in concept drift detection.""
  - ""The included studies were diverse regarding the study design, drift types, and methods used for concept drift detection.""
  - ""The objective of the quality assessment was to evaluate methodological rigor and potential sources of bias in the included studies,""
  - ""The key criteria used for the assessment included methodological clarity, empirical validation, reproducibility, and practical relevance.""
  - ""The studies considered high quality (a score of 4-5) had solid foundations with clear problem statements, detailed and rigorous methodologies, robust validations, and high transparency.""",,"- The paper uses the T5 model for natural language processing to enhance screening and data extraction, indicating a reliance on deep learning techniques.
- The review process involves integration with existing academic databases (IEEE and Science Direct) through APIs and Python SDK, suggesting a structured system architecture.
- The paper discusses various concept drift detection methods, including statistical techniques, machine learning approaches, and deep learning methods, indicating a broad technical architecture.
- The review process is structured into stages of identification, screening, eligibility, and inclusion, suggesting a systematic approach to data collection and analysis.
- The paper evaluates the quality of studies based on methodological clarity, empirical validation, reproducibility, and practical relevance, indicating a focus on rigorous methodology.
- The integration of theoretical frameworks into concept drift detection methods suggests an emphasis on adaptability and effectiveness in the technical architecture.","  - ""This review employs an innovative method integrating the advanced natural language processing model T5 (Text-to-Text Transfer Transformer) to enhance the accuracy and efficiency of screening and data extraction processes.""
  - ""The primary goal of this literature review is to illuminate the landscape of existing methods employed in detecting concept drift.""
  - ""This review comprehensively examines various concept drift detection methods developed over the last two decades.""
  - ""We assess strategies for handling the concept drift in machine learning using high-impact publications from notable databases that were made accessible via the IEEE and Science Direct APIs.""
  - ""The studies were categorized based on the methods used for detecting concept drift.""
  - ""DDMs. The concept of drift detection was significantly advanced by the work of Gama et al. in 2004 with their introduction of the DDM""
  - ""WBMs. WBMs are pivotal in the detection of concept drift within data streams.""
  - ""USSMs: USSMs detect concept drift by leveraging clustering, density estimation, and other techniques to monitor changes in data distributions.""
  - ""EMs: EMs have become exceedingly popular for concept drift detection due to their ability to combine multiple models, improving predictive performance and robustness.""
  - ""NNs: Neural networks (NNs) have attracted significant attention in the field of concept drift detection because of their powerful learning capabilities and adaptability to changing data distributions.""
  - ""The theoretical foundations of NNs in drift detection are grounded in statistical learning and neural computation, emphasizing adaptability and generalization.""
  - ""The objective of the quality assessment was to evaluate methodological rigor and potential sources of bias in the included studies, ensuring reliable and valid findings.""
  - ""The key criteria used for the assessment included methodological clarity, empirical validation, reproducibility, and practical relevance.""
  - ""The studies considered high quality (a score of 4-5) had solid foundations with clear problem statements, detailed and rigorous methodologies, robust validations, and high transparency.""
  - ""The included studies varied in study design, drift types, data types, and methods.""
  - ""This review aims to offer a thorough and up-to-date synthesis of the methodologies and resources pertinent to concept drift detection by covering these areas.""
  - ""The findings are summarized in Table 4""
  - ""The included studies were diverse regarding the study design, drift types, and methods used for concept drift detection.""
  - ""Table 2 . Summary of the concept drift types.""",,"- The paper discusses various methods for detecting concept drift, including drift detection mechanisms (DDMs), window-based mechanisms (WBMs), unsupervised and semi-supervised methods (USSMs), ensemble methods (EMs), and neural networks (NNs).
- The paper highlights the use of algorithms such as statistical tests, control charts, clustering, density estimation, and neural networks for concept drift detection.
- The paper mentions the use of feature extraction techniques like clustering and density estimation in USSMs.
- The paper discusses data processing pipelines in terms of the use of window-based mechanisms for real-time drift detection.
- Training methodologies are implied through the discussion of ensemble methods and neural networks, which involve combining multiple models and adapting to changing data distributions.
- Preprocessing steps are not explicitly detailed but are implied through the discussion of data processing and feature extraction techniques.
- The technical workflow involves categorizing studies based on their methods and evaluating their strengths and weaknesses.","  - ""This review employs an innovative method integrating the advanced natural language processing model T5 (Text-to-Text Transfer Transformer) to enhance the accuracy and efficiency of screening and data extraction processes.""
  - ""The initial research was conducted on two primary databases, IEEE and Science Direct, utilizing their inherent Application Programming Interfaces (APIs) and Python SDK.""
  - ""Our search encompassed a comprehensive overview of all relevant topics, with search queries including terms such as the following: As we delved deeper into the literature during our review process, we found additional key papers in the references sections of various articles.""
  - ""To ensure the relevance and quality of the collected literature, we established strict inclusion and exclusion criteria.""
  - ""The eligibility assessment phase aimed to ensure that the included studies adhered strictly to the pre-established quality and relevance criteria.""
  - ""we conducted a citation analysis to identify the most influential articles on concept drift detection.""
  - ""The objective of the quality assessment was to evaluate methodological rigor and potential sources of bias in the included studies, ensuring reliable and valid findings.""",,"- The paper uses the T5 model for natural language processing to enhance screening and data extraction, which is a content-based matching technique.
- The search was conducted using APIs and Python SDK from IEEE and Science Direct, indicating a search and retrieval algorithm.
- The use of strict inclusion and exclusion criteria suggests a relevance ranking approach.
- Citation analysis was used to identify influential articles, which is a citation analysis method.
- The eligibility assessment phase involved evaluating the quality and relevance of studies, which includes metadata extraction and utilization.
- The paper does not explicitly mention recommendation system approaches or expert/authority identification methods.","  - ""However, challenges remain, particularly with imbalanced data, computational efficiency, and the application of concept drift detection to non-tabular data like images.""
  - ""Despite the extensive research and numerous advancements in the field, several weak points remain. One major limitation is the handling of imbalanced data, which continues to pose significant challenges for many algorithms.""
  - ""Another area that requires attention is the computational efficiency of concept drift detection methods. As the volume and velocity of data streams increase, the need for fast and scalable algorithms becomes more critical.""
  - ""The application of concept drift detection methods to non-tabular datasets, such as image and time-series data, remains underexplored.""
  - ""Another pressing issue is the limited focus on unsupervised drift detection methods, which are critical for scenarios where labeled data are scarce or unavailable.""
  - ""the majority of existing studies focus on synthetic and controlled datasets, which may not fully capture the complexities of real-world data streams.""
  - ""the evaluation of concept drift detection methods would benefit from the development of standardized protocols.""
  - ""While NNs excel in adaptability and accuracy, they also have notable disadvantages. NNs are computationally intensive, requiring significant resources for training and inference, which can limit their scalability in real-time applications.""
  - ""Additionally, they are prone to catastrophic forgetting, where the model's performance on previously seen data degrades as it learns new patterns.""
  - ""Recommendation: Investigate hybrid frameworks combining oversampling with adaptive cost-sensitive algorithms to improve performance on imbalanced datasets.""
  - ""Recommendation: Explore parallel computing and hardware acceleration, such as GPUoptimized neural networks, and investigate lightweight, low-latency detection models for deployment in real-time scenarios.""
  - ""Recommendation: Develop regression-specific drift detection metrics and algorithms that focus on subtle shifts in continuous relationships, such as changes in correlation structures or error variance.""
  - ""Recommendation: Investigate the application of unsupervised feature extraction techniques, such as autoencoders, combined with domainspecific neural architectures to handle non-tabular data effectively.""
  - ""Recommendation: Incorporate self-supervised learning techniques to generate pseudo-labels, enabling more effective drift detection in unlabeled datasets. Develop approaches that require less computational cost.""
  - ""Recommendation: Curate real-world benchmark datasets with annotations for known drift types, including contextual metadata, to improve the practical evaluation of drift detection methods.""
  - ""Recommendation: Establish a unified evaluation framework incorporating detection latency, scalability metrics, and resource usage alongside traditional accuracy measures to promote practical applicability.""",,"- The paper identifies several technical challenges in concept drift detection, including handling imbalanced data, computational efficiency, and applying methods to non-tabular data.
- Imbalanced data is a significant challenge as many algorithms struggle with class imbalance, affecting their performance.
- Computational efficiency is a concern due to the increasing volume and velocity of data streams, requiring fast and scalable algorithms.
- The application to non-tabular data like images and time-series data is underexplored, indicating a need for more research in these areas.
- The limited focus on unsupervised methods is a challenge, especially in scenarios with scarce labeled data.
- The reliance on synthetic datasets may not fully capture real-world complexities, suggesting a need for more diverse datasets.
- The evaluation of methods lacks standardized protocols, which could improve practical applicability.
- Neural networks face challenges such as computational intensity and catastrophic forgetting, affecting their scalability and performance.
- Recommendations are provided to address these challenges, including hybrid frameworks for imbalanced data, parallel computing for efficiency, and developing new metrics and algorithms for non-tabular data and regression tasks.","  - ""This review employs an innovative method integrating the advanced natural language processing model T5 (Text-to-Text Transfer Transformer) to enhance the accuracy and efficiency of screening and data extraction processes.""
  - ""The primary goal of this literature review is to illuminate the landscape of existing methods employed in detecting concept drift.""
  - ""We assess strategies for handling the concept drift in machine learning using high-impact publications from notable databases that were made accessible via the IEEE and Science Direct APIs.""
  - ""The integration of these theoretical frameworks into concept drift detection methods enhances their adaptability and effectiveness:""
  - ""Hybrid approaches combining lightweight methods (e.g., DDMs) with adaptive techniques (e.g., NNs) show promise in balancing computational efficiency with detection accuracy.""
  - ""Recommendation: Investigate hybrid frameworks combining oversampling with adaptive cost-sensitive algorithms to improve performance on imbalanced datasets.""
  - ""Recommendation: Explore parallel computing and hardware acceleration, such as GPUoptimized neural networks, and investigate lightweight, low-latency detection models for deployment in real-time scenarios.""
  - ""Recommendation: Develop regression-specific drift detection metrics and algorithms that focus on subtle shifts in continuous relationships, such as changes in correlation structures or error variance.""
  - ""Recommendation: Investigate the application of unsupervised feature extraction techniques, such as autoencoders, combined with domainspecific neural architectures to handle non-tabular data effectively.""
  - ""Recommendation: Incorporate self-supervised learning techniques to generate pseudo-labels, enabling more effective drift detection in unlabeled datasets. Develop approaches that require less computational cost.""
  - ""Recommendation: Establish a unified evaluation framework incorporating detection latency, scalability metrics, and resource usage alongside traditional accuracy measures to promote practical applicability.""",,"- The paper introduces the use of the T5 model for enhancing screening and data extraction processes, which is a novel technical approach.
- The review highlights various strategies for concept drift detection, including drift detection mechanisms (DDMs), window-based mechanisms (WBMs), unsupervised and semi-supervised methods (USSMs), ensemble methods (EMs), and neural networks (NNs), which are innovative solutions in themselves.
- The paper suggests hybrid approaches combining different methods to balance efficiency and accuracy, which is a creative problem-solving method.
- Recommendations for future research include developing hybrid frameworks, exploring parallel computing and hardware acceleration, and creating novel evaluation metrics, which are innovative solutions.
- The paper also suggests using unsupervised feature extraction techniques and self-supervised learning, which are novel feature engineering techniques.
- The recommendation to establish a unified evaluation framework is an innovative evaluation metric.","  - ""Key findings highlight the effectiveness of diverse methodologies such as drift detection methods, window-based methods, unsupervised statistical methods, and neural network techniques.""
  - ""The primary goal of this literature review is to illuminate the landscape of existing methods employed in detecting concept drift.""
  - ""The included studies were diverse regarding the study design, drift types, and methods used for concept drift detection.""
  - ""The objective of the quality assessment was to evaluate methodological rigor and potential sources of bias in the included studies, ensuring reliable and valid findings.""
  - ""The studies considered high quality (a score of 4-5) had solid foundations with clear problem statements, detailed and rigorous methodologies, robust validations, and high transparency.""
  - ""The key criteria used for the assessment included methodological clarity, empirical validation, reproducibility, and practical relevance.""
  - ""The studies were categorized based on the methods used for detecting concept drift.""
  - ""The findings are summarized in Table 4""
  - ""The included studies varied in study design, drift types, data types, and methods.""
  - ""The use of synthetic and real-world datasets has been crucial in evaluating the performance of these methods.""
  - ""the THU-Concept-Drift-Datasets and the MOA (Massive Online Analysis) framework have emerged as valuable resources for researchers, offering diverse scenarios to rigorously test and compare different methods.""
  - ""Despite the extensive research and numerous advancements in the field, several weak points remain.""
  - ""One major limitation is the handling of imbalanced data, which continues to pose significant challenges for many algorithms.""
  - ""Another area that requires attention is the computational efficiency of concept drift detection methods.""
  - ""The application of concept drift detection methods to non-tabular datasets, such as image and time-series data, remains underexplored.""
  - ""the evaluation of concept drift detection methods would benefit from the development of standardized protocols.""
  - ""Recent studies have systematically compared these methods, providing deeper insights into their relative performance across various contexts.""
  - ""Barros et al. presented two papers in 2018 and 2019 where researchers evaluated different methods across multiple datasets and drift types.""
  - ""Hinder et al. (2023) provided a survey and standardized experiments to benchmark unsupervised drift detection methods.""
  - ""Evaluating how models handle imbalanced classification is essential as it affects a model's ability to learn from minority classes and maintain high overall accuracy.""
  - ""The speed of model training and prediction is a critical factor, especially in real-time applications.""
  - ""Evaluating these metrics helps us understand the trade-offs between model complexity, accuracy, and computational demands such that the chosen approach can meet the real-time requirements of the application.""
  - ""While neural networks (NNs) and ensemble methods (EMs) provide the highest accuracy, their computational cost limits their realtime applicability.""
  - ""Unsupervised and semi-supervised methods (USSMs) excel in novel class detection, and WBMs are ideal for streaming environments.""
  - ""Hybrid approaches combining lightweight methods (e.g., DDMs) with adaptive techniques (e.g., NNs) show promise in balancing computational efficiency with detection accuracy.""",,"- The paper provides a comprehensive review of concept drift detection methods, focusing on their performance in terms of accuracy, computational efficiency, and applicability.
- The authors highlight the effectiveness of various methodologies such as drift detection methods, window-based methods, unsupervised statistical methods, and neural network techniques.
- The paper discusses the use of prequential error as a measure of accuracy and the importance of handling imbalanced classification.
- It also emphasizes the need for efficient processing speed and metrics such as training time, prediction time, throughput, and latency.
- The authors note that while neural networks and ensemble methods offer high accuracy, their computational cost can limit real-time applicability.
- Unsupervised and semi-supervised methods are highlighted for their excellence in novel class detection, and window-based methods are ideal for streaming environments.
- Hybrid approaches are suggested as promising for balancing computational efficiency with detection accuracy.
- The paper does not provide specific quantitative performance results but offers a qualitative overview of the strengths and weaknesses of different methods.","  - ""The initial research was conducted on two primary databases, IEEE and Science Direct, utilizing their inherent Application Programming Interfaces (APIs) and Python SDK.""
  - ""This led to the identification of 450 potential studies.""
  - ""Our search encompassed a comprehensive overview of all relevant topics, with search queries including terms such as the following: As we delved deeper into the literature during our review process, we found additional key papers in the references sections of various articles.""
  - ""These relevant publications, curated from platforms like Springer, ResearchGate, and ACM Digital Library, contributed an extra 40 studies.""
  - ""Therefore, our review process expanded, ultimately examining 490 studies, enhancing the comprehensiveness of our research base.""
  - ""To ensure the relevance and quality of the collected literature, we established strict inclusion and exclusion criteria.""
  - ""Papers to be included had to address concept drift detection methodologies explicitly, provide empirical evaluations, and be published in reputable journals or conferences.""
  - ""Conversely, we excluded papers that focused solely on general machine learning concepts without specific relevance to concept drift detection.""
  - ""From our initial pool of 490 studies, these criteria allowed us to sift down to 356 papers that satisfied all conditions.""
  - ""To further underscore the quality of our chosen literature, we conducted a citation analysis to identify the most influential articles on concept drift detection.""
  - ""The aim was to identify articles that adequately met our inclusion criteria, which required the works to be peer-reviewed, written in English, and published in either research journals or conference proceedings.""
  - ""Each study needed to demonstrate applications or simulations related to concept drift and implement or evaluate techniques for detecting concept drift.""
  - ""Furthermore, the outcomes of the studies had to involve measuring or observing the effectiveness of the concept drift detection techniques.""
  - ""Adhering to these criteria, we narrowed our pool to 254 articles.""
  - ""The eligibility assessment phase aimed to ensure that the included studies adhered strictly to the pre-established quality and relevance criteria.""
  - ""From the 254 studies that passed the initial screening, we conducted a meticulous full-text review of each paper to ascertain its compliance with our inclusion criteria.""
  - ""This process emphasized the relevance of each paper to our research questions, the scope of the investigation, and its alignment with the objectives of this review.""
  - ""During this phase, we delved deeper into the studies' methods, results, and conclusions.""
  - ""We performed this careful examination to include only high-quality and relevant studies in our review.""
  - ""Studies that used these techniques for unrelated tasks were excluded to maintain the focus and relevance of our review.""
  - ""This diligent step refined our focus, resulting in a concentrated pool of 111 studies.""
  - ""The objective of the quality assessment was to evaluate methodological rigor and potential sources of bias in the included studies, ensuring reliable and valid findings.""
  - ""The key criteria used for the assessment included methodological clarity, empirical validation, reproducibility, and practical relevance.""
  - ""Methodological clarity involved assessing whether the concept drift problem was clearly defined and whether methods and algorithms were described in detail.""
  - ""Empirical validation included evaluating the relevance and quality of datasets; the clarity and relevance of performance metrics, such as accuracy and detection delay; and whether methods were compared with existing baseline methods.""
  - ""We assessed reproducibility by checking the availability of data, code, and steps to reproduce the study for replication and the transparency of the study's methodology and reporting.""
  - ""Finally, we determined the practical relevance based on whether the methods were applicable to real-world scenarios and the overall impact and contribution to the field.""
  - ""Each study was rated on a scale from 1 to 5 for each criterion, and the overall quality scores were calculated.""
  - ""The studies considered high quality (a score of 4-5) had solid foundations with clear problem statements, detailed and rigorous methodologies, robust validations, and high transparency.""
  - ""Those of moderate quality (3)(4) provided clear methodologies but had some limitations in data availability or comparative analysis-they offered useful insights but required careful interpretation.""
  - ""Those of low quality (below 3) had unclear problem statements and insufficient methodological detail-they had methodological weaknesses and were considered with caution.""","  - ""(Page 5, Table 1) | Quality Category | Score Range | Number of Studies | Percentage of Total |\n|------------------|-------------|-------------------|---------------------|\n| High             | 4–5         | 51                | 45%                 |""
  - ""(Page 7, Table 1) | Drift Type         | Description                                           | Real-World Example                                                        |\n|--------------------|-------------------------------------------------------|---------------------------------------------------------------------------|\n| Virtual Drift      | Changes in \( P(X) \) without altering \( P(Y|X) \). | Seasonal variations in customer transactions.                             |""
  - ""(Page 9, Table 1) | Study                                                                 | Drift Type                     | Method     | Findings                                                                                                                                                       |\n|-----------------------------------------------------------------------|--------------------------------|------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| DDM [19]                                                              | Sudden+Gradual                 | DDM        | Detects abrupt and gradual drifts by error rate changes.                                                                                                       |""
  - ""(Page 10, Table 1) | Study                                                                 | Drift Type                  | Method | Findings                                                                                     |\n|----------------------------------------------------------------------|-----------------------------|--------|----------------------------------------------------------------------------------------------|\n| CEVOT [38]                                                           | Sudden+Gradual              | WBM    | Combines evolutionary algorithms with decision trees for incremental adaptation.             |""
  - ""(Page 11, Table 1) | Study                                                                 | Drift Type                  | Method | Findings                                                                                                                                                       |\n|----------------------------------------------------------------------|-----------------------------|--------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| OLINDDA technique [59]                                               | Novel Class                 | USSM   | Presents a cluster-based approach for detecting novelty and concept drift                                                                                      |""
  - ""(Page 12, Table 1) | Study                                                   | Drift Type                  | Method | Findings                                                                                                                                                       |\n|---------------------------------------------------------|-----------------------------|--------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Extreme learning machine (ELM) [66]                     | Sudden+Gradual+Reoccuring   | NN     | With a quick learning speed, uses a single-step LSE method for training SLFN, bypassing iterative gradient descent methods.                                     |""
  - ""(Page 17, Table 1) | Method | Accuracy  | Computational Cost | Applicability                                                                 |\n|--------|-----------|--------------------|-------------------------------------------------------------------------------|\n| DDM    | High      | Low                | Cost effective and easy to apply in real time                                 |""
  - ""(Page 18, Table 1) | Dataset Type            | Name                          | Characteristics                        | Short Description                                                                                                                                  |\n|-------------------------|-------------------------------|----------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n| Synthetic               | SEA Concepts                  | Abrupt drift                           | Data points generated from three attributes, with concept drift introduced by changing the decision boundary.                                      |""
  - ""(Page 19, Table 1) | Dataset Type                  | Name            | Characteristics                      | Short Description                                                                                   |\n|-------------------------------|-----------------|--------------------------------------|-----------------------------------------------------------------------------------------------------|\n| THU-Concept-Drift-Datasets    | Linear          | Gradual, sudden, recurrent, abrupt drift | Decision boundary is a straight line, with drift simulated by rotating the line.                     |""","- The paper uses academic databases such as IEEE and Science Direct for initial research, indicating these as primary data sources.
- The search process involved using APIs and Python SDKs, suggesting a structured data collection method.
- Additional studies were found through references in other articles, indicating a snowballing effect in data collection.
- The paper mentions strict inclusion and exclusion criteria for study selection, ensuring relevance and quality.
- The use of citation analysis and the T5 model for abstract summarization indicates a systematic approach to evaluating study relevance.
- The eligibility assessment phase involved a full-text review to ensure compliance with criteria, emphasizing quality control.
- Quality assessment involved evaluating methodological rigor, bias, and practical relevance, using scales like NOS and CASP.
- The paper does not provide specific details on dataset sizes or characteristics beyond mentioning various datasets used in concept drift detection.
- There is no mention of training/validation/test data splits or specific data preprocessing and cleaning approaches.
- External knowledge bases or ontologies are not explicitly mentioned in the context of data sources.","  - ""This review aims to trace the growth and evolution of concept drift mitigation strategies and to provide a valuable resource that guides future research and deepens our understanding of this rapidly changing field.""
  - ""The primary goal of this literature review is to illuminate the landscape of existing methods employed in detecting concept drift.""
  - ""This review comprehensively examines various concept drift detection methods developed over the last two decades.""
  - ""The included studies varied in study design, drift types, data types, and methods.""
  - ""The objective of the quality assessment was to evaluate methodological rigor and potential sources of bias in the included studies, ensuring reliable and valid findings.""
  - ""The studies were categorized based on the methods used for detecting concept drift.""
  - ""The included studies were diverse regarding the study design, drift types, and methods used for concept drift detection.""
  - ""This systematic literature review explored the evolving strategies in concept drift detection over the past two decades.""
  - ""The use of synthetic and real-world datasets has been crucial in evaluating the performance of these methods.""
  - ""the THU-Concept-Drift-Datasets and the MOA (Massive Online Analysis) framework have emerged as valuable resources for researchers, offering diverse scenarios to rigorously test and compare different methods.""
  - ""Despite the extensive research and numerous advancements in the field, several weak points remain.""
  - ""Recommendation: Investigate hybrid frameworks combining oversampling with adaptive cost-sensitive algorithms to improve performance on imbalanced datasets.""
  - ""Recommendation: Explore parallel computing and hardware acceleration, such as GPUoptimized neural networks, and investigate lightweight, low-latency detection models for deployment in real-time scenarios.""
  - ""Recommendation: Develop regression-specific drift detection metrics and algorithms that focus on subtle shifts in continuous relationships, such as changes in correlation structures or error variance.""
  - ""Recommendation: Investigate the application of unsupervised feature extraction techniques, such as autoencoders, combined with domainspecific neural architectures to handle non-tabular data effectively.""
  - ""Recommendation: Incorporate self-supervised learning techniques to generate pseudo-labels, enabling more effective drift detection in unlabeled datasets. Develop approaches that require less computational cost.""
  - ""Recommendation: Curate real-world benchmark datasets with annotations for known drift types, including contextual metadata, to improve the practical evaluation of drift detection methods.""
  - ""Recommendation: Establish a unified evaluation framework incorporating detection latency, scalability metrics, and resource usage alongside traditional accuracy measures to promote practical applicability.""",,"- The paper is a systematic review focused on concept drift detection in machine learning, indicating its primary application context is within the field of machine learning and data science.
- The review aims to guide future research and provide a resource for understanding concept drift mitigation strategies, suggesting its target research discipline is machine learning and related fields.
- The paper addresses the literature review task of categorizing and evaluating existing concept drift detection methods, indicating its specific literature review task.
- The paper processes academic documents related to concept drift detection, as evidenced by the inclusion of studies from various databases and the use of a systematic review process.
- The user types and requirements are likely researchers and practitioners in machine learning and data science who need to understand and apply concept drift detection methods.
- The integration with research workflows is implied by the paper's focus on guiding future research and providing a resource for understanding concept drift mitigation strategies.
- The paper does not explicitly mention commercial applications, but its focus on guiding future research suggests it is primarily aimed at academic applications."
9 Hierarchical arrangement of scholarly and novel information (HASANI): a digital platform to synthesize research evidence in real time,"Izhar Hasan, Uzair Hasan, Babar Rao, Salman Habib Abbasi",10.1136/bmjebm-2019-EBMLive.17,https://doi.org/10.1136/bmjebm-2019-EBMLive.17,Oral Presentations,0,2019,"- Main AI/ML frameworks used: Not mentioned
- System architecture components: Digital curation framework, critical appraisal process, integrated visual abstract application, automated data analytical interface, platform API
- Technical infrastructure: SaaS-based model, angular JS framework
- Integration with existing academic databases or platforms: API integration with online journals
- Overall system design approach: SaaS-based model","- Algorithms and models used: Automated data analytical interface for systematic reviews and meta-analysis
- Feature extraction techniques: Integrated visual abstract application for authors to submit pre-appraised elements
- Data processing pipelines: SaaS-based model with API integration for online journals
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Digital curation framework for knowledge extraction and summarization",Not mentioned (the abstract does not specify the paper discovery techniques used in the HASANI platform),"- Algorithm performance issues: Potential challenges in accuracy and efficiency in identifying and curating relevant literature.
- Integration difficulties: Ensuring seamless communication between the platform and online journals through APIs.
- Scalability challenges: Handling increased data and user growth with a SaaS-based model and Angular JS framework.
- Data quality or availability problems: Ensuring accuracy and reliability of automated processes to eliminate human bias and errors.","- Digital curation framework for automating knowledge extraction and summarization
- Critical appraisal process at publication submission
- Integrated visual abstract application for authors
- Automated data analytical interface for systematic reviews and meta-analysis
- Platform API for automatic paper summarization
- SaaS-based model in angular JS framework with API integration
- Real-time automated pooling of similar research studies
- Elimination of human bias and errors through automation","- Efficiency: Preliminary beta study results confirmed efficiency in identifying, curating, and synthesizing literature summaries.
- Processing speed and efficiency metrics: Saves time and money; eliminates human bias and errors.
- Comparison with baseline methods: Not mentioned.
- User satisfaction or usability results: Not mentioned.
- System reliability and robustness measures: Not mentioned.
- Scalability test results: Not mentioned.","Not mentioned (the abstract does not provide specific information about data sources, dataset sizes, data collection methods, or external knowledge bases used)","- Target research disciplines or fields: General research disciplines, particularly for clinicians and researchers.
- Specific literature review tasks addressed: Automated systematic reviews and meta-analysis.
- Types of academic documents processed: Published and unpublished publications.
- User types and requirements: Authors and publishers; requires submission of critically appraised elements and customization of templates.
- Integration with research workflows: Integration with online journals through APIs; customization of templates for different study types.
- Commercial vs. academic applications: SaaS-based model with a focus on academic applications for clinicians and researchers.","  - ""Implement a digital curation framework to automate knowledge extraction and summerization of published and unpublished publications""
  - ""Implement a critical appraisal process at the time of publication submission to journals.""
  - ""Implement an integrated visual abstract application for authors to submit critically appraised elements of the study""
  - ""Implement an automated data analytical interface to generate automated systematic reviews and meta–analysis""
  - ""Implement a platform API for publishers to automatically summarize their published papers for automated systematic reviews and met analysis.""
  - ""The application is implemented as SaaS-based model in angular JS framework with Application program interface (API) to integrate with online journals.""
  - ""This creates an opportunity to synthesize research evidence in real time for a given research topic, population or territory.""
  - ""Each publisher or organization can customize templates for various study types to create automated systematic review, meta-analysis, and qualitative review studies""",,"- The abstract mentions the implementation of a ""digital curation framework"" which suggests a structured approach to managing and synthesizing research evidence, but it does not specify any AI/ML frameworks like deep learning or NLP.
- The use of ""angular JS framework"" indicates the technical infrastructure is based on a JavaScript framework, which is a common choice for web applications.
- The mention of ""Application program interface (API)"" suggests integration with online journals, which could imply integration with existing academic databases or platforms.
- The system architecture includes an ""automated data analytical interface"" for generating systematic reviews and meta-analysis, but the specific technical components like databases or computational resources are not detailed.
- The abstract does not mention any specific cloud platforms or computational resources used for the technical infrastructure.
- The overall system design approach is described as a ""SaaS-based model,"" which indicates a software-as-a-service architecture, but again, specific technical details are not provided.","  - ""Implement a digital curation framework to automate knowledge extraction and summerization of published and unpublished publications""
  - ""Implement a critical appraisal process at the time of publication submission to journals.""
  - ""Implement an integrated visual abstract application for authors to submit critically appraised elements of the study""
  - ""Implement an automated data analytical interface to generate automated systematic reviews and meta–analysis""
  - ""Implement a platform API for publishers to automatically summarize their published papers for automated systematic reviews and met analysis.""
  - ""The application is implemented as SaaS-based model in angular JS framework with Application program interface (API) to integrate with online journals.""
  - ""Each publisher or organization can customize templates for various study types to create automated systematic review, meta-analysis, and qualitative review studies""
  - ""A beta testing of our platform (HASANI) has been conducted to display the automated article summary creation from critical appraisal elements.""",,"- The abstract mentions the implementation of a ""digital curation framework"" to automate knowledge extraction and summarization, which suggests a structured approach to processing and organizing data.
- The use of an ""integrated visual abstract application"" implies a feature extraction technique where authors submit pre-appraised elements, which are then used to generate summaries.
- The mention of an ""automated data analytical interface"" suggests the use of algorithms or models to generate systematic reviews and meta-analyses, although specific algorithms like neural networks or clustering are not mentioned.
- The platform is described as a ""SaaS-based model in angular JS framework,"" indicating a web-based application with a user interface built using Angular JS.
- The use of an ""Application program interface (API)"" to integrate with online journals suggests a data processing pipeline that involves API calls to retrieve and process data from journals.
- The ability for publishers to customize templates for different study types implies a flexible data processing workflow that can adapt to various study formats.
- The abstract does not provide specific details on training methodologies, preprocessing steps, or technical workflow beyond the general framework and API integration.","  - ""Implement a digital curation framework to automate knowledge extraction and summerization of published and unpublished publications""
  - ""Implement a critical appraisal process at the time of publication submission to journals.""
  - ""Implement an integrated visual abstract application for authors to submit critically appraised elements of the study""
  - ""Implement an automated data analytical interface to generate automated systematic reviews and meta–analysis""
  - ""Implement a platform API for publishers to automatically summarize their published papers for automated systematic reviews and met analysis.""
  - ""This creates an opportunity to synthesize research evidence in real time for a given research topic, population or territory.""
  - ""Each publisher or organization can customize templates for various study types to create automated systematic review, meta-analysis, and qualitative review studies""
  - ""Preliminary beta study results confirmed that our framework was efficient in identifying, curating and synthesizing the literature article summary of a given article to pool against similar studies.""",,"- The abstract mentions the implementation of a ""digital curation framework"" which suggests a method for organizing and selecting relevant papers, but it does not specify the exact techniques used for paper discovery.
- The ""critical appraisal process"" implies some form of evaluation, but again, it does not detail how papers are initially found or matched.
- The ""integrated visual abstract application"" and ""automated data analytical interface"" suggest tools for summarizing and analyzing papers, but not how they are discovered.
- The mention of ""customizable templates"" for different study types implies some form of categorization, but it does not specify how papers are matched to these templates.
- The abstract does not explicitly mention search and retrieval algorithms, relevance ranking approaches, citation analysis methods, content-based matching techniques, metadata extraction and utilization, recommendation system approaches, or expert/authority identification methods.","  - ""Implement a digital curation framework to automate knowledge extraction and summerization of published and unpublished publications""
  - ""Implement a critical appraisal process at the time of publication submission to journals.""
  - ""Implement an integrated visual abstract application for authors to submit critically appraised elements of the study""
  - ""Implement an automated data analytical interface to generate automated systematic reviews and meta–analysis""
  - ""Implement a platform API for publishers to automatically summarize their published papers for automated systematic reviews and met analysis.""
  - ""The application is implemented as SaaS-based model in angular JS framework with Application program interface (API) to integrate with online journals.""
  - ""Preliminary beta study results confirmed that our framework was efficient in identifying, curating and synthesizing the literature article summary of a given article to pool against similar studies.""
  - ""This strategy not only saves time and money in synthesizing new research evidence, but also provides a platform for insuring a quality research publication as it eliminates a human bias and errors.""
  - ""A real time automated pooling of similar research studies expedites a creation of automated systematic reviews generation which addresses the research publication overload in coming years""",,"- The abstract discusses the implementation of a digital platform for synthesizing research evidence in real-time, which involves several technical components such as a digital curation framework, critical appraisal process, visual abstract application, automated data analytical interface, and platform API.
- The platform is designed to automate knowledge extraction and summarization, which could imply potential challenges related to algorithm performance issues, especially in terms of accuracy and efficiency in identifying and curating relevant literature.
- The integration of the platform with online journals through APIs could pose integration difficulties, as ensuring seamless communication between different systems can be challenging.
- The abstract mentions the use of a SaaS-based model and Angular JS framework, which might introduce scalability challenges as the platform grows and handles more data.
- The mention of eliminating human bias and errors suggests that the platform is designed to address data quality issues, but this could also imply challenges in ensuring the accuracy and reliability of automated processes.
- The abstract does not explicitly mention any technical challenges or limitations, but the complexity of the system and its goals suggest potential challenges in algorithm performance, data quality, scalability, and integration.","  - ""Implement a digital curation framework to automate knowledge extraction and summerization of published and unpublished publications""
  - ""Implement a critical appraisal process at the time of publication submission to journals.""
  - ""Implement an integrated visual abstract application for authors to submit critically appraised elements of the study""
  - ""Implement an automated data analytical interface to generate automated systematic reviews and meta–analysis""
  - ""Implement a platform API for publishers to automatically summarize their published papers for automated systematic reviews and met analysis.""
  - ""The application is implemented as SaaS-based model in angular JS framework with Application program interface (API) to integrate with online journals.""
  - ""This strategy not only saves time and money in synthesizing new research evidence, but also provides a platform for insuring a quality research publication as it eliminates a human bias and errors.""
  - ""A real time automated pooling of similar research studies expedites a creation of automated systematic reviews generation which addresses the research publication overload in coming years""
  - ""Our initial findings provide supportive evidence of automating the literature curating and extraction strategy.""",,"- The abstract describes several innovative solutions related to the automation of literature synthesis and review processes. These include the implementation of a digital curation framework, which is a novel approach to automating knowledge extraction and summarization.
- The critical appraisal process at the time of publication submission is a creative problem-solving method that ensures quality and reduces bias.
- The use of an integrated visual abstract application for authors to submit critically appraised elements is a hybrid approach that streamlines the review process.
- The automated data analytical interface for generating systematic reviews and meta-analysis is a new algorithmic contribution that expedites the synthesis of research evidence.
- The platform API for publishers to automatically summarize papers is a technical workaround for the known problem of manual summarization, which is time-consuming and prone to errors.
- The SaaS-based model in angular JS framework with API integration is an original system design element that facilitates real-time synthesis and integration with online journals.
- The elimination of human bias and errors through automation is a significant technical innovation, addressing a common challenge in literature synthesis.","  - ""Preliminary beta study results confirmed that our framework was efficient in identifying, curating and synthesizing the literature article summary of a given article to pool against similar studies.""
  - ""Our initial findings provide supportive evidence of automating the literature curating and extraction strategy.""
  - ""This strategy not only saves time and money in synthesizing new research evidence, but also provides a platform for insuring a quality research publication as it eliminates a human bias and errors.""
  - ""A real time automated pooling of similar research studies expedites a creation of automated systematic reviews generation which addresses the research publication overload in coming years""",,"- The abstract mentions that the preliminary beta study results confirmed the efficiency of the framework in identifying, curating, and synthesizing literature summaries. This suggests a qualitative performance outcome related to efficiency.
- The mention of saving time and money, as well as eliminating human bias and errors, implies a qualitative improvement in processing speed and efficiency.
- The abstract does not provide specific quantitative metrics such as accuracy, precision, recall, F1-scores, or any specific comparison with baseline methods.
- There is no mention of user satisfaction, system reliability, robustness measures, or scalability test results in the abstract.
- The focus is on the qualitative benefits of the system, such as efficiency and quality assurance, rather than quantitative performance metrics.","  - ""Implement a digital curation framework to automate knowledge extraction and summerization of published and unpublished publications""
  - ""Implement a critical appraisal process at the time of publication submission to journals.""
  - ""Implement an integrated visual abstract application for authors to submit critically appraised elements of the study""
  - ""Implement an automated data analytical interface to generate automated systematic reviews and meta–analysis""
  - ""Implement a platform API for publishers to automatically summarize their published papers for automated systematic reviews and met analysis.""
  - ""The application is implemented as SaaS-based model in angular JS framework with Application program interface (API) to integrate with online journals.""
  - ""Each publisher or organization can customize templates for various study types to create automated systematic review, meta-analysis, and qualitative review studies""
  - ""A beta testing of our platform (HASANI) has been conducted to display the automated article summary creation from critical appraisal elements.""",,"- The abstract mentions the implementation of a digital curation framework to automate knowledge extraction and summarization, which suggests the use of some form of data sources, but it does not specify which academic databases are used.
- The mention of integrating with online journals through an API implies that data might be sourced from these journals, but again, specific databases like Scopus or Web of Science are not mentioned.
- There is no information provided about dataset sizes, characteristics, data collection methods, training/validation/test data splits, or data preprocessing and cleaning approaches.
- The abstract does not mention any external knowledge bases or ontologies used in the platform.
- The focus is on the automation of systematic reviews and meta-analysis, but the specific data sources or datasets used for these processes are not detailed.","  - ""Our main objective is to create a real time evidence generating platform for the most uptodate evidence-based practice with following specific goals.""
  - ""Implement a digital curation framework to automate knowledge extraction and summerization of published and unpublished publications""
  - ""Implement a critical appraisal process at the time of publication submission to journals.""
  - ""Implement an integrated visual abstract application for authors to submit critically appraised elements of the study""
  - ""Implement an automated data analytical interface to generate automated systematic reviews and meta–analysis""
  - ""Implement a platform API for publishers to automatically summarize their published papers for automated systematic reviews and met analysis.""
  - ""The application is implemented as SaaS-based model in angular JS framework with Application program interface (API) to integrate with online journals.""
  - ""This creates an opportunity to synthesize research evidence in real time for a given research topic, population or territory.""
  - ""Each publisher or organization can customize templates for various study types to create automated systematic review, meta-analysis, and qualitative review studies""
  - ""A real time automated pooling of similar research studies expedites a creation of automated systematic reviews generation which addresses the research publication overload in coming years""
  - ""Our initial findings provide supportive evidence of automating the literature curating and extraction strategy.""
  - ""This strategy provides a reassurance to academic audience that published articles have been formally appraised to be included in building a research evidence towards a particular topic or subject.""
  - ""More importantly, a real-time knowledge synthesis from this strategy will provide a more robust and uptodate practice guidelines for clinicians to focus on interpretation of research findings for applicability in their patient population, rather waiting for new systematic review and meta-analysis creation.""",,"- The abstract describes a platform designed to automate the synthesis of research evidence in real-time, which is relevant to various research disciplines and fields.
- The platform addresses specific literature review tasks such as automated systematic reviews and meta-analysis, indicating its focus on these tasks.
- It processes both published and unpublished publications, suggesting a broad scope of academic documents.
- The platform is designed for authors and publishers, as it allows authors to submit critically appraised elements and publishers to automatically summarize papers.
- Integration with research workflows is evident through the use of APIs to integrate with online journals and the ability to customize templates for different study types.
- The platform is described as a SaaS-based model, which suggests a commercial application, but its primary focus is on academic use, particularly for clinicians and researchers.
- The abstract mentions the platform's ability to provide real-time knowledge synthesis, which is crucial for keeping practice guidelines up-to-date for clinicians."
Retrieving and discovering new knowledge from documents' abstracts in scientific databases: Proposing a query-based abstractive summarization model,"Neda Abbasi Dashtaki, Mozafar CheshmehSohrabi, Mitra Pashootanizadeh, Hamidreza Baradaran Kashani",10.1016/j.jjimei.2025.100366,https://doi.org/10.1016/j.jjimei.2025.100366,Int. J. Inf. Manag. Data Insights,0,2025,"- Main AI/ML frameworks used: Not mentioned (implied use of NLP for abstractive summarization)
- System architecture components: Not mentioned
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Not mentioned
- Overall system design approach: Query-based indirect abstractive summarization approach",Not mentioned (the abstract does not provide specific technical implementation details),"- Search and retrieval algorithms: Improved utilization of scientifically validated documents
- Relevance ranking approaches: Systematic review and meta-analysis framework using 33 queries
- Content-based matching techniques: Use of PRISMA protocol for screening and inclusion
- Metadata extraction and utilization: Extraction, labeling, grouping, analysis, and inference steps
- Recommendation system approaches: Query-based indirect abstractive summarization approach","- Algorithm performance issues: Current systems lack the capability to extract and discover new knowledge from abstracts.
- Data quality or availability problems: Current search engines do not effectively utilize scientifically validated documents.
- Scalability challenges: The need to automate processes and propose a new model suggests scalability issues.
- Integration difficulties: The integration of new tools into scientific databases requires adapting existing systems.",The innovative solution is the proposal of a conceptual model using a query-based indirect abstractive summarization approach to enhance knowledge retrieval and discovery in scientific databases.,Not mentioned (the abstract does not provide any performance results or metrics),"- Academic databases used: Scopus
- Dataset sizes and characteristics: Not mentioned
- Data collection methods: Not mentioned
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: Broad application across various scientific fields
- Specific literature review tasks addressed: Systematic review and meta-analysis
- Types of academic documents processed: Abstracts from scientific databases
- User types and requirements: Database designers, administrators, and researchers
- Integration with research workflows: Enhanced access to scientific knowledge
- Commercial vs. academic applications: Academic","  - ""To automate these processes, we have proposed a conceptual model from query-based indirect abstractive summarization approach.""
  - ""To clarify the process of KR and KD as we define it, we employed a systematic review and meta-analysis framework using 33 queries.""
  - ""Current search engines for Knowledge Retrieval (KR) and Knowledge Discovery (KD) do not effectively utilize scientifically validated documents, especially those indexed in scientific databases.""
  - ""The integration of such tools into scientific databases will enhance user access to scientific knowledge to meet their informational and research needs.""
  - ""The aim is to introduce a model that can efficiently perform these tasks.""",,"- The abstract mentions the need for a model to efficiently perform knowledge retrieval and discovery tasks, indicating a focus on developing a new technical architecture.
- The use of a ""systematic review and meta-analysis framework"" suggests a structured approach to data analysis, which may involve AI/ML techniques, but specific frameworks like deep learning or NLP are not explicitly mentioned.
- The mention of a ""query-based indirect abstractive summarization approach"" implies the use of natural language processing (NLP) techniques, as abstractive summarization is a common application of NLP.
- The abstract does not provide details on specific system architecture components, technical infrastructure, or integration with existing databases beyond the mention of enhancing access to scientific knowledge.
- There is no explicit mention of main AI/ML frameworks, system architecture components, technical infrastructure, or integration with existing academic databases or platforms.","  - ""We conducted the identification, screening, eligibility, and inclusion steps following the PRISMA protocol.""
  - ""The aim is to introduce a model that can efficiently perform these tasks.""
  - ""To automate these processes, we have proposed a conceptual model from query-based indirect abstractive summarization approach.""
  - ""To clarify the process of KR and KD as we define it, we employed a systematic review and meta-analysis framework using 33 queries.""
  - ""Next, we performed extraction, labeling, grouping, analysis, and inference.""",,"- The abstract mentions the introduction of a model to perform tasks related to Knowledge Retrieval (KR) and Knowledge Discovery (KD), but it does not specify any technical implementation details such as algorithms, models, or feature extraction techniques.
- The use of a systematic review and meta-analysis framework with 33 queries suggests a structured approach to data analysis, but again, no specific technical methods are detailed.
- The mention of following the PRISMA protocol indicates adherence to a standard methodology for systematic reviews, but this is more about the process than technical implementation.
- The abstract refers to a ""conceptual model from query-based indirect abstractive summarization approach,"" which implies some form of summarization technique, but no specific algorithms or technical details are provided.
- Overall, the abstract lacks specific technical implementation details such as algorithms, feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or technical workflow.","  - ""Current search engines for Knowledge Retrieval (KR) and Knowledge Discovery (KD) do not effectively utilize scientifically validated documents, especially those indexed in scientific databases.""
  - ""Their Information Retrieval (IR) system only perform document searches and lack the capability to extract and discover new knowledge from documents' abstract in these databases and responding to users’ queries.""
  - ""The aim is to introduce a model that can efficiently perform these tasks.""
  - ""To clarify the process of KR and KD as we define it, we employed a systematic review and meta-analysis framework using 33 queries.""
  - ""We conducted the identification, screening, eligibility, and inclusion steps following the PRISMA protocol.""
  - ""Next, we performed extraction, labeling, grouping, analysis, and inference.""
  - ""To automate these processes, we have proposed a conceptual model from query-based indirect abstractive summarization approach.""",,"- The abstract discusses the limitations of current search engines in utilizing scientifically validated documents, which implies a need for improved search and retrieval algorithms.
- The mention of a systematic review and meta-analysis framework using 33 queries suggests a structured approach to identifying relevant papers, which could involve relevance ranking approaches.
- The use of the PRISMA protocol indicates a structured method for screening and inclusion, which is a form of content-based matching technique.
- The abstract mentions the extraction, labeling, grouping, analysis, and inference steps, which could involve metadata extraction and utilization.
- The proposal of a conceptual model from a query-based indirect abstractive summarization approach suggests a recommendation system approach to match papers with user queries.
- The abstract does not explicitly mention citation analysis methods or expert/authority identification methods, but these could be implied within the broader context of knowledge retrieval and discovery.","  - ""Their Information Retrieval (IR) system only perform document searches and lack the capability to extract and discover new knowledge from documents' abstract in these databases and responding to users’ queries.""
  - ""To automate these processes, we have proposed a conceptual model from query-based indirect abstractive summarization approach.""
  - ""The integration of such tools into scientific databases will enhance user access to scientific knowledge to meet their informational and research needs.""
  - ""Current search engines for Knowledge Retrieval (KR) and Knowledge Discovery (KD) do not effectively utilize scientifically validated documents, especially those indexed in scientific databases.""
  - ""The aim is to introduce a model that can efficiently perform these tasks.""",,"- The abstract mentions that current search engines do not effectively utilize scientifically validated documents, which implies a technical challenge related to data quality or availability problems.
- The lack of capability to extract and discover new knowledge from abstracts suggests a technical challenge related to algorithm performance issues, as current systems are not designed to perform these tasks efficiently.
- The need to automate processes and propose a new model indicates a technical challenge related to scalability challenges, as the current system is not scalable to handle these tasks.
- The integration of new tools into scientific databases suggests integration difficulties, as it requires adapting existing systems to incorporate new functionalities.
- The abstract does not explicitly mention evaluation methodology limitations, computational resource constraints, or technical bottlenecks or failure points, but these could be inferred as potential challenges in implementing and integrating new models.","  - ""To automate these processes, we have proposed a conceptual model from query-based indirect abstractive summarization approach.""
  - ""Current search engines for Knowledge Retrieval (KR) and Knowledge Discovery (KD) do not effectively utilize scientifically validated documents, especially those indexed in scientific databases.""
  - ""The aim is to introduce a model that can efficiently perform these tasks.""
  - ""The outcomes of this research offer fresh insights to database designers, administrators, and researchers, enabling the development of tools for KR and KD within these invaluable knowledge repositories.""",,"- The abstract mentions that current search engines do not effectively utilize scientifically validated documents, indicating a gap in the current technology that the proposed model aims to address.
- The introduction of a model to efficiently perform tasks related to knowledge retrieval and discovery suggests a new technical approach.
- The mention of a ""query-based indirect abstractive summarization approach"" implies a novel technical contribution, as it involves a new method for summarizing abstracts based on user queries.
- The abstract does not specify any new algorithmic contributions, creative problem-solving methods, hybrid or ensemble approaches, novel feature engineering techniques, innovative evaluation metrics, or technical workarounds for known problems. However, the proposed model itself is an innovative solution to the problem of effectively utilizing scientifically validated documents.","  - ""The aim is to introduce a model that can efficiently perform these tasks.""
  - ""To automate these processes, we have proposed a conceptual model from query-based indirect abstractive summarization approach.""
  - ""The integration of such tools into scientific databases will enhance user access to scientific knowledge to meet their informational and research needs.""
  - ""The outcomes of this research offer fresh insights to database designers, administrators, and researchers, enabling the development of tools for KR and KD within these invaluable knowledge repositories.""",,"- The abstract discusses the proposal of a conceptual model for query-based indirect abstractive summarization, which is aimed at improving knowledge retrieval and discovery in scientific databases.
- The focus is on the introduction and conceptualization of the model rather than its performance results.
- There is no mention of specific performance metrics such as accuracy, precision, recall, F1-scores, processing speed, efficiency metrics, comparison with baseline methods, user satisfaction, system reliability, or scalability test results.
- The abstract emphasizes the potential benefits and insights provided by the model but does not provide quantitative or qualitative performance outcomes.","  - ""The statistical population for this study encompasses all scientific databases, with a particular emphasis on Scopus.""
  - ""To clarify the process of KR and KD as we define it, we employed a systematic review and meta-analysis framework using 33 queries.""
  - ""To automate these processes, we have proposed a conceptual model from query-based indirect abstractive summarization approach.""
  - ""Next, we performed extraction, labeling, grouping, analysis, and inference.""
  - ""We conducted the identification, screening, eligibility, and inclusion steps following the PRISMA protocol.""",,"- The abstract mentions that the study focuses on ""all scientific databases, with a particular emphasis on Scopus,"" indicating that Scopus is a primary academic database used in the study.
- The use of a ""systematic review and meta-analysis framework using 33 queries"" suggests that these queries were used to collect data from the databases, but it does not specify the dataset sizes or characteristics.
- The abstract does not provide details on data collection methods, training/validation/test data splits, or data preprocessing and cleaning approaches.
- There is no mention of external knowledge bases or ontologies used in the study.
- The abstract does not provide specific information on dataset sizes, data collection methods, or data preprocessing techniques.","  - ""Current search engines for Knowledge Retrieval (KR) and Knowledge Discovery (KD) do not effectively utilize scientifically validated documents, especially those indexed in scientific databases.""
  - ""Scientific databases e.g., Scopus primarily consist of document-based content and provide documents' abstract.""
  - ""The statistical population for this study encompasses all scientific databases, with a particular emphasis on Scopus.""
  - ""To clarify the process of KR and KD as we define it, we employed a systematic review and meta-analysis framework using 33 queries.""
  - ""The outcome of these processes provided us with novel insights, which contribute to our exploratory knowledge.""
  - ""To automate these processes, we have proposed a conceptual model from query-based indirect abstractive summarization approach.""
  - ""The outcomes of this research offer fresh insights to database designers, administrators, and researchers, enabling the development of tools for KR and KD within these invaluable knowledge repositories.""
  - ""The integration of such tools into scientific databases will enhance user access to scientific knowledge to meet their informational and research needs.""",,"- The abstract discusses the application context in terms of improving knowledge retrieval and discovery in scientific databases, particularly focusing on Scopus.
- The target research disciplines or fields are not explicitly mentioned, but the focus on scientific databases suggests a broad application across various scientific fields.
- The specific literature review tasks addressed include systematic review and meta-analysis, as indicated by the use of a systematic review and meta-analysis framework.
- The types of academic documents processed are abstracts from scientific databases.
- The user types and requirements include database designers, administrators, and researchers who need enhanced access to scientific knowledge.
- The integration with research workflows is implied by the aim to enhance user access to scientific knowledge, which suggests integration into existing research processes.
- The application is academic rather than commercial, as it focuses on improving access to scientific knowledge within scientific databases."
Beyond Boundaries: A Machine Learning and Text Mining Approach to Mapping Corporate Entrepreneurship Research,"Isabella Fitzky, Niklas Schmid, Guido H. Baltes",10.1109/ICE/ITMC65658.2025.11106548,https://doi.org/10.1109/ICE/ITMC65658.2025.11106548,"International Conference on Engineering, Technology and Innovation",0,2025,"- Main AI/ML frameworks used: Text mining and machine learning algorithms
- System architecture components: Not mentioned
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Implied but not specified
- Overall system design approach: Machine learning-enhanced framework for literature review",Not mentioned (the abstract does not provide specific technical implementation details),"- Machine learning algorithms
- Text mining
- Content-based matching techniques
- Metadata extraction","- Scalability challenges: manual screening constraints, time-consuming analysis processes
- Evaluation methodology limitations: potential researcher bias","- Hybrid approach: Leveraging text mining and machine learning algorithms for literature review and analysis.
- Innovative system design element: Reproducible and scalable methodology for exploring corporate entrepreneurship research trends.",Not mentioned (the abstract does not provide specific performance results or metrics),"Not mentioned (the abstract does not provide specific information on data sources, dataset sizes, data collection methods, or data preprocessing)","- Target research disciplines or fields: Corporate entrepreneurship (CE)
- Specific literature review tasks addressed: Processing extensive academic databases, uncovering complex interconnections, identifying emerging research patterns
- Types of academic documents processed: Implied to be related to corporate entrepreneurship research
- User types and requirements: Researchers and practitioners in corporate entrepreneurship
- Integration with research workflows: Extends literature review practices to make them more efficient and adaptable
- Commercial vs. academic applications: Academic","  - ""Leveraging text mining and machine learning algorithms,""
  - ""developing a machine learning-enhanced framework for comprehensive literature review and analysis in the field of corporate entrepreneurship.""
  - ""uncovering complex interconnections and emerging research patterns that conventional manual reviews might overlook.""
  - ""extend literature review practices in corporate entrepreneurship research, offering researchers and practitioners a more efficient, comprehensive, and adaptable approach to understanding the evolving landscape of corporate entrepreneurship scholarship.""
  - ""processing extensive academic databases,""",,"- The abstract mentions the use of ""machine learning-enhanced framework"" and ""text mining and machine learning algorithms,"" which suggests that the main AI/ML frameworks used are likely related to text mining and machine learning.
- The mention of ""processing extensive academic databases"" implies integration with existing academic databases or platforms, but specific databases or platforms are not named.
- The abstract does not provide specific details about the system architecture components such as databases, APIs, interfaces, or technical infrastructure like cloud platforms or computational resources.
- The overall system design approach seems to focus on leveraging machine learning and text mining to improve literature review efficiency and comprehensiveness, but specific technical architecture details are not provided.","  - ""This study addresses these challenges by developing a machine learning-enhanced framework for comprehensive literature review and analysis in the field of corporate entrepreneurship.""
  - ""Leveraging text mining and machine learning algorithms, we provide an objective, dynamic approach to processing extensive academic databases, uncovering complex interconnections and emerging research patterns that conventional manual reviews might overlook.""",,"- The abstract mentions the use of ""machine learning-enhanced framework"" and ""text mining and machine learning algorithms,"" which suggests that these are key components of the implementation methods.
- However, the abstract does not provide specific technical details such as the types of algorithms (e.g., neural networks, clustering, classification), feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or technical workflow.
- The focus is on the general approach rather than specific technical implementation details.","  - ""Leveraging text mining and machine learning algorithms, we provide an objective, dynamic approach to processing extensive academic databases, uncovering complex interconnections and emerging research patterns that conventional manual reviews might overlook.""
  - ""This study addresses these challenges by developing a machine learning-enhanced framework for comprehensive literature review and analysis in the field of corporate entrepreneurship.""",,"- The abstract mentions the use of a ""machine learning-enhanced framework"" for literature review and analysis, which suggests the use of machine learning algorithms for paper discovery.
- The phrase ""leveraging text mining and machine learning algorithms"" indicates that these techniques are used to process academic databases, which could involve content-based matching techniques and metadata extraction.
- The abstract does not specifically mention search and retrieval algorithms, relevance ranking approaches, citation analysis methods, recommendation system approaches, or expert/authority identification methods. However, the use of machine learning and text mining implies some form of content-based matching and possibly metadata extraction.
- The focus on ""uncovering complex interconnections and emerging research patterns"" suggests that the methodology might involve some form of network analysis or pattern recognition, which could be related to citation analysis or content-based matching.","  - ""Traditional literature review methodologies face significant limitations, including manual screening constraints, potential researcher bias, and time-consuming analysis processes.""
  - ""This study addresses these challenges by developing a machine learning-enhanced framework for comprehensive literature review and analysis in the field of corporate entrepreneurship.""
  - ""Leveraging text mining and machine learning algorithms, we provide an objective, dynamic approach to processing extensive academic databases, uncovering complex interconnections and emerging research patterns that conventional manual reviews might overlook.""",,"- The abstract mentions ""manual screening constraints"" and ""time-consuming analysis processes"" as limitations of traditional literature review methodologies. These can be interpreted as scalability challenges and computational resource constraints, as manual screening is labor-intensive and time-consuming.
- The mention of ""potential researcher bias"" suggests a limitation in evaluation methodology, as human bias can affect the objectivity of the review process.
- The abstract does not explicitly mention algorithm performance issues, data quality or availability problems, integration difficulties, or technical bottlenecks or failure points. However, it implies that these challenges are addressed by the development of a machine learning-enhanced framework, which suggests an attempt to overcome scalability and evaluation methodology limitations.","  - ""Leveraging text mining and machine learning algorithms, we provide an objective, dynamic approach to processing extensive academic databases, uncovering complex interconnections and emerging research patterns that conventional manual reviews might overlook.""
  - ""This study addresses these challenges by developing a machine learning-enhanced framework for comprehensive literature review and analysis in the field of corporate entrepreneurship.""
  - ""By extending and complementing previous systematic review approaches, our research introduces a reproducible and scalable methodology for exploring corporate entrepreneurship research trends.""",,"- The abstract mentions the development of a ""machine learning-enhanced framework"" for literature review and analysis, which suggests a novel technical approach by integrating machine learning into traditional literature review methodologies.
- The use of ""text mining and machine learning algorithms"" indicates a hybrid approach, combining these technologies to process academic databases and uncover patterns that might be missed by manual reviews.
- The methodology is described as ""reproducible and scalable,"" which implies an innovative system design element that allows for efficient and adaptable analysis.
- The abstract does not specify new algorithmic contributions, creative problem-solving methods, or novel feature engineering techniques explicitly, but the integration of machine learning and text mining suggests a technical workaround for the limitations of traditional manual screening and analysis processes.","  - ""This study addresses these challenges by developing a machine learning-enhanced framework for comprehensive literature review and analysis in the field of corporate entrepreneurship.""
  - ""Leveraging text mining and machine learning algorithms, we provide an objective, dynamic approach to processing extensive academic databases, uncovering complex interconnections and emerging research patterns that conventional manual reviews might overlook.""
  - ""Our methodology aims to extend literature review practices in corporate entrepreneurship research, offering researchers and practitioners a more efficient, comprehensive, and adaptable approach to understanding the evolving landscape of corporate entrepreneurship scholarship.""",,"- The abstract discusses the development of a machine learning-enhanced framework for literature review and analysis, which implies some level of performance improvement over traditional methods.
- The use of text mining and machine learning algorithms suggests an objective and dynamic approach, which could imply improved accuracy and efficiency compared to manual reviews.
- The abstract mentions that the methodology is ""more efficient, comprehensive, and adaptable,"" which could be interpreted as qualitative performance outcomes related to processing speed and efficiency.
- However, the abstract does not provide specific quantitative performance metrics such as accuracy, precision, recall, F1-scores, or any other specific metrics like processing speed or user satisfaction results.
- There is no mention of comparison with baseline methods, system reliability, robustness measures, or scalability test results in the abstract.","  - ""Leveraging text mining and machine learning algorithms, we provide an objective, dynamic approach to processing extensive academic databases, uncovering complex interconnections and emerging research patterns that conventional manual reviews might overlook.""",,"- The abstract mentions the use of ""extensive academic databases"" but does not specify which databases were used (e.g., Scopus, Web of Science).
- There is no mention of dataset sizes or characteristics, data collection methods, training/validation/test data splits, or data preprocessing and cleaning approaches.
- The abstract does not mention any external knowledge bases or ontologies used in the study.
- The focus is on the methodology and its benefits rather than the specific data sources or technical details of data handling.","  - ""The field of corporate entrepreneurship (CE) continues to evolve rapidly, yet existing research remains fragmented across regions, firm types, and sectors.""
  - ""By extending and complementing previous systematic review approaches, our research introduces a reproducible and scalable methodology for exploring corporate entrepreneurship research trends.""
  - ""This study addresses these challenges by developing a machine learning-enhanced framework for comprehensive literature review and analysis in the field of corporate entrepreneurship.""
  - ""Our methodology aims to extend literature review practices in corporate entrepreneurship research, offering researchers and practitioners a more efficient, comprehensive, and adaptable approach to understanding the evolving landscape of corporate entrepreneurship scholarship.""
  - ""Leveraging text mining and machine learning algorithms, we provide an objective, dynamic approach to processing extensive academic databases, uncovering complex interconnections and emerging research patterns that conventional manual reviews might overlook.""",,"- The target research discipline or field is clearly identified as ""corporate entrepreneurship (CE)"".
- The specific literature review tasks addressed include processing extensive academic databases, uncovering complex interconnections, and identifying emerging research patterns.
- The types of academic documents processed are implied to be those related to corporate entrepreneurship research, although specific document types (e.g., articles, papers) are not explicitly mentioned.
- The user types and requirements are likely researchers and practitioners in the field of corporate entrepreneurship, as they are mentioned as beneficiaries of the methodology.
- Integration with research workflows is suggested by the aim to extend literature review practices, making it more efficient and adaptable.
- The application context is academic, as it focuses on enhancing literature review practices in corporate entrepreneurship research."
"Deep Learning and Autonomous Vehicles: Strategic Themes, Applications, and Research Agenda Using SciMAT and Content-Centric Analysis, a Systematic Review","Fábio Eid Morooka, Adalberto Manoel Junior, T. F. Sigahi, Jefferson de Souza Pinto, I. Rampasso, R. Anholon",10.3390/make5030041,https://doi.org/10.3390/make5030041,Machine Learning and Knowledge Extraction,21,2023,"- Main AI/ML frameworks used: SciMAT, deep learning (CNNs, RNNs)
- System architecture components: Scopus, Web of Science (WoS), RIS extension
- Technical infrastructure: SciMAT software
- Integration with existing academic databases: Scopus, Web of Science (WoS)
- Overall system design approach: Bibliometric analysis and content analysis","The implementation methods involve using SciMAT for co-word analysis to identify strategic themes in DL-AV research. Data was extracted from Scopus and WoS, processed into RIS format, and preprocessed by grouping similar terms. The study maps neural network techniques and models, using content analysis to categorize them, but does not detail specific algorithms or feature extraction techniques. The KITTI benchmark is mentioned as a dataset used in AV design.","The paper employs search and retrieval algorithms using Scopus and WoS databases, relevance ranking through strategic diagrams and cluster networks generated by SciMAT, content-based matching techniques via co-word analysis and content analysis, and metadata extraction and utilization by creating RIS files for SciMAT. It does not explicitly mention citation analysis methods, recommendation system approaches, or expert/authority identification methods.","- Algorithm performance issues: Need for robust learning in CNNs, evolution of object detection techniques.
- Data quality or availability problems: Importance of robust and reliable databases for AI model training and validation.
- Scalability challenges: Need for advanced training and fine-tuning approaches to unlock full potential of neural networks.
- Integration difficulties: Ensuring seamless integration with existing components and systems, addressing hardware limitations.
- Evaluation methodology limitations: Focus on specific databases and software, potential lack of coverage of all relevant publications or perspectives.
- Computational resource constraints: Issues of algorithmic complexity, model interpretability, and real-time constraints.
- Technical bottlenecks or failure points: Limitations and potential risks associated with neural network-based techniques.","- New algorithmic contributions: Liquid neural networks and transformers
- Creative problem-solving methods: Integration of neural networks and AI models in AVs
- Hybrid or ensemble approaches: Use of CNNs and RNNs in AVs
- Novel feature engineering techniques: Use of datasets like KITTI and over 73 other datasets
- Innovative evaluation metrics: Research agenda for optimizing AV design and performance
- Technical workarounds for known problems: Addressing computational resource requirements and model interpretability
- Original system design elements: Focus on transdisciplinary themes like energy efficiency, legislation, ethics, and cybersecurity",Not mentioned (the paper does not provide specific performance metrics or outcomes),"- Academic databases used: Scopus, Web of Science
- Dataset sizes and characteristics: KITTI benchmark includes 389 stereo and optical flow image pairs, 39.2 km of stereo visual odometry sequences, and over 200,000 annotations of 3D objects.
- Data collection methods: Not mentioned
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: Not mentioned","- Target research disciplines or fields: Computing, Environmental, Social Sciences
- Specific literature review tasks addressed: Identifying strategic themes and trends in DL-AV research using SciMAT and content analysis
- Types of academic documents processed: Journal articles and reviews
- User types and requirements: Experienced scholars and new researchers interested in technological development and social and environmental impacts
- Integration with research workflows: Use of SciMAT for bibliometric analysis and content analysis for categorizing contributions
- Commercial vs. academic applications: Primarily academic","  - ""The databases chosen for this research were Scopus and Web of Science (WoS) due to a combination of important features, including their wide global and regional coverage of scientific journals [15], which encompasses journals from other relevant databases such as Emerald and IEEE; its high-quality peer-reviewed journals in the areas of interest when compared to EBSCO, Google Scholar, or others [15]; and the availability of compatible metadata for bibliometric analysis software [16].""
  - ""The data from the 59 articles was then extracted from the Scopus and WoS databases by creating a file in the RIS (Research Information Systems) extension, which allowed uploading to the SciMAT software [20].""
  - ""After the data retrieval, the recommendations of Cobo et al. [20] were followed to structure the application of SciMAT as summarized in Figure 2""
  - ""SciMAT generates cluster networks to support the analysis of theme co-occurrence, providing a view of the degree of interaction between themes and subthemes [20].""
  - ""The integrated examination of strategic diagrams, cluster networks, and content analysis provides a chronological and conceptual understanding of the relevance of the motor themes and their connections.""
  - ""The content analysis enabled the mapping of neural network techniques used in AVs (Figure 8""
  - ""AI models used in AV design are built using one (or more) DL techniques.""
  - ""The KITTI benchmark [56] is a popular and widely used benchmark in the design of AVs.""
  - ""Other datasets identified, such as CityScapes, BDD100K, ApolloScape, Caltech, CamVId, nuScenes, PASCAL, and Waymo Open Dataset, are very similar to KITTI but were created by other research groups from universities or companies, using different platforms and sensors for data collection.""
  - ""The integration of neural networks and AI models in AVs holds immense potential for shaping the future of transportation.""",,"- The paper primarily uses Scopus and Web of Science (WoS) as databases for extracting articles, indicating these are key components of the technical architecture.
- The use of SciMAT software for bibliometric analysis suggests that this tool is a main AI/ML framework used in the study.
- The mention of creating a file in the RIS extension for uploading to SciMAT indicates a specific technical infrastructure for data processing.
- The paper discusses the use of neural networks and AI models, particularly CNNs and RNNs, as main AI/ML frameworks.
- The integration with existing academic databases is evident through the use of Scopus and WoS.
- The overall system design approach involves bibliometric analysis and content analysis to identify strategic themes and trends.","  - ""This study aims to identify the strategic themes and trends in DL-AV research using the Science Mapping Analysis Tool (SciMAT) and content analysis.""
  - ""The databases chosen for this research were Scopus and Web of Science (WoS) due to a combination of important features, including their wide global and regional coverage of scientific journals""
  - ""The identification of studies followed the PRISMA protocol as depicted in Figure 1""
  - ""The data from the 59 articles was then extracted from the Scopus and WoS databases by creating a file in the RIS (Research Information Systems) extension, which allowed uploading to the SciMAT software""
  - ""In the preprocessing stage, similar terms were grouped using singular/plural, 1-and 2-character difference, and manual evaluation by meaning (e.g., ""Internet of Things"" and ""IoT""). This process resulted in 573 groups of words.""
  - ""SciMAT's co-word analysis capabilities allow researchers to identify key topics, concepts, and themes that are prevalent within a given research domain""
  - ""The main graphic outcomes from SciMAT are the strategic diagrams and network structures.""
  - ""The content analysis enabled the mapping of neural network techniques used in AVs (Figure 8""
  - ""The content analysis enabled the identification of more than 100 different models, of which the most frequently mentioned are shown in Figure 9""
  - ""the datasets were also mapped (Figure 10""",,"- The study uses the Science Mapping Analysis Tool (SciMAT) for bibliometric analysis, which involves co-word analysis to identify key topics and themes in the research domain of deep learning and autonomous vehicles.
- Data was extracted from Scopus and Web of Science databases, and processed into a format compatible with SciMAT (RIS extension).
- Preprocessing involved grouping similar terms based on singular/plural forms, minor character differences, and manual evaluation for meaning, resulting in 573 groups of words.
- The study does not explicitly mention specific algorithms or models used in the implementation but refers to the mapping of neural network techniques and models, as well as datasets like the KITTI benchmark.
- The methodological approach includes content analysis to categorize neural network techniques and AI models used in autonomous vehicles.
- The study follows the PRISMA protocol for systematic literature review, ensuring a structured approach to identifying and analyzing relevant studies.","  - ""The databases chosen for this research were Scopus and Web of Science (WoS) due to a combination of important features, including their wide global and regional coverage of scientific journals [15], which encompasses journals from other relevant databases such as Emerald and IEEE; its high-quality peer-reviewed journals in the areas of interest when compared to EBSCO, Google Scholar, or others [15]; and the availability of compatible metadata for bibliometric analysis software [16].""
  - ""Prior SLRs, e.g., [1,8,11], were used as a basis for developing the search string, which was defined as follows: (""artificial intelligence"" OR ""deep learning"") AND (""autonomous vehicle*"" OR ""autonomous driv*"" OR ""self-driv*"").""
  - ""The following criteria were used as filters: only journal articles and reviews; publications from 2017 to 2022; document available in English; and search terms appear in the title, abstract, or keywords.""
  - ""The identification of studies followed the PRISMA protocol as depicted in Figure 1""
  - ""The data from the 59 articles was then extracted from the Scopus and WoS databases by creating a file in the RIS (Research Information Systems) extension, which allowed uploading to the SciMAT software [20].""
  - ""After the data retrieval, the recommendations of Cobo et al. [20] were followed to structure the application of SciMAT as summarized in Figure 2""
  - ""In the preprocessing stage, similar terms were grouped using singular/plural, 1-and 2-character difference, and manual evaluation by meaning (e.g., ""Internet of Things"" and ""IoT""). This process resulted in 573 groups of words.""
  - ""SciMAT's co-word analysis capabilities allow researchers to identify key topics, concepts, and themes that are prevalent within a given research domain [20,22].""
  - ""The main graphic outcomes from SciMAT are the strategic diagrams and network structures.""
  - ""The strategic diagram (Figure 3 ) represents the centrality (degree of importance) of the themes on the X axis and the density (degree of development) on the Y axis, resulting in four quadrants: (Q1) motor themes, which are important topics with high development; (Q2) basic and transversal themes, which may become motor themes in the future due to their high centrality; (Q3) emerging or declining themes, which require a qualitative analysis to determine the extent to which they are emerging or declining; and (Q4) highly developed and isolated themes, which are themes that have lost importance because of the appearance of a new concept or technology [21,22].""
  - ""SciMAT generates cluster networks to support the analysis of theme co-occurrence, providing a view of the degree of interaction between themes and subthemes [20].""
  - ""The content analysis enabled the mapping of neural network techniques used in AVs (Figure 8""
  - ""AI models used in AV design are built using one (or more) DL techniques. The content analysis enabled the identification of more than 100 different models, of which the most frequently mentioned are shown in Figure 9""
  - ""When developing a machine-learning project, one of the first things to be studied and defined is the dataset that will be used for training, testing, and validation of the model or algorithm. A robust and reliable database is required in the development of AV projects in order to perform AI model training and validation. Thus, the datasets were also mapped (Figure 10""
  - ""The KITTI benchmark [56] is a popular and widely used benchmark in the design of AVs.""
  - ""The content analysis enabled the identification of more than 100 different models, of which the most frequently mentioned are shown in Figure 9""
  - ""the datasets were also mapped (Figure 10""",,"- The paper uses Scopus and Web of Science (WoS) databases for retrieving academic papers, highlighting their wide coverage and high-quality peer-reviewed journals. This indicates a reliance on established databases for search and retrieval.
- The search string was developed based on prior systematic literature reviews (SLRs), indicating a methodical approach to search query formulation.
- Filters were applied to ensure relevance, such as limiting to journal articles and reviews, publications from 2017 to 2022, and documents in English with search terms in the title, abstract, or keywords.
- The PRISMA protocol was used for study identification, which is a systematic method for selecting relevant studies.
- Data extraction involved creating a file in the RIS format for use with SciMAT, a bibliometric software, indicating metadata extraction and utilization.
- SciMAT was used for co-word analysis, strategic diagrams, and cluster networks, which are content-based matching techniques to identify key topics and themes.
- The paper discusses preprocessing steps like grouping similar terms, which aids in content-based matching and relevance ranking.
- The strategic diagrams and network structures generated by SciMAT help in understanding the centrality and density of themes, which can be seen as a form of relevance ranking.
- Content analysis was used to map neural network techniques and AI models, indicating a detailed examination of content for matching purposes.","  - ""The main advancement of CNNs was the capability of robust learning regarding high-level image feature representation""
  - ""The analysis of the remaining quadrants reveals additional important themes in DL-AV research.""
  - ""The cluster ""OBJECT-DETECTION"" appears in Q2 of 2019-2020 and then moves to Q1 as the most relevant motor theme in 2021-2022.""
  - ""The cluster ""CONVOLUTION-NEURAL-NETWORK"" is the only one that is positioned in Q1 in the period 2019-2020 and, despite its reduced density, remains a motor theme in the period 2021-2022.""
  - ""The content analysis enabled the mapping of neural network techniques used in AVs""
  - ""The content analysis enabled the identification of more than 100 different models, of which the most frequently mentioned are shown in Figure 9""
  - ""the datasets were also mapped (Figure 10""
  - ""The full potential of neural networks can be unlocked by employing advanced training and fine-tuning approaches.""
  - ""neural network-based techniques in AV applications come with limitations and potential risks that need to be carefully investigated.""
  - ""Researchers must navigate issues such as computational resource requirements, algorithmic complexity, model interpretability, and real-time constraints.""
  - ""ensuring seamless integration with existing components and systems, addressing hardware limitations, and designing robust validation and testing frameworks are crucial aspects to overcome in order to successfully harness the power of DL in AV design""
  - ""The methodology's shortcomings involve the focus on the Scopus and WoS databases considering publications from 2017 to June 2023.""
  - ""Another methodological limitation is the use of only one type of software, SciMAT.""
  - ""the methodology used in this study was centered on examining the structures of networks of motor themes with the highest degrees of density and centrality, which opens up several avenues for future research to expand the analysis of transversal, highly developed, and/or new emerging themes.""",,"- The paper discusses the strategic themes and trends in deep learning (DL) and autonomous vehicles (AVs) research, which includes technical challenges.
- The technical challenges mentioned include algorithm performance issues, such as the need for robust learning in CNNs and the evolution of object detection techniques.
- Data quality or availability problems are implied by the mention of dataset mapping and the importance of robust and reliable databases for AI model training and validation.
- Scalability challenges are suggested by the need for advanced training and fine-tuning approaches to unlock the full potential of neural networks.
- Integration difficulties are highlighted by the need to ensure seamless integration with existing components and systems, as well as addressing hardware limitations.
- Evaluation methodology limitations are indicated by the focus on specific databases and software, which may not cover all relevant publications or perspectives.
- Computational resource constraints are mentioned as a challenge, along with issues of algorithmic complexity, model interpretability, and real-time constraints.
- Technical bottlenecks or failure points are implied by the need to address limitations and potential risks associated with neural network-based techniques.","  - ""The cluster ""OBJECT-DETECTION"" appears in Q2 of 2019-2020 and then moves to Q1 as the most relevant motor theme in 2021-2022.""
  - ""The cluster ""CONVOLUTION-NEURAL-NETWORK"" is the only one that is positioned in Q1 in the period 2019-2020 and, despite its reduced density, remains a motor theme in the period 2021-2022.""
  - ""The content analysis enabled the mapping of neural network techniques used in AVs (Figure 8""
  - ""Regarding neural network techniques used in AVs, a research opportunity is to further explore new techniques such as liquid neural networks and transformers.""
  - ""The liquid neural network (LNN) was created by researchers at the Massachusetts Institute of Technology's (MIT's) Computer Science and Artificial Intelligence Laboratory, and its main feature is that it learns on the fly, rather than just during training.""
  - ""Transformer is an architecture designed to solve sequence-to-sequence tasks while handling complex dependencies.""
  - ""AI models used in AV design are built using one (or more) DL techniques. The content analysis enabled the identification of more than 100 different models, of which the most frequently mentioned are shown in Figure 9""
  - ""When developing a machine-learning project, one of the first things to be studied and defined is the dataset that will be used for training, testing, and validation of the model or algorithm.""
  - ""The KITTI benchmark [56] is a popular and widely used benchmark in the design of AVs.""
  - ""Finally, it is worth mentioning that more than 73 different datasets were found.""
  - ""Several potential research avenues emerge when considering DL applications in AV project design.""
  - ""The exploration of DL techniques has the potential to revolutionize the design and performance of AV projects.""
  - ""By leveraging DL algorithms, researchers can delve into unexplored territories and uncover new possibilities for optimizing the overall functionality, efficiency, and safety of AVs.""
  - ""The integration of neural networks and AI models in AVs holds immense potential for shaping the future of transportation.""
  - ""Neural networks enable AVs to process vast amounts of data, learn from them, and make intelligent decisions in real time""
  - ""The transdisciplinary themes that emerge from the intersection of DL and AVs offer exciting possibilities for the future.""
  - ""DL techniques can be applied not only to enhance the technical aspects of AVs but also to address broader societal and ethical considerations""
  - ""The integrated bibliometric and content analysis approach enabled the identification of strategic themes (RQ1) and trends (RQ2) in DL-AV research.""
  - ""The findings presented in this study can benefit both experienced scholars who can gain access to condensed information about the literature on DL-AV and new researchers who may be attracted to topics related to technological development and other issues with social and environmental impacts.""
  - ""Identification of motor themes and research opportunities can fuel collaboration among researchers from all areas of knowledge, integrating concepts, theories, and methods primarily from computing, environmental, and social sciences for enhancing debates on themes such as energy, legislation, ethics, and cybersecurity in the context of AVs.""
  - ""Another significant contribution of this study was the proposal of a research agenda and future perspectives regarding three topics: DL application in AV project design; neural networks and AI models used in Avs; and transdisciplinary themes in DL-AV.""
  - ""It is expected that research will advance in these areas and provide valuable contributions to individuals, organizations, and society as a whole.""
  - ""the methodology used in this study was centered on examining the structures of networks of motor themes with the highest degrees of density and centrality, which opens up several avenues for future research to expand the analysis of transversal, highly developed, and/or new emerging themes.""",,"- The paper discusses the strategic themes and trends in deep learning (DL) and autonomous vehicles (AVs) research, which includes innovative solutions such as object detection and convolutional neural networks (CNNs).
- The use of liquid neural networks and transformers is highlighted as a research opportunity, indicating novel algorithmic contributions.
- The paper mentions the integration of neural networks and AI models in AVs, which suggests hybrid or ensemble approaches.
- The discussion on datasets like KITTI and the mention of over 73 different datasets indicate novel feature engineering techniques.
- The paper proposes a research agenda that includes exploring DL techniques for optimizing AV design and performance, which implies innovative evaluation metrics.
- The integration of DL algorithms into AV design processes and addressing challenges like computational resource requirements and model interpretability suggest technical workarounds for known problems.
- The paper's focus on transdisciplinary themes such as energy efficiency, legislation, ethics, and cybersecurity indicates original system design elements.","  - ""The content analysis enabled the mapping of neural network techniques used in AVs (Figure 8""
  - ""The content analysis enabled the identification of more than 100 different models, of which the most frequently mentioned are shown in Figure 9""
  - ""the datasets were also mapped (Figure 10""
  - ""The KITTI benchmark [56] is a popular and widely used benchmark in the design of AVs.""
  - ""Geiger et al. [56] asserted that KITTI is composed of 389 stereo and optical flow image pairs, 39.2 km of stereo visual odometry sequences, and over 200,000 annotations of 3D objects captured in cluttered scenarios.""
  - ""Other datasets identified, such as CityScapes, BDD100K, ApolloScape, Caltech, CamVId, nuScenes, PASCAL, and Waymo Open Dataset, are very similar to KITTI but were created by other research groups from universities or companies, using different platforms and sensors for data collection.""
  - ""Finally, it is worth mentioning that more than 73 different datasets were found.""
  - ""The integrated bibliometric and content analysis approach enabled the identification of strategic themes (RQ1) and trends (RQ2) in DL-AV research.""
  - ""The findings presented in this study can benefit both experienced scholars who can gain access to condensed information about the literature on DL-AV and new researchers who may be attracted to topics related to technological development and other issues with social and environmental impacts.""",,"- The paper primarily focuses on a systematic review of deep learning and autonomous vehicles, using bibliometric analysis and content analysis to identify strategic themes and trends.
- The paper discusses various neural network techniques and AI models used in AVs, as well as datasets like KITTI, but it does not provide specific performance metrics such as accuracy, precision, recall, or F1-scores.
- The paper does not mention processing speed and efficiency metrics, nor does it compare the performance of different methods or baseline models.
- There is no mention of user satisfaction or usability results, system reliability and robustness measures, or scalability test results.
- The paper is more focused on identifying themes and trends rather than evaluating specific performance outcomes.","  - ""The databases chosen for this research were Scopus and Web of Science (WoS) due to a combination of important features, including their wide global and regional coverage of scientific journals [15], which encompasses journals from other relevant databases such as Emerald and IEEE; its high-quality peer-reviewed journals in the areas of interest when compared to EBSCO, Google Scholar, or others [15]; and the availability of compatible metadata for bibliometric analysis software [16].""
  - ""The data from the 59 articles was then extracted from the Scopus and WoS databases by creating a file in the RIS (Research Information Systems) extension, which allowed uploading to the SciMAT software [20].""
  - ""The KITTI benchmark [56] is a popular and widely used benchmark in the design of AVs. This dataset was developed by researchers at the Karlsruhe Institute of Technology and the Toyota Technological Institute (KITTI). Its goal is to contribute to the research and development of AV projects, specifically stereo tasks, optical flow, visual odometry or SLAM, and 3D object detection.""
  - ""Geiger et al. [56] asserted that KITTI is composed of 389 stereo and optical flow image pairs, 39.2 km of stereo visual odometry sequences, and over 200,000 annotations of 3D objects captured in cluttered scenarios.""
  - ""Other datasets identified, such as CityScapes, BDD100K, ApolloScape, Caltech, CamVId, nuScenes, PASCAL, and Waymo Open Dataset, are very similar to KITTI but were created by other research groups from universities or companies, using different platforms and sensors for data collection.""
  - ""Finally, it is worth mentioning that more than 73 different datasets were found.""",,"- The paper uses Scopus and Web of Science as the primary academic databases for data extraction.
- The datasets mentioned include KITTI, which is described in detail with its composition and purpose.
- Other datasets like CityScapes, BDD100K, ApolloScape, Caltech, CamVId, nuScenes, PASCAL, and Waymo Open Dataset are mentioned as similar to KITTI but with different origins and characteristics.
- The paper does not provide specific information on data collection methods, training/validation/test data splits, or data preprocessing and cleaning approaches.
- There is no mention of external knowledge bases or ontologies used in the study.","  - ""This study aims to identify the strategic themes and trends in DL-AV research using the Science Mapping Analysis Tool (SciMAT) and content analysis.""
  - ""The content analysis allowed categorization of the contribution of the academic literature on DL applications in AV project design; neural networks and AI models used in AVs; and transdisciplinary themes in DL-AV research, including energy, legislation, ethics, and cybersecurity.""
  - ""The databases chosen for this research were Scopus and Web of Science (WoS) due to a combination of important features, including their wide global and regional coverage of scientific journals""
  - ""The identification of studies followed the PRISMA protocol as depicted in Figure 1""
  - ""The content analysis enabled the mapping of neural network techniques used in AVs""
  - ""AI models used in AV design are built using one (or more) DL techniques.""
  - ""The integration of neural networks and AI models in AVs holds immense potential for shaping the future of transportation.""
  - ""The integrated bibliometric and content analysis approach enabled the identification of strategic themes (RQ1) and trends (RQ2) in DL-AV research.""
  - ""The findings presented in this study can benefit both experienced scholars who can gain access to condensed information about the literature on DL-AV and new researchers who may be attracted to topics related to technological development and other issues with social and environmental impacts.""
  - ""Identification of motor themes and research opportunities can fuel collaboration among researchers from all areas of knowledge, integrating concepts, theories, and methods primarily from computing, environmental, and social sciences for enhancing debates on themes such as energy, legislation, ethics, and cybersecurity in the context of AVs.""
  - ""The methodology's shortcomings involve the focus on the Scopus and WoS databases considering publications from 2017 to June 2023.""
  - ""Another methodological limitation is the use of only one type of software, SciMAT.""
  - ""Regarding the scope of the analyzed documents, this study primarily focuses on journal articles.""
  - ""More comprehensive approaches can be adopted, including grey literature.""
  - ""The following supporting information can be downloaded at: https: //www.mdpi.com/article/10.3390/make5030041/s1, PRISMA 2020.""","  - ""(Page 12, Table 1) | Topics                                      | Research Questions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | References  |\n|---------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|\n| DL application in AV project design         | • How can DL techniques be applied to optimize the design and performance of AV projects?                                                                                                                                                                                                                                                                                                                                                                                                                 | [66,67]     |""","- The paper focuses on the application of deep learning (DL) in autonomous vehicles (AVs), which is a specific domain within the broader field of artificial intelligence (AI).
- The target research disciplines or fields include computing, environmental, and social sciences, as these are integrated in the study to address themes like energy, legislation, ethics, and cybersecurity.
- The specific literature review tasks addressed include identifying strategic themes and trends in DL-AV research using SciMAT and content analysis.
- The types of academic documents processed are journal articles and reviews, as indicated by the focus on Scopus and Web of Science databases.
- The user types and requirements include both experienced scholars and new researchers interested in technological development and social and environmental impacts.
- The integration with research workflows is evident through the use of SciMAT for bibliometric analysis and content analysis for categorizing contributions.
- The application context is primarily academic, as it focuses on systematic literature reviews and research agendas rather than commercial applications."
Exploring the intellectual cores of the blockchain-Internet of Things (BIoT),"Y. Tsang, Chun-Ho Wu, Andrew W. H. Ip, Wen-Lung Shiau",10.1108/JEIM-10-2020-0395,https://doi.org/10.1108/JEIM-10-2020-0395,Journal of Enterprise Information Management,54,2021,Not mentioned (the abstract does not provide information on the technical architecture or system components),"- Algorithms and models used: Co-citation proximity analysis–based systematic review (CPASR), exploratory factor analysis (EFA), hierarchical cluster analysis (HCA), k-means clustering (KMC), multidimensional scaling (MDS)
- Feature extraction techniques: Not mentioned
- Data processing pipelines: Not mentioned
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Co-citation proximity analysis–based systematic review (CPASR)","- Citation analysis methods: Co-citation proximity analysis–based systematic review (CPASR)
- Content-based matching techniques: Exploratory factor analysis (EFA), hierarchical cluster analysis (HCA), k-means clustering (KMC), multidimensional scaling (MDS)
- Relevance ranking approaches: Selection of 44 highly influential articles out of 473 relevant research studies","- Integration difficulties: Scattered R&D efforts due to lack of knowledge structure examination.
- Evaluation methodology limitations: Challenges in evaluating and analyzing vast literature.
- Data privacy and security issues: Challenges in ensuring data privacy and security for BIoT systems.
- System security theories: Challenges in developing robust system security theories for BIoT.
- Applied security strategies: Challenges in implementing effective security strategies for using blockchain with IoT.","- Co-citation proximity analysis–based systematic review (CPASR)
- Combination of exploratory factor analysis (EFA), hierarchical cluster analysis (HCA), k-means clustering (KMC), and multidimensional scaling (MDS)",Not mentioned (the abstract does not provide any performance results or metrics),"- Dataset size and characteristics: 44 highly influential articles out of 473 relevant research studies
- Data collection method: Co-citation proximity analysis–based systematic review (CPASR)
- Other information: Not mentioned","- Target research disciplines or fields: Blockchain and Internet of Things (BIoT)
- Specific literature review tasks addressed: Co-citation proximity analysis-based systematic review (CPASR) of correlations between influential articles
- Types of academic documents processed: Research articles
- User types and requirements: Researchers and industrial practitioners
- Integration with research workflows: Not mentioned
- Commercial vs. academic applications: Academic","  - ""This study investigates the intellectual core of BIoT through a co-citation proximity analysis–based systematic review (CPASR) of the correlations between 44 highly influential articles out of 473 relevant research studies.""
  - ""Our findings indicate that there are nine categories in the intellectual core of BIoT: (1) data privacy and security for BIoT systems, (2) models and applications of BIoT, (3) system security theories for BIoT, (4) frameworks for BIoT deployment, (5) the fusion of BIoT with emerging methods and technologies, (6) applied security strategies for using blockchain with the IoT, (7) the design and development of industrial BIoT, (8) establishing trust through BIoT and (9) the BIoT ecosystem.Originality/valueWe""
  - ""Subsequently, we apply a series of statistical analyses, including exploratory factor analysis (EFA), hierarchical cluster analysis (HCA), k-means clustering (KMC) and multidimensional scaling (MDS) to establish the intellectual core.FindingsOur""",,"- The abstract discusses the use of a co-citation proximity analysis-based systematic review (CPASR) and various statistical analyses (EFA, HCA, KMC, MDS) to identify the intellectual core of BIoT. These are methodological approaches rather than technical architecture components.
- The abstract does not mention any specific AI/ML frameworks, system architecture components, technical infrastructure, or integration with existing academic databases or platforms.
- The focus of the study is on identifying the intellectual core of BIoT, which involves categorizing research areas rather than detailing technical architecture or system components.
- The abstract does not provide information on the technical architecture or system components as requested in the question.","  - ""Subsequently, we apply a series of statistical analyses, including exploratory factor analysis (EFA), hierarchical cluster analysis (HCA), k-means clustering (KMC) and multidimensional scaling (MDS) to establish the intellectual core.FindingsOur""
  - ""This study investigates the intellectual core of BIoT through a co-citation proximity analysis–based systematic review (CPASR) of the correlations between 44 highly influential articles out of 473 relevant research studies.""",,"- The abstract mentions the use of a ""co-citation proximity analysis–based systematic review (CPASR)"" as the primary method for investigating the intellectual core of BIoT. This suggests that the study uses a systematic review approach that involves analyzing the co-citation patterns among influential articles.
- The abstract lists several statistical analyses used in the study: exploratory factor analysis (EFA), hierarchical cluster analysis (HCA), k-means clustering (KMC), and multidimensional scaling (MDS). These are specific technical implementation details related to data analysis and clustering.
- The abstract does not provide specific details on feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or technical workflow beyond the mentioned statistical analyses.","  - ""This study investigates the intellectual core of BIoT through a co-citation proximity analysis–based systematic review (CPASR) of the correlations between 44 highly influential articles out of 473 relevant research studies.""
  - ""Subsequently, we apply a series of statistical analyses, including exploratory factor analysis (EFA), hierarchical cluster analysis (HCA), k-means clustering (KMC) and multidimensional scaling (MDS) to establish the intellectual core.FindingsOur""",,"- The abstract mentions the use of ""co-citation proximity analysis–based systematic review (CPASR)"" as a method to investigate the intellectual core of BIoT. This suggests that citation analysis is a key technique used in the paper.
- The application of ""exploratory factor analysis (EFA), hierarchical cluster analysis (HCA), k-means clustering (KMC) and multidimensional scaling (MDS)"" indicates the use of statistical methods to analyze and structure the intellectual core. These methods can be related to content-based matching techniques and metadata extraction and utilization.
- The focus on identifying ""44 highly influential articles out of 473 relevant research studies"" implies a relevance ranking approach, as the authors are selecting a subset of papers based on their influence.","  - ""Although several authors have conducted literature reviews on the topic, none have examined the development of the knowledge structure of BIoT, resulting in scattered research and development (R&D) efforts.Design/methodology/approachThis""
  - ""Subsequently, we apply a series of statistical analyses, including exploratory factor analysis (EFA), hierarchical cluster analysis (HCA), k-means clustering (KMC) and multidimensional scaling (MDS) to establish the intellectual core.FindingsOur""
  - ""This study investigates the intellectual core of BIoT through a co-citation proximity analysis–based systematic review (CPASR) of the correlations between 44 highly influential articles out of 473 relevant research studies.""
  - ""Our findings indicate that there are nine categories in the intellectual core of BIoT: (1) data privacy and security for BIoT systems, (2) models and applications of BIoT, (3) system security theories for BIoT, (4) frameworks for BIoT deployment, (5) the fusion of BIoT with emerging methods and technologies, (6) applied security strategies for using blockchain with the IoT, (7) the design and development of industrial BIoT, (8) establishing trust through BIoT and (9) the BIoT ecosystem.Originality/valueWe""",,"- The abstract mentions that previous literature reviews have not examined the development of the knowledge structure of BIoT, leading to scattered R&D efforts. This suggests a challenge in integrating and organizing existing research, which could be considered an integration difficulty.
- The study uses a co-citation proximity analysis and various statistical analyses to establish the intellectual core of BIoT. This implies that there might be challenges in evaluating and analyzing the vast amount of literature, which could relate to evaluation methodology limitations.
- The categories identified in the intellectual core of BIoT include data privacy and security, system security theories, and applied security strategies. These areas often involve technical challenges such as algorithm performance issues and data quality or availability problems.
- The abstract does not explicitly mention specific technical challenges like scalability challenges, computational resource constraints, or technical bottlenecks. However, the focus on establishing trust and the BIoT ecosystem suggests potential challenges in these areas.","  - ""This study investigates the intellectual core of BIoT through a co-citation proximity analysis–based systematic review (CPASR) of the correlations between 44 highly influential articles out of 473 relevant research studies.""
  - ""We use the CPASR method to examine the intellectual core of BIoT, which is an under-researched and topical area.""
  - ""Subsequently, we apply a series of statistical analyses, including exploratory factor analysis (EFA), hierarchical cluster analysis (HCA), k-means clustering (KMC) and multidimensional scaling (MDS) to establish the intellectual core.FindingsOur""
  - ""The paper also provides a structural framework for investigating BIoT research that may be applicable to other knowledge domains.""",,"- The abstract mentions the use of a ""co-citation proximity analysis–based systematic review (CPASR)"" as a method to investigate the intellectual core of BIoT. This is a novel approach in the context of literature reviews, as it uses co-citation analysis to identify influential articles and their relationships.
- The application of ""exploratory factor analysis (EFA), hierarchical cluster analysis (HCA), k-means clustering (KMC) and multidimensional scaling (MDS)"" suggests a hybrid approach to analyzing the intellectual core, which is innovative in the context of systematic reviews.
- The abstract does not explicitly mention new algorithmic contributions, creative problem-solving methods, or novel feature engineering techniques. However, the use of CPASR and the combination of statistical analyses can be considered as innovative solutions in the context of literature reviews.
- The abstract does not provide specific details on new algorithms, technical workarounds, or original system design elements. The focus is on the methodological approach rather than specific technical innovations.","  - ""Subsequently, we apply a series of statistical analyses, including exploratory factor analysis (EFA), hierarchical cluster analysis (HCA), k-means clustering (KMC) and multidimensional scaling (MDS) to establish the intellectual core.FindingsOur""
  - ""This study investigates the intellectual core of BIoT through a co-citation proximity analysis–based systematic review (CPASR) of the correlations between 44 highly influential articles out of 473 relevant research studies.""
  - ""Our findings indicate that there are nine categories in the intellectual core of BIoT: (1) data privacy and security for BIoT systems, (2) models and applications of BIoT, (3) system security theories for BIoT, (4) frameworks for BIoT deployment, (5) the fusion of BIoT with emerging methods and technologies, (6) applied security strategies for using blockchain with the IoT, (7) the design and development of industrial BIoT, (8) establishing trust through BIoT and (9) the BIoT ecosystem.Originality/valueWe""",,"- The abstract discusses the methodology used to investigate the intellectual core of BIoT, which includes a co-citation proximity analysis and various statistical analyses. However, it does not provide any specific performance results or metrics such as accuracy, precision, recall, F1-scores, processing speed, efficiency metrics, comparison with baseline methods, user satisfaction, system reliability, or scalability test results.
- The focus of the study is on identifying the intellectual core of BIoT and categorizing it into nine areas, rather than evaluating the performance of a specific system or method.
- The abstract does not mention any quantitative or qualitative performance outcomes related to the methods used in the study.","  - ""This study investigates the intellectual core of BIoT through a co-citation proximity analysis–based systematic review (CPASR) of the correlations between 44 highly influential articles out of 473 relevant research studies.""",,"- The abstract mentions that the study uses a ""co-citation proximity analysis–based systematic review (CPASR)"" to investigate the intellectual core of BIoT. This suggests that the data sources are primarily academic articles.
- The abstract specifies that the study analyzed ""44 highly influential articles out of 473 relevant research studies."" This indicates the dataset size and characteristics, as it involves a selection of influential articles from a larger pool.
- The abstract does not provide specific information about the academic databases used (e.g., Scopus, Web of Science), data collection methods, training/validation/test data splits, data preprocessing and cleaning approaches, or external knowledge bases or ontologies used.
- The focus of the study is on analyzing the intellectual core of BIoT through a systematic review of academic articles, rather than using specific datasets or external knowledge bases.","  - ""the fusion of blockchain and the Internet of Things (BIoT) has drawn considerable attention from researchers and industrial practitioners and is regarded as a future trend in technological development.""
  - ""We use the CPASR method to examine the intellectual core of BIoT, which is an under-researched and topical area.""
  - ""This study investigates the intellectual core of BIoT through a co-citation proximity analysis–based systematic review (CPASR) of the correlations between 44 highly influential articles out of 473 relevant research studies.""
  - ""The paper also provides a structural framework for investigating BIoT research that may be applicable to other knowledge domains.""",,"- The abstract discusses the fusion of blockchain and the Internet of Things (BIoT), indicating a focus on these technological fields as the target research disciplines.
- The study uses a co-citation proximity analysis-based systematic review (CPASR) to examine the intellectual core of BIoT, which suggests that the specific literature review task addressed is the analysis of correlations between influential articles.
- The abstract mentions the processing of ""44 highly influential articles out of 473 relevant research studies,"" indicating that the types of academic documents processed are research articles.
- The study is aimed at researchers and industrial practitioners, suggesting that these are the user types and requirements being addressed.
- The abstract does not explicitly mention integration with research workflows or whether the application is commercial or academic, but the focus on research studies suggests an academic application.
- The abstract does not provide specific details on user types beyond researchers and industrial practitioners, nor does it mention specific commercial applications."
"A review on method entities in the academic literature: extraction, evaluation, and application","Yuzhuo Wang, Chengzhi Zhang, Kai Li",10.1007/s11192-022-04332-7,https://doi.org/10.1007/s11192-022-04332-7,Scientometrics,19,2022,"- Main AI/ML frameworks used: NLP, machine learning models (HMM, CRF, ME, SVM)
- System architecture components: Method entity retrieval platforms, databases for storing method entities
- Technical infrastructure: Computational resources for processing large-scale data
- Integration with existing academic databases or platforms: Method entity corpora and databases
- Overall system design approach: Large-scale extraction, evaluation, and application of method entities","- Algorithms and models used: HMM, CRF, ME, SVM, CNN, LSTM, Bi-LSTM, BERT, SciBERT
- Feature extraction techniques: Choosing better features of method entities
- Data processing pipelines: Manual annotation, rule-based extraction, statistical machine learning, deep learning
- Training methodologies: Supervised and semi-supervised machine learning
- Preprocessing steps: Not explicitly mentioned
- Technical workflow or methodology: Combination of manual annotation, rule-based methods, and machine learning approaches","Search and retrieval algorithms, Citation analysis methods, Content-based matching techniques, Metadata extraction and utilization, Recommendation system approaches, Expert/authority identification methods","Algorithm performance issues, Data quality or availability problems, Scalability challenges, Evaluation methodology limitations, Computational resource constraints, Technical bottlenecks related to semantic disambiguation","- New algorithmic contributions: Use of deep learning models like Bi-LSTM and BERT for method entity extraction.
- Hybrid or ensemble approaches: Combining CNN with CRF and Bi-LSTM for improved extraction accuracy.
- Novel feature engineering techniques: Focus on better feature selection for method entities.
- Innovative evaluation metrics: Use of BERT and SciBERT for method entity extraction.
- Technical workarounds for known problems: Reducing reliance on complex feature engineering through deep learning models.","- Accuracy, precision, recall, F1-scores: Various tables provide specific performance metrics for different extraction methods, such as manual annotation, rule-based methods, and machine learning approaches. For example, Table 1 shows precision and recall for manual annotation methods.
- Processing speed and efficiency metrics: Not mentioned.
- Comparison with baseline methods: Not explicitly mentioned.
- User satisfaction or usability results: Not mentioned.
- System reliability and robustness measures: Not mentioned.
- Scalability test results: Not mentioned.","- Academic databases used: Web of Science, Google Scholar, Springer Link, ScienceDirect
- Dataset sizes and characteristics: Datasets are in PDF and XML formats; sizes vary from hundreds to tens of thousands
- Data collection methods: Searching with specific keywords; screening relevant documents
- Training/validation/test data splits: Not mentioned
- Data preprocessing and cleaning approaches: Not mentioned
- External knowledge bases or ontologies used: ACL ARC, PLOS, PubMed Central","- Target research disciplines or fields: Computer science, biology, medicine
- Specific literature review tasks addressed: Identifying methods in academic papers, evaluating matching degree between methods and user queries, assessing scientific influence
- Types of academic documents processed: Full-text articles, abstracts, bibliographic content
- User types and requirements: Scholars seeking to understand existing methods, select appropriate methods for research tasks, propose new methods
- Integration with research workflows: Evaluating scientific influence, providing learning resources
- Commercial vs. academic applications: Focus on academic applications such as method entity retrieval platforms and corpora","  - ""With the popularity of machine learning in natural language processing, this approach is also used to extract named entities from academic texts.""
  - ""The first common optimization strategy is to choose better features of method entities.""
  - ""The third optimization strategy is reducing the investment in the extraction process.""
  - ""The method entity retrieval platforms are built on method entities and related resources and information, such as tasks, data sets, and academic documents.""
  - ""The first type of platform provides users with learning resources related to method entities,""
  - ""The second type of platform is constructed based on academic literature, providing users with the original sentences where method entities are cited or mentioned.""
  - ""The third type of platform display the ranks extracted method entities based on their performance evaluated by metrics (accuracy, recall, F-value) in different task.""
  - ""The large-scale extraction, evaluation, and application of method entities from academic literature can help scholars to understand method entities in specific fields comprehensively.""
  - ""the method entity corpus and database. These corpora contain at least a series of independent method entities, and the richer corpus will also provide the sentence or article content where the method entities are located.""",,"- The paper discusses the use of machine learning, particularly natural language processing (NLP), for extracting named entities from academic texts. This indicates the use of AI/ML frameworks like NLP and machine learning models such as HMM, CRF, ME, and SVM for sequence tagging tasks.
- The paper mentions the optimization strategies for machine learning models, which include feature selection and reducing the investment in the extraction process, suggesting a focus on improving model efficiency and performance.
- The system architecture includes method entity retrieval platforms that integrate with academic databases and provide users with resources related to method entities. These platforms are categorized into three types: learning resources, academic literature-based platforms, and performance evaluation platforms.
- The technical infrastructure likely involves databases for storing method entities and related information, as well as computational resources for processing large-scale data.
- The integration with existing academic databases or platforms is implied through the use of method entity corpora and databases, which are essential for the extraction and evaluation processes.
- The overall system design approach focuses on large-scale extraction, evaluation, and application of method entities to aid scholars in understanding and utilizing methods in specific fields.","  - ""Manual annotation is the task of reading a particular preselected document and providing additional information in the form of the so-called annotations, which can occur at any level of a linguistic component, i.e., document, paragraph, sentence, phrase, word, or character""
  - ""Rule-based method entity extraction is based on textual rules of how method entities are mentioned.""
  - ""With the popularity of machine learning in natural language processing, this approach is also used to extract named entities from academic texts.""
  - ""The first common optimization strategy is to choose better features of method entities. Features are the properties and attributes of textual objects in a computational model, which are further used by the learning methods for generating a model""
  - ""The third optimization strategy is reducing the investment in the extraction process. To be more specific, this strategy uses semi-supervised machine learning approaches to reduce the need for expensive training data produced by human annotators in supervised machine learning models""
  - ""With the development of technology, the deep learning methods derived from the neural network model are becoming a hot topic in machine learning.""
  - ""Works have been done to compare the deep learning model with statistical machine learning models in method entity extraction""
  - ""The comparison between SVM, CNN, LSTM, RCNN, Bi-LSTM indicates that the character vector-based Bi-LSTM model achieved better performance in identifying a series of scientific entities, including many method entities covered by our definition""
  - ""the CNN model is combined with CRF + bi LSTM to extract method entities""
  - ""Yao et al. (2020) built the CNN+ Bi-LSTM+CRF model to extract method entities from AI academic papers.""
  - ""Jiang et al. (2019) used BERT to extract software entities from full-text papers and achieved an F-1 value of 85.44%.""
  - ""Zheng et al. (2021) chose another special BERT trained on scientific text, SciBERT, to extract general method entities and achieved an F-1 value of 81.4%.""","  - ""(Page 14, Table 1) | ID | Extraction methods  | Rules/Features/ Properties            | Types of entity                              | Data set                                                                 | Results                                                                                      | Precision (%) | Recall (%) | F-value (%) |\n|----|---------------------|---------------------------------------|----------------------------------------------|-------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|---------------|------------|-------------|\n|    |                     |                                       |                                              |                                                                         | Agreement (%)                                                                                |               |            |             |""
  - ""(Page 15, Table 1) |    |                         |                                                                 | Methodology terms                                                                 | Method and results sections of 3,720 papers from the domain of sleep disorders |       |       |       |\n|----|-------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------------------------|--------------------------------------------------------------------------------|-------|-------|-------|\n| 12 | rule-based method       | Part-of-speech, lexicon, syntax                                 |                                                                                   |                                                                                | 70.00 | 77.00 | 75.51 |""
  - ""(Page 16, Table 1) | Sentences classification: Rule-based method |                                                                 | Method, dataset, resource, implementation |                                                                                                           |       |       |       |\n|---------------------------------------------|-----------------------------------------------------------------|-------------------------------------------|-----------------------------------------------------------------------------------------------------------|-------|-------|-------|\n|                                             |                                                                 |                                           | 6500 pairs of sentences from 189 different journals and 2000 papers in PubMed                             | 85.4  | 100   | 91.89 |""","- The paper discusses various methods for extracting method entities from academic literature, including manual annotation, rule-based extraction, statistical machine learning, and deep learning.
- Manual annotation involves human annotators labeling method entities in documents, which is time-consuming but provides accurate baseline data.
- Rule-based methods use predefined rules to identify method entities, which can be efficient but may not capture all entities due to the complexity of language.
- Statistical machine learning approaches include techniques like HMM, CRF, ME, and SVM, which are used for sequence tagging and classification tasks.
- Deep learning models such as CNN, LSTM, Bi-LSTM, and BERT are used for method entity extraction, with BERT and SciBERT showing high performance in extracting software and general method entities.
- Feature extraction techniques involve selecting relevant features from textual data to improve model performance.
- Semi-supervised machine learning is used to reduce the need for large amounts of labeled training data.
- The tables provide specific examples of algorithms and models used in various studies, along with their performance metrics.","  - ""The academic search engine with the method search function needs to identify the methods in many academic papers and evaluate the matching degree between each method and the user's query.""
  - ""Scientific evaluation can also be carried out based on method use information, which can be used as a new indicator to evaluate the scientific influence of academic publications and researchers.""
  - ""The method entity retrieval platforms are built on method entities and related resources and information, such as tasks, data sets, and academic documents.""
  - ""The first type of platform provides users with learning resources related to method entities, and SAGE

Research Methods3 is one of the representatives.""
  - ""The second type of platform is constructed based on academic literature, providing users with the original sentences where method entities are cited or mentioned.""
  - ""The third type of platform display the ranks extracted method entities based on their performance evaluated by metrics (accuracy, recall, F-value) in different task.""
  - ""The At the same time, according to table 2, it can be seen that prosperities and context information are added to analyze the influence of different methods in different fields,""
  - ""Method entities create value in both academia and industry, especially in the data-driven paradigm of knowledge production.""
  - ""Applications include the following categories, the corpus of method entity, and the method entity retrieval platform.""",,"- The paper discusses the importance of academic search engines with method search functions, which implies the use of search and retrieval algorithms to identify methods in academic papers and match them with user queries.
- The mention of scientific evaluation based on method use information suggests the use of citation analysis methods to evaluate the influence of academic publications.
- The description of method entity retrieval platforms indicates the use of content-based matching techniques and metadata extraction to provide users with relevant information about method entities.
- The platforms mentioned, such as SAGE Research Methods and IBM Science Summarizer, imply the use of recommendation system approaches to provide users with learning resources and summaries based on their queries.
- The paper highlights the role of method entities in knowledge production, suggesting the use of expert/authority identification methods to determine influential methods and researchers.","  - ""The traditional approach of manually exploring the related publications to find methods is becoming challenging because most scholars have never been able to keep completely up-to-date with publications considering the unending increase in quantity and diversity of research within their areas of focus""
  - ""The performance of method entity extraction needs to be improved""
  - ""The singular evaluation of method entities""
  - ""The source of method entities is limited""
  - ""the performance of the extraction model is not as good as that in the general NER.""
  - ""the semantic disambiguation of method entities.""
  - ""The combination of these contextual information and method entity characteristics may be a good choice for exploring the reason.""
  - ""the evaluation of method entities' influence can also be conducted by bibliometric approaches and altmetric approaches,""
  - ""The current evaluation treats every mention of the method equally, but semantics can help scholars judge the difference in each mention and the difference in influence between method entities with the same influence.""
  - ""The large-scale extraction, evaluation, and application of method entities from academic literature can help scholars to understand method entities in specific fields comprehensively.""
  - ""the current popular deep learning models often require large-scale training data, which makes less attention on method entity extraction, and the performance of the extraction model is not as good as that in the general NER.""
  - ""the public data set of method entities has a small number and a single type.""
  - ""the performance of method entity recognition and subsequent evaluation.""",,"- The paper highlights several technical challenges related to method entity extraction and evaluation in academic literature.
- **Algorithm Performance Issues**: The performance of method entity extraction models is not as good as general Named Entity Recognition (NER) systems, indicating a need for improvement in algorithmic performance.
- **Data Quality or Availability Problems**: There is a limited source of method entities, primarily from natural sciences, and a small number of public datasets, which affects the quality and availability of data for training models.
- **Scalability Challenges**: The traditional manual approach to exploring publications is not scalable due to the increasing volume and diversity of research.
- **Integration Difficulties**: The paper does not explicitly mention integration difficulties, but the need for combining contextual information and method entity characteristics suggests potential challenges in integrating diverse data sources.
- **Evaluation Methodology Limitations**: The current evaluation methods treat all mentions equally, lacking semantic differentiation, which limits the ability to accurately assess the influence of method entities.
- **Computational Resource Constraints**: The need for large-scale training data for deep learning models implies significant computational resource requirements.
- **Technical Bottlenecks or Failure Points**: Semantic disambiguation of method entities is identified as a critical issue, indicating a technical bottleneck in accurately identifying and classifying method entities.","  - ""The first common optimization strategy is to choose better features of method entities. Features are the properties and attributes of textual objects in a computational model, which are further used by the learning methods for generating a model""
  - ""The third optimization strategy is reducing the investment in the extraction process. To be more specific, this strategy uses semi-supervised machine learning approaches to reduce the need for expensive training data produced by human annotators in supervised machine learning models""
  - ""With the development of technology, the deep learning methods derived from the neural network model are becoming a hot topic in machine learning. Compared with the statistical machine learning models, deep learning models no longer rely on complex features, significantly saving the workforce and time spent on feature engineering.""
  - ""The comparison between SVM, CNN, LSTM, RCNN, Bi-LSTM indicates that the character vector-based Bi-LSTM model achieved better performance in identifying a series of scientific entities, including many method entities covered by our definition""
  - ""the CNN model is combined with CRF + bi LSTM to extract method entities""
  - ""Yao et al. (2020) built the CNN+ Bi-LSTM+CRF model to extract method entities from AI academic papers.""
  - ""Jiang et al. (2019) used BERT to extract software entities from full-text papers and achieved an F-1 value of 85.44%.""
  - ""Zheng et al. (2021) chose another special BERT trained on scientific text, SciBERT, to extract general method entities and achieved an F-1 value of 81.4%.""",,"- The paper discusses several innovative solutions in the context of method entity extraction and evaluation. These include new algorithmic contributions such as the use of deep learning models like Bi-LSTM and BERT, which are novel in the context of method entity extraction.
- The integration of different extraction models, such as combining CNN with CRF and Bi-LSTM, represents a hybrid approach that is innovative in this field.
- The use of semi-supervised machine learning approaches to reduce the need for extensive training data is a creative problem-solving method that addresses the challenge of limited labeled data.
- The paper highlights novel feature engineering techniques by focusing on better feature selection for method entities, which is crucial for improving extraction accuracy.
- The use of BERT and SciBERT for method entity extraction is an innovative evaluation metric as these models have shown superior performance compared to traditional methods.
- The paper also discusses technical workarounds for known problems, such as reducing the reliance on complex feature engineering through deep learning models.","  - ""The first common optimization strategy is to choose better features of method entities. Features are the properties and attributes of textual objects in a computational model, which are further used by the learning methods for generating a model (Goyal, Gupta & Kumar, 2018).""
  - ""The third optimization strategy is reducing the investment in the extraction process. To be more specific, this strategy uses semi-supervised machine learning approaches to reduce the need for expensive training data produced by human annotators""
  - ""The success inspired scholars to continue to propose new optimization strategies to improve the performance of classic machine learning approaches and obtain better extraction results.""
  - ""The first type of platform provides users with learning resources related to method entities, and SAGE

Research Methods3 is one of the representatives.""
  - ""The second type of platform is constructed based on academic literature, providing users with the original sentences where method entities are cited or mentioned.""
  - ""The third type of platform display the ranks extracted method entities based on their performance evaluated by metrics (accuracy, recall, F-value) in different task.""
  - ""The large-scale extraction, evaluation, and application of method entities from academic literature can help scholars to understand method entities in specific fields comprehensively.""
  - ""The performance of method entity extraction needs to be improved""
  - ""The singular evaluation of method entities""
  - ""the evaluation of method entities' influence can also be conducted by bibliometric approaches and altmetric approaches, which means scholars measure method entities by different kinds of frequency, and then get the rank of methods in a specific field or task.""
  - ""The combination of these contextual information and method entity characteristics may be a good choice for exploring the reason.""
  - ""the evaluation of method entities can bring theoretical contributions to the use of methods and the development of the field.""","  - ""(Page 14, Table 1) | ID | Extraction methods  | Rules/Features/ Properties            | Types of entity                              | Data set                                                                 | Results                                                                                      | Precision (%) | Recall (%) | F-value (%) |\n|----|---------------------|---------------------------------------|----------------------------------------------|-------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|---------------|------------|-------------|\n|    |                     |                                       |                                              |                                                                         | Agreement (%)                                                                                |               |            |             |""
  - ""(Page 15, Table 1) |    |                         |                                                                 | Methodology terms                                                                 | Method and results sections of 3,720 papers from the domain of sleep disorders |       |       |       |\n|----|-------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------------------------|--------------------------------------------------------------------------------|-------|-------|-------|\n| 12 | rule-based method       | Part-of-speech, lexicon, syntax                                 |                                                                                   |                                                                                | 70.00 | 77.00 | 75.51 |""
  - ""(Page 16, Table 1) | Sentences classification: Rule-based method |                                                                 | Method, dataset, resource, implementation |                                                                                                           |       |       |       |\n|---------------------------------------------|-----------------------------------------------------------------|-------------------------------------------|-----------------------------------------------------------------------------------------------------------|-------|-------|-------|\n|                                             |                                                                 |                                           | 6500 pairs of sentences from 189 different journals and 2000 papers in PubMed                             | 85.4  | 100   | 91.89 |""
  - ""(Page 17, Table 1) |    | Models                  | Process (method), task, material                                                                 | Method                                                                                      | Dataset, metric, task, method                                      | Software                                                                                      |        |        |        |\n|----|-------------------------|--------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|--------------------------------------------------------------------|------------------------------------------------------------------------------------------------|--------|--------|--------|\n| 23 | CRF                     |                                                                                                  |                                                                                             |                                                                    |                                                                                                | 91.32  | 69.02  | 78.62  |""
  - ""(Page 21, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 23, Table 1) | Corpus         | Year | Field                                      | Scale                                           | URL                                                                 |\n|----------------|------|--------------------------------------------|-------------------------------------------------|---------------------------------------------------------------------|\n| ACL RD-TEC     | 2014 | Computer science                           | 10,922 papers with 13,832 technical terms       | https://github.com/languagerecipes/the-acl-rd-tec                   |""
  - ""(Page 25, Table 1) | Platform               | Year | Developer    | Function                                                                                                                                                                                                 | Field               | Scale                                                                                     |\n|------------------------|------|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------|-------------------------------------------------------------------------------------------|\n| SAGE Research Methods  | 2014 | SAGE press   | According to the search terms, SAGE supports users searching sociological research methods, and then feedback books, reference books, manuals, journal literature, cases, teaching videos, and university survey data. | Social science      | More than 1,000 monographs and papers, 770 method terms                                  |""
  - ""(Page 39, Table 1) Below is a rendering of the page up to the first error.""","- The paper discusses various strategies for optimizing the performance of method entity extraction, including choosing better features and reducing the need for expensive training data.
- It mentions the use of semi-supervised machine learning approaches to improve efficiency.
- The paper highlights the importance of evaluating method entities based on metrics such as accuracy, recall, and F-value.
- The tables included in the paper provide specific performance results for different extraction methods, including precision, recall, and F1-scores.
- The paper discusses the limitations and challenges in method entity extraction and evaluation, indicating areas for improvement.
- The platforms mentioned in the paper provide resources and tools for method entity extraction and evaluation, which can impact user satisfaction and usability.
- The paper does not explicitly mention processing speed and efficiency metrics, user satisfaction or usability results, system reliability and robustness measures, or scalability test results.","  - ""Following the above definition, we listed some nouns related to the research objects of this review, including ""method entity, method knowledge element, method term, method name, technology, software, algorithm, model, data set and so on"", some words about the research field, such as scientific research, academic literature, academic papers et al., and words about the research topic, including extract, assess, evaluate, apply and corresponding nouns. The three types of words were combined as search terms and searched on the Web of Science, Google Scholar, Springer Link, and ScienceDirect.""
  - ""By browsing the title and abstract of each article, we screened out relevant documents from the search results, and searched for related literature in the reference list when reviewing obtained articles. Finally, we obtained a total of 69 related publications about extracting method entities from academic literature, evaluating the influence of the method entities obtained from academic literature, and the application platforms of these method entities.""
  - ""Datasets serve as the primary ingredient in method entity extraction, and they are the resources of various method entities. Commonly used data sets are divided into two formats: PDF and XML.""
  - ""Most databases provide full-text content of academic literature. ACL ARC, PLOS, and PubMed Central provide full-text data in XML format that machines can process directly.""
  - ""The appendix shows the popular databases that provide data set for existing method entity extraction.""
  - ""The currently available method entity corpora are mainly concentrated in computer science, and the amount of data in different corpora varies from hundreds to tens of thousands.""
  - ""Structured full-text content and labeled entities provide training data for automatic method entity extraction, but we are unsure whether these corpora are of high quality.""
  - ""Whether the method entity corpus of a specific field can be used for entity extraction in other fields still needs further discussion.""
  - ""The large-scale extraction, evaluation, and application of method entities from academic literature can help scholars to understand method entities in specific fields comprehensively.""","  - ""(Page 23, Table 1) | Corpus         | Year | Field                                      | Scale                                           | URL                                                                 |\n|----------------|------|--------------------------------------------|-------------------------------------------------|---------------------------------------------------------------------|\n| ACL RD-TEC     | 2014 | Computer science                           | 10,922 papers with 13,832 technical terms       | https://github.com/languagerecipes/the-acl-rd-tec                   |""
  - ""(Page 39, Table 1) Below is a rendering of the page up to the first error.""","- The paper mentions the use of academic databases such as Web of Science, Google Scholar, Springer Link, and ScienceDirect for searching relevant publications.
- The dataset sizes and characteristics are not explicitly detailed in terms of specific numbers or types, but it is mentioned that datasets are in PDF and XML formats.
- The data collection method involved searching with specific keywords and screening relevant documents.
- There is no mention of specific training/validation/test data splits or detailed data preprocessing and cleaning approaches.
- The paper refers to external knowledge bases or ontologies such as ACL ARC, PLOS, and PubMed Central, which provide full-text content in XML format.
- The tables at the end of the paper likely contain more detailed information about datasets and knowledge resources, but without access to the content, we cannot provide specific details.","  - ""The authors describe details of the method in the abstract and body text, and key entities in academic literature reflecting names of the method are called method entities.""
  - ""Exploring diverse method entities in a tremendous amount of academic literature helps scholars understand existing methods, select the appropriate method for research tasks, and propose new methods.""
  - ""The academic search engine with the method search function needs to identify the methods in many academic papers and evaluate the matching degree between each method and the user's query.""
  - ""Scientific evaluation can also be carried out based on method use information, which can be used as a new indicator to evaluate the scientific influence of academic publications and researchers.""
  - ""In scientific research, academic literature is an important resource for obtaining methods, and analyzing as much literature as possible can ensure the richness of the obtained methods.""
  - ""The large-scale extraction, evaluation, and application of method entities from academic literature can help scholars to understand method entities in specific fields comprehensively.""
  - ""Applications include the following categories, the corpus of method entity, and the method entity retrieval platform.""
  - ""A large number of method entities obtained from academic literature are important resources for the method entity corpus and database.""
  - ""The method entity retrieval platforms are built on method entities and related resources and information, such as tasks, data sets, and academic documents.""
  - ""The first type of platform provides users with learning resources related to method entities, and SAGE

Research Methods3 is one of the representatives.""
  - ""The second type of platform is constructed based on academic literature, providing users with the original sentences where method entities are cited or mentioned.""
  - ""The third type of platform display the ranks extracted method entities based on their performance evaluated by metrics (accuracy, recall, F-value) in different task.""
  - ""Future retrieval platforms should first try to mine all method entities involved in different tasks, and then analyze the relationship between method entities and other entities.""
  - ""Users can obtain the landscape of solving various tasks, including the steps to solve the task, the methods used in each step, and the final results.""","  - ""(Page 25, Table 1) | Platform               | Year | Developer    | Function                                                                                                                                                                                                 | Field               | Scale                                                                                     |\n|------------------------|------|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------|-------------------------------------------------------------------------------------------|\n| SAGE Research Methods  | 2014 | SAGE press   | According to the search terms, SAGE supports users searching sociological research methods, and then feedback books, reference books, manuals, journal literature, cases, teaching videos, and university survey data. | Social science      | More than 1,000 monographs and papers, 770 method terms                                  |""","- The paper discusses the extraction and evaluation of method entities from academic literature, which is a key component of literature review tasks.
- The target research disciplines or fields include computer science, biology, and medicine, as indicated by the databases and corpora mentioned.
- Specific literature review tasks addressed include identifying methods in academic papers, evaluating the matching degree between methods and user queries, and assessing scientific influence.
- Types of academic documents processed include full-text articles, abstracts, and bibliographic content.
- User types and requirements include scholars seeking to understand existing methods, select appropriate methods for research tasks, and propose new methods.
- Integration with research workflows involves using method entities to evaluate scientific influence and provide learning resources.
- Commercial vs. academic applications are not explicitly distinguished, but the focus is on academic applications such as method entity retrieval platforms and corpora."
RecCite: A Hybrid Approach to Recommend Potential Papers,"Pratyush Yadav, Nikhila Remala, N. Pervin",10.1109/BigData47090.2019.9006220,https://doi.org/10.1109/BigData47090.2019.9006220,2019 IEEE International Conference on Big Data (Big Data),6,2019,Not mentioned (the abstract does not provide specific details on the technical architecture or system components),"Not mentioned (the abstract does not provide specific technical implementation details such as algorithms, models, feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or technical workflow)","- Citation analysis methods: Link-based approaches on citation networks
- Content-based matching techniques: Semantic similarity
- Recommendation system approaches: Hybrid approach ""RecCite"" blending popularity with semantic similarity
- Relevance ranking approaches: Semantic similarity for relevance","- Algorithm performance issues: Current methods do not consider the semantics of papers.
- Data quality or availability problems: Information overload due to the growth of publications.
- Scalability challenges: Achieving online response time is a significant concern.","- New algorithmic contributions: Hybrid approach ""RecCite"" that blends paper popularity with semantic similarity.
- Creative problem-solving methods: Integration of semantic context to improve relevance.
- Hybrid or ensemble approaches: ""RecCite"" combines popularity and semantic similarity.
- Technical workarounds for known problems: Systematic evaluation against a baseline method using a public dataset.","- Precision: 37.66% improvement
- Recall: 20.14% improvement
- Response Time: 97.24% improvement
- Comparison with baseline method: Outperforms baseline with specified improvements
- Scalability: Improved response time indicates good scalability","- Dataset used: Arnet-Miner data set
- No information on academic databases, dataset sizes, data collection methods, data splits, or data preprocessing
- No external knowledge bases or ontologies mentioned","- Target research disciplines or fields: Special Interest Groups (SIGs)
- Specific literature review tasks addressed: Efficient academic paper recommendation
- Types of academic documents processed: Academic papers
- User types and requirements: Not mentioned
- Integration with research workflows: Not mentioned
- Commercial vs. academic applications: Academic","  - ""The state-of-the-art methods in this domain primarily employ link-based approaches on citation network and do not consider the semantics (context) of papers being recommended, although the later one largely accounts for the relevance of recommendations.""
  - ""The proposed approach has been systematically evaluated with state-of-the-art baseline method using the publicly available Arnet-Miner data set.""
  - ""In this context we propose a hybrid approach “RecCite” that blends the popularity of papers with semantic similarity to acquire relevance.""",,"- The abstract mentions that current methods use link-based approaches on citation networks, which implies a focus on network analysis rather than deep learning or NLP.
- The proposed ""RecCite"" approach blends popularity with semantic similarity, suggesting the use of semantic analysis techniques, possibly involving NLP to understand the context of papers.
- The evaluation uses the Arnet-Miner dataset, which is a publicly available dataset, but there is no mention of specific AI/ML frameworks, system architecture components, technical infrastructure, or integration with existing databases.
- The abstract does not provide details on the technical architecture, system components, or infrastructure used in the RecCite approach.","  - ""The proposed approach has been systematically evaluated with state-of-the-art baseline method using the publicly available Arnet-Miner data set.""
  - ""The state-of-the-art methods in this domain primarily employ link-based approaches on citation network and do not consider the semantics (context) of papers being recommended, although the later one largely accounts for the relevance of recommendations.""
  - ""In this context we propose a hybrid approach “RecCite” that blends the popularity of papers with semantic similarity to acquire relevance.""",,"- The abstract mentions that current methods use ""link-based approaches on citation network,"" which implies a focus on structural relationships between papers rather than their content.
- The proposed ""RecCite"" approach is described as a ""hybrid"" method that combines ""popularity of papers with semantic similarity,"" suggesting a blend of quantitative metrics (popularity) and qualitative analysis (semantic similarity).
- The abstract does not specify any particular algorithms, models, or feature extraction techniques used in ""RecCite,"" nor does it detail any data processing pipelines, training methodologies, preprocessing steps, or technical workflow.
- The evaluation of ""RecCite"" is mentioned in terms of performance metrics (precision, recall, response time) but not in terms of specific technical implementation details.","  - ""The state-of-the-art methods in this domain primarily employ link-based approaches on citation network and do not consider the semantics (context) of papers being recommended,""
  - ""In this context we propose a hybrid approach “RecCite” that blends the popularity of papers with semantic similarity to acquire relevance.""
  - ""The proposed approach has been systematically evaluated with state-of-the-art baseline method using the publicly available Arnet-Miner data set.""
  - ""An efficient academic paper recommendation process could be a promising maneuver for this purpose.""
  - ""A systematic review of literature is a crucial aspect in pursuing any research problem.""",,"- The abstract discusses the importance of systematic reviews and the need for efficient paper recommendation processes, which are part of paper discovery techniques.
- It mentions that current methods use link-based approaches on citation networks, which is a type of citation analysis method.
- The proposed ""RecCite"" approach is a hybrid method that combines popularity with semantic similarity, which can be considered a content-based matching technique and a recommendation system approach.
- The use of semantic similarity suggests a focus on relevance ranking approaches.
- The abstract does not explicitly mention search and retrieval algorithms, metadata extraction, or expert/authority identification methods, but it does imply a focus on relevance ranking and recommendation systems.","  - ""The state-of-the-art methods in this domain primarily employ link-based approaches on citation network and do not consider the semantics (context) of papers being recommended, although the later one largely accounts for the relevance of recommendations.""
  - ""With the galloping growth in the number of researchers and scientific publications available online in digital libraries, there is information overload which makes the search of relevant papers cumbersome.""
  - ""The proposed approach has been systematically evaluated with state-of-the-art baseline method using the publicly available Arnet-Miner data set.""
  - ""Also achieving online response time (scalability) is a perennial desire for any recommendation system.""",,"- The abstract mentions ""information overload"" as a challenge, which is related to data quality or availability problems. The sheer volume of publications makes it difficult to find relevant papers.
- The current methods are criticized for not considering the semantics of papers, which is an algorithm performance issue. This indicates a limitation in the current algorithms' ability to provide relevant recommendations.
- Scalability is explicitly mentioned as a challenge, as achieving online response time is a significant concern for recommendation systems.
- The abstract does not mention integration difficulties, evaluation methodology limitations, computational resource constraints, or technical bottlenecks or failure points.","  - ""In this context we propose a hybrid approach “RecCite” that blends the popularity of papers with semantic similarity to acquire relevance.""
  - ""The proposed approach has been systematically evaluated with state-of-the-art baseline method using the publicly available Arnet-Miner data set.""
  - ""The state-of-the-art methods in this domain primarily employ link-based approaches on citation network and do not consider the semantics (context) of papers being recommended, although the later one largely accounts for the relevance of recommendations.""
  - ""Satisfyingly, “RecCite” outperforms the baseline method with up to 37.66%, 20.14%, and 97.24% improvement in precision, recall, and response time, respectively.""",,"- The abstract mentions that current methods primarily use link-based approaches on citation networks but do not consider the semantics or context of papers. This indicates a gap in current methods.
- The authors propose a ""hybrid approach 'RecCite'"" that combines the popularity of papers with semantic similarity. This is a novel approach as it integrates two different methods to improve relevance.
- The use of a hybrid approach is a creative problem-solving method as it addresses the limitation of current methods by incorporating semantic context.
- The evaluation of ""RecCite"" against a baseline method using a publicly available dataset is a technical workaround for known problems, as it provides a systematic way to assess the effectiveness of the new approach.
- The significant improvements in precision, recall, and response time indicate that ""RecCite"" is an innovative solution that addresses scalability and relevance issues.","  - ""Satisfyingly, “RecCite” outperforms the baseline method with up to 37.66%, 20.14%, and 97.24% improvement in precision, recall, and response time, respectively.""
  - ""The proposed approach has been systematically evaluated with state-of-the-art baseline method using the publicly available Arnet-Miner data set.""",,"- The abstract mentions that the ""RecCite"" approach was evaluated using the Arnet-Miner data set, which is a publicly available dataset. This indicates that the performance results are based on a systematic evaluation.
- The abstract provides specific quantitative performance outcomes: ""RecCite"" outperforms the baseline method with improvements in precision (37.66%), recall (20.14%), and response time (97.24%). These are key metrics for evaluating the performance of a recommendation system.
- The comparison with a baseline method is explicitly mentioned, indicating that the performance results are relative to existing state-of-the-art methods.
- The abstract does not mention accuracy, F1-scores, user satisfaction, system reliability, or robustness measures. It focuses on precision, recall, and response time.
- The significant improvement in response time suggests that the system has good scalability, as it can handle requests efficiently.","  - ""The proposed approach has been systematically evaluated with state-of-the-art baseline method using the publicly available Arnet-Miner data set.""",,"- The abstract mentions the use of the ""Arnet-Miner data set,"" which is a publicly available dataset. This indicates that the Arnet-Miner dataset was used as a data source for evaluating the proposed approach.
- There is no mention of any academic databases such as Scopus or Web of Science being used.
- The abstract does not provide details about the dataset size, characteristics, data collection methods, training/validation/test data splits, or data preprocessing and cleaning approaches.
- There is no mention of any external knowledge bases or ontologies being used in the study.","  - ""Further, the approach follows a top-down methodology for filtering papers from a more generic and larger network to a community pertaining to a SIG (Special Interest Group)""
  - ""The proposed approach has been systematically evaluated with state-of-the-art baseline method using the publicly available Arnet-Miner data set.""
  - ""The state-of-the-art methods in this domain primarily employ link-based approaches on citation network and do not consider the semantics (context) of papers being recommended,""
  - ""An efficient academic paper recommendation process could be a promising maneuver for this purpose.""
  - ""A systematic review of literature is a crucial aspect in pursuing any research problem.""",,"- The abstract mentions that a systematic review of literature is crucial, indicating that the application context involves literature review tasks.
- The focus on an ""efficient academic paper recommendation process"" suggests that the system is designed to assist in finding relevant papers, which is a key task in literature reviews.
- The mention of ""link-based approaches on citation network"" and ""semantics (context) of papers"" implies that the system processes academic papers and considers their context and relevance.
- The use of the ""Arnet-Miner data set"" suggests that the system is evaluated using a dataset related to academic papers, which are the types of documents processed.
- The top-down methodology for filtering papers to a community related to a Special Interest Group (SIG) indicates that the system is designed to serve researchers within specific disciplines or fields.
- The abstract does not explicitly mention user types or requirements, nor does it specify whether the application is commercial or academic. However, the context suggests an academic application, as it is focused on literature reviews and academic paper recommendation."
Cost-effectiveness of Microsoft Academic Graph with machine learning for automated study identification in a living map of coronavirus disease 2019 (COVID-19) research.,"I. Shemilt, Anneliese Arno, James Thomas, T. Lorenc, Claire Khouja, G. Raine, K. Sutcliffe, D'Souza Preethy, Irene Kwan, K. Wright, A. Sowden",10.12688/wellcomeopenres.17141.1,https://doi.org/10.12688/wellcomeopenres.17141.1,Wellcome Open Research,11,2021,"- Main AI/ML frameworks used: Machine learning-based recommender model (AutoUpdate model), binary ML classifier for distinguishing eligible records.
- System architecture components: Microsoft Academic Graph (MAG) dataset, EPPI-Reviewer software.
- Technical infrastructure: EPPI-Reviewer hosted by UCL, running in a web browser.
- Integration with existing academic databases or platforms: Integration with MAG dataset, accessible within EPPI-Reviewer.
- Overall system design approach: Use of machine learning classifiers for automated record identification and prioritization.","- Algorithms and models used: Machine learning-based recommender model (AutoUpdate), binary ML classifiers for classification.
- Feature extraction techniques: Graph and text features from MAG records.
- Data processing pipelines: Automated searching of MAG dataset, deduplication, manual screening.
- Training methodologies: Supervised training with feedback from manual screening decisions (active learning).
- Preprocessing steps: Deduplication, scoring by binary ML classifier.
- Technical workflow or methodology: Use of EPPI-Reviewer software for automated searching and screening.","- Search and retrieval algorithms: Custom search feature using Boolean-type search strategies
- Relevance ranking approaches: Active learning for prioritizing records
- Content-based matching techniques: Binary ML classifiers for discriminating between eligible and ineligible records
- Metadata extraction and utilization: Use of Microsoft Academic Graph (MAG) dataset
- Recommendation system approaches: AutoUpdate model for continuous evidence surveillance","- Algorithm performance issues: Inability to fully simulate the MAG-enabled workflow due to prospective deployment of the AutoUpdate model.
- Data quality or availability problems: Need for explicit analytic assumptions due to missing data inputs for some model parameters.
- Scalability challenges: Limited resources led to screening only a subset of records, potentially underestimating recall and overestimating baseline performance.
- Integration difficulties: Issue with non-English titles and abstracts being discarded or deprioritized due to algorithm limitations.
- Evaluation methodology limitations: Retrospective simulation could not incorporate prospective AutoUpdate model deployment.
- Computational resource constraints: Limited resources for screening all identified records.
- Technical bottlenecks or failure points: Algorithm's reliance on text features rather than graph features for non-English records.","- New algorithmic contributions: Use of a binary ML classifier to distinguish between eligible and ineligible records.
- Creative problem-solving methods: Integration of MAG with machine learning for efficient study identification.
- Hybrid or ensemble approaches: Combination of MAG with ML and priority screening mode in EPPI-Reviewer.
- Novel feature engineering techniques: Use of the 'AutoUpdate' model to infer relevance of new MAG records.
- Technical workarounds for known problems: Automatic translation of non-English language records to address language bias.","- Recall: Up to 678 eligible records saved from exclusion
- Precision: Higher compared to conventional methods
- Cost-effectiveness: MAG-enabled workflow dominated conventional methods
- Efficiency: Lower screening workloads and costs
- Comparison with baseline: MAG-enabled workflow was more efficient and cost-effective","- Academic databases used: Microsoft Academic Graph (MAG), MEDLINE, Embase
- Dataset sizes and characteristics: MAG contains over 250 million bibliographic records; network graph structure
- Data collection methods: Boolean searches of MEDLINE and Embase; use of MAG for semi-automated workflows
- Training/validation/test data splits: Not explicitly mentioned
- Data preprocessing and cleaning approaches: Not explicitly mentioned
- External knowledge bases or ontologies used: MAG's network graph features","- Target research disciplines or fields: Health research, specifically COVID-19 studies
- Specific literature review tasks addressed: Identifying new, eligible studies for integration into living systematic reviews and maps
- Types of academic documents processed: Empirical primary research articles, modelling studies, systematic reviews related to COVID-19
- User types and requirements: Information specialists and researchers involved in map production
- Integration with research workflows: EPPI-Reviewer software adapted to incorporate MAG and machine learning workflows
- Commercial vs. academic applications: Primarily academic","  - ""Microsoft Academic Graph (MAG) is a large open-access dataset and repository that currently comprises over 250 million bibliographic records of scientific research articles""
  - ""A key feature of the MAG dataset is that its bibliographic records are all connected in a large network graph of conceptual, citation and other relationships.""
  - ""We have therefore been actively developing a suite of tools to enable automated searching of a local copy of the MAG dataset in EPPI-Reviewer, systematic review software that is hosted by UCL and runs in a web browser""
  - ""These tools include a novel machine learning-based recommender model for continuous evidence surveillance (the 'AutoUpdate' model) 5 , developed in collaboration with Microsoft™ to support the Human Behaviour-Change Project""
  - ""The AutoUpdate model is trained to infer the relevance of newly published MAG records to existing living maps, registers, systematic reviews and databases of research articles that subscribe to it.""
  - ""A binary ML classifier 11 , designed to distinguish between eligible title-abstract records included in (positive class) and ineligible records excluded from (negative class) our COVID-19 living map, was deployed to score new records identified from either the MAG dataset or MEDLINE-Embase databases.""
  - ""In EPPI-Reviewer's priority screening mode, retained records were manually screened for potential inclusion in our living map in prioritised rank order (highest to lowest) based on their scores assigned by the binary ML classifier,""
  - ""The tools for searching MAG and using the AutoUpdate models are available in EPPI-Reviewer and can be accessed by other teams wanting to explore the potential value of automated searching.""",,"- The main AI/ML framework used is a machine learning-based recommender model called the ""AutoUpdate"" model, which is used for continuous evidence surveillance.
- The system architecture includes the use of the Microsoft Academic Graph (MAG) dataset, which is a large open-access repository of bibliographic records connected in a network graph.
- The technical infrastructure involves the use of EPPI-Reviewer, a systematic review software hosted by UCL and running in a web browser, which integrates with the MAG dataset.
- Integration with existing academic databases or platforms is evident through the use of MAG and the ability to access these tools within EPPI-Reviewer.
- The overall system design approach involves the use of machine learning classifiers to automate the identification and prioritization of eligible records for inclusion in living maps.","  - ""Microsoft Academic Graph (MAG) is a large open-access dataset and repository that currently comprises over 250 million bibliographic records of scientific research articles""
  - ""A key feature of the MAG dataset is that its bibliographic records are all connected in a large network graph of conceptual, citation and other relationships.""
  - ""We have therefore been actively developing a suite of tools to enable automated searching of a local copy of the MAG dataset in EPPI-Reviewer, systematic review software that is hosted by UCL and runs in a web browser""
  - ""These tools include a novel machine learning-based recommender model for continuous evidence surveillance (the 'AutoUpdate' model) 5 , developed in collaboration with Microsoft™ to support the Human Behaviour-Change Project""
  - ""The AutoUpdate model is trained to infer the relevance of newly published MAG records to existing living maps, registers, systematic reviews and databases of research articles that subscribe to it.""
  - ""This is based on a supervised dataset, comprising graph and text features from MAG records previously selected for inclusion in those resources.""
  - ""A 'custom search' feature is also available, enabling semi-automated searches of the updated MAG dataset using more conventional Boolean-type search strategies.""
  - ""Machine learning (ML) tools that enable both approaches have been under active development and made available in select systematic review softwares, including EPPI Reviewer""
  - ""For (1), binary ML classifiers can be trained to discriminate between text features of eligible and ineligible records, calibrated to a threshold score that ensures an acceptably high recall (proportion of eligible records are retained).""
  - ""For (2), a rank-ordered list of (retained) records can be continuously reprioritised for manual screening by the same underlying ML classifier, incorporating 'feedback' from the growing corpus of manual screening decisions already made about eligible and ineligible records (known as 'active learning').""
  - ""A binary ML classifier 11 , designed to distinguish between eligible title-abstract records included in (positive class) and ineligible records excluded from (negative class) our COVID-19 living map, was deployed to score new records identified from either the MAG dataset or MEDLINE-Embase databases.""
  - ""In EPPI-Reviewer's priority screening mode, retained records were manually screened for potential inclusion in our living map in prioritised rank order (highest to lowest) based on their scores assigned by the binary ML classifier, described above.""
  - ""The rank order of records awaiting screening was periodically automatically reprioritised based on all preceding eligibility decisions.""
  - ""When applying a fixed screening target, manual screening of records in priority screening mode was truncated after a specified target number of records had been screened.""",,"- The paper describes the use of Microsoft Academic Graph (MAG) as a comprehensive dataset for study identification, which is a key component of the implementation.
- The AutoUpdate model is a machine learning-based recommender model used for continuous evidence surveillance, indicating the use of machine learning algorithms.
- The model is trained on a supervised dataset that includes graph and text features from MAG records, suggesting feature extraction techniques involving both graph and text data.
- The paper mentions the use of binary ML classifiers to discriminate between eligible and ineligible records, which is a classification algorithm.
- Active learning is used to reprioritize records for manual screening, indicating a dynamic training methodology that incorporates feedback from manual screening decisions.
- The paper describes a data processing pipeline involving automated searching of the MAG dataset, deduplication, and manual screening, which are part of the technical workflow.
- The use of EPPI-Reviewer software for automated searching and screening suggests a specific technical workflow or methodology.","  - ""Microsoft Academic Graph (MAG) is a large open-access dataset and repository that currently comprises over 250 million bibliographic records of scientific research articles""
  - ""A key feature of the MAG dataset is that its bibliographic records are all connected in a large network graph of conceptual, citation and other relationships.""
  - ""We have therefore been actively developing a suite of tools to enable automated searching of a local copy of the MAG dataset in EPPI-Reviewer, systematic review software that is hosted by UCL and runs in a web browser""
  - ""The AutoUpdate model is trained to infer the relevance of newly published MAG records to existing living maps, registers, systematic reviews and databases of research articles that subscribe to it.""
  - ""A 'custom search' feature is also available, enabling semi-automated searches of the updated MAG dataset using more conventional Boolean-type search strategies.""
  - ""Machine learning (ML) tools that enable both approaches have been under active development and made available in select systematic review softwares, including EPPI Reviewer""
  - ""binary ML classifiers can be trained to discriminate between text features of eligible and ineligible records, calibrated to a threshold score that ensures an acceptably high recall (proportion of eligible records are retained).""
  - ""a rank-ordered list of (retained) records can be continuously reprioritised for manual screening by the same underlying ML classifier, incorporating 'feedback' from the growing corpus of manual screening decisions already made about eligible and ineligible records (known as 'active learning').""
  - ""In EPPI-Reviewer's priority screening mode, retained records were manually screened for potential inclusion in our living map in prioritised rank order (highest to lowest) based on their scores assigned by the binary ML classifier,""
  - ""The outcome measure of benefit was the number of eligible records 'saved' from inappropriate exclusion from our living map""
  - ""Resource use was measured as the total time (hours) spent by our information specialists and screen-coding team on completing the manual tasks in each workflow (study arm).""
  - ""Cost-effectiveness was defined as the incremental cost per eligible study report (record) 'saved' from inappropriate exclusion from our living map, compared with current practice,""","  - ""(Page 5, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 8, Table 1) | Study arm | Comparator/Intervention | Recall* | Precision** | Incremental effectiveness*** |\n|-----------|-------------------------|---------|-------------|------------------------------|\n| 1         | Comparator A            | 0.83    | 0.40        | -                            |""
  - ""(Page 9, Table 1) | Study arm | Comparator/Intervention | Resource use (hours) | Total cost  | Incremental cost |\n|-----------|--------------------------|----------------------|-------------|------------------|\n| **Manual**        |                          |                      |             |                  |""","- The paper discusses the use of Microsoft Academic Graph (MAG) as a comprehensive dataset for study identification, which involves metadata extraction and utilization.
- The AutoUpdate model is a machine learning-based recommender system that helps in identifying relevant studies, which aligns with recommendation system approaches.
- The use of binary ML classifiers for discriminating between eligible and ineligible records is a content-based matching technique.
- The paper mentions the use of active learning for prioritizing records, which is a relevance ranking approach.
- The custom search feature allows for Boolean-type search strategies, which is a search and retrieval algorithm.
- The paper does not explicitly mention citation analysis methods or expert/authority identification methods, but the use of MAG implies some level of citation analysis due to its network graph structure.","  - ""The main limitation of this study is that we were unable to precisely simulate all components of the MAG-enabled study identification workflow that we have subsequently implemented in practice.""
  - ""Specifically, it was not feasible to incorporate use of the AutoUpdate model, which is (prospectively) deployed in automated update searches of the MAG dataset in our 'live' workflow, into our (retrospective) simulation.""
  - ""A second limitation is that, like most model-based economic evaluations, we needed to make explicit analytic assumptions in the absence of data inputs for some model parameters""
  - ""A third limitation is that, due to limited resources, we decided to stop after screening (for evaluation purposes) the top ranked (using active learning) 1,500 records out of almost 5,000 MAG records identified by our 'custom search' of the MAG dataset""
  - ""One aspect we are aiming to address is the issue of fully non-English titles and abstracts.""
  - ""Further investigations have revealed this is due to non-English language records being both discarded after scoring by the binary ML classifier (i.e. they fall below the calibrated threshold score) and also de-prioritised in the list of records to be manually screened by active learning, both of which currently use an algorithm that is exclusively based on the text features (and not graph features) of candidate records.""
  - ""We plan to address this limitation in our 'live' map workflow by automatically identifying and translating all non-English language records into English language before submitting them for scoring by the binary ML classifier.""",,"- The paper discusses several technical challenges and limitations in the context of using Microsoft Academic Graph (MAG) with machine learning for automated study identification.
- One major challenge was the inability to fully simulate the MAG-enabled workflow due to the prospective deployment of the AutoUpdate model, which could not be incorporated into the retrospective simulation.
- Another limitation was the need for explicit analytic assumptions due to missing data inputs for some model parameters, which could affect the certainty of model outputs.
- The study was also limited by resource constraints, as they only screened a subset of records identified by the MAG dataset, which might underestimate recall and overestimate baseline workflow performance.
- The issue of non-English titles and abstracts is highlighted as a technical challenge, as these records are currently discarded or deprioritized due to the algorithm's reliance on text features rather than graph features.
- The plan to address this limitation involves translating non-English records into English, which suggests a technical challenge related to language processing and integration.","  - ""Microsoft Academic Graph (MAG) is a potentially comprehensive single source which also contains metadata that can be used in machine learning to help efficiently identify eligible studies.""
  - ""We conducted an eight-arm cost-effectiveness analysis to assess the costs, recall and precision of semi-automated workflows, incorporating MAG with adjunctive machine learning, for continually updating our living map.""
  - ""The semi-automated MAG-enabled workflow dominated conventional workflows in both the base case and sensitivity analyses.""
  - ""A binary ML classifier 11 , designed to distinguish between eligible title-abstract records included in (positive class) and ineligible records excluded from (negative class) our COVID-19 living map, was deployed to score new records identified from either the MAG dataset or MEDLINE-Embase databases.""
  - ""In EPPI-Reviewer's priority screening mode, retained records were manually screened for potential inclusion in our living map in prioritised rank order (highest to lowest) based on their scores assigned by the binary ML classifier, described above.""
  - ""The AutoUpdate model is trained to infer the relevance of newly published MAG records to existing living maps, registers, systematic reviews and databases of research articles that subscribe to it.""
  - ""We plan to address this limitation in our 'live' map workflow by automatically identifying and translating all non-English language records into English language before submitting them for scoring by the binary ML classifier.""","  - ""(Page 5, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 8, Table 1) | Study arm | Comparator/Intervention | Recall* | Precision** | Incremental effectiveness*** |\n|-----------|-------------------------|---------|-------------|------------------------------|\n| 1         | Comparator A            | 0.83    | 0.40        | -                            |""
  - ""(Page 9, Table 1) | Study arm | Comparator/Intervention | Resource use (hours) | Total cost  | Incremental cost |\n|-----------|--------------------------|----------------------|-------------|------------------|\n| **Manual**        |                          |                      |             |                  |""","- The paper introduces the use of Microsoft Academic Graph (MAG) as a comprehensive single source for study identification, which is a novel approach compared to traditional methods.
- The integration of machine learning (ML) with MAG is highlighted as a key innovation, particularly the use of a binary ML classifier to distinguish between eligible and ineligible records.
- The paper describes the use of a novel machine learning-based recommender model called the 'AutoUpdate' model, which is designed to infer the relevance of new MAG records to existing living maps and systematic reviews.
- The implementation of priority screening mode in EPPI-Reviewer, which uses ML scores to prioritize records for manual screening, is another innovative solution.
- The plan to automatically translate non-English language records into English to improve the workflow is a technical workaround for known language bias issues.
- The tables provide detailed information on the different study arms and interventions, which include various combinations of MAG, ML, and other tools, indicating a hybrid approach to study identification.","  - ""The semi-automated MAG-enabled workflow dominated conventional workflows in both the base case and sensitivity analyses.""
  - ""At one month our MAG-enabled workflow with machine learning, active learning and fixed screening targets identified 469 additional, eligible articles for inclusion in our living map, and cost £3,179 GBP per week less, compared with conventional methods relying on Boolean searches of Medline and Embase.""
  - ""Compared with workflows incorporating conventional searches of MEDLINE and Embase (arms 1-5), those workflows using MAG as a single source (arms 6-8) had both higher recall and higher precision, saving up to 678 eligible records (arm 6) from inappropriate exclusion from our living map during the four-week study period, as compared with current standard practice (arm 1).""
  - ""Incorporating use of automation technologies (arms 4-8), fixed screening targets (arms 3, 5 and 8) and relaxed target recall (arms 2-5, 7 and 8) all resulted in lower screening workloads, and therefore, lower total costs, compared with workflows not using these tools and targets.""
  - ""Cost-effectiveness results (base-case analysis) are plotted on the cost-effectiveness plane shown in Figure 2""
  - ""Overall, these findings and monitoring data demonstrate the clear potential of our novel workflow, which combines automated searching of the MAG dataset with the use of ML tools, to improve the effectiveness and efficiency of study identification workflows for living maps of research and related evidence synthesis.""","  - ""(Page 8, Table 1) | Study arm | Comparator/Intervention | Recall* | Precision** | Incremental effectiveness*** |\n|-----------|-------------------------|---------|-------------|------------------------------|\n| 1         | Comparator A            | 0.83    | 0.40        | -                            |""
  - ""(Page 9, Table 1) | Study arm | Comparator/Intervention | Resource use (hours) | Total cost  | Incremental cost |\n|-----------|--------------------------|----------------------|-------------|------------------|\n| **Manual**        |                          |                      |             |                  |""","- The paper discusses the performance of a semi-automated workflow using Microsoft Academic Graph (MAG) with machine learning compared to conventional methods. It highlights that the MAG-enabled workflow was more efficient and cost-effective.
- The recall and precision of the MAG-enabled workflow were higher compared to conventional methods, with up to 678 eligible records saved from exclusion.
- The use of automation technologies and fixed screening targets resulted in lower screening workloads and costs.
- The cost-effectiveness analysis showed that the MAG-enabled workflow dominated conventional methods in terms of cost and effectiveness.
- The tables provide quantitative data on recall, precision, and cost-effectiveness, which are key performance metrics.
- The paper does not mention specific metrics like accuracy, F1-scores, or user satisfaction, but it does discuss the efficiency and cost-effectiveness of the workflow.","  - ""Microsoft Academic Graph (MAG) is a large open-access dataset and repository that currently comprises over 250 million bibliographic records of scientific research articles""
  - ""Microsoft makes the entire MAG dataset available for third-party use under a creative commons license.""
  - ""A key feature of the MAG dataset is that its bibliographic records are all connected in a large network graph of conceptual, citation and other relationships.""
  - ""We conducted an eight-arm cost-effectiveness analysis to assess the costs, recall and precision of semi-automated workflows, incorporating MAG with adjunctive machine learning, for continually updating our living map.""
  - ""The semi-automated MAG-enabled workflow dominated conventional workflows in both the base case and sensitivity analyses.""
  - ""We simulated the incremental costs and effects of using eight-variant manual (comparator) or semi-automated (intervention) study identification workflows to maintain our living map for one month between 22 nd June and 23 rd July 2020 (Search 16 to Search 19).""
  - ""We searched for potentially eligible records using conventional Boolean searches of MEDLINE (Ovid) and Embase (Ovid) databases each week after their weekly updates.""
  - ""Retrieved records were downloaded into an EndNote library (version X9) for deduplication between the two sources, followed by deduplication against records retrieved in all previous weeks.""
  - ""The AutoUpdate model is trained to infer the relevance of newly published MAG records to existing living maps, registers, systematic reviews and databases of research articles that subscribe to it.""
  - ""The outcome measure of benefit was the number of eligible records 'saved' from inappropriate exclusion from our living map""
  - ""The total cost of running each workflow was then calculated by multiplying the estimates of total time-on-task (hours) by the UK and Australian unit costs (per hour).""","  - ""(Page 5, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 8, Table 1) | Study arm | Comparator/Intervention | Recall* | Precision** | Incremental effectiveness*** |\n|-----------|-------------------------|---------|-------------|------------------------------|\n| 1         | Comparator A            | 0.83    | 0.40        | -                            |""
  - ""(Page 9, Table 1) | Study arm | Comparator/Intervention | Resource use (hours) | Total cost  | Incremental cost |\n|-----------|--------------------------|----------------------|-------------|------------------|\n| **Manual**        |                          |                      |             |                  |""","- The primary data source used in this study is the Microsoft Academic Graph (MAG), which is a large open-access dataset containing over 250 million bibliographic records.
- The MAG dataset is characterized by its network graph structure, connecting bibliographic records through conceptual, citation, and other relationships.
- The study also uses conventional Boolean searches of MEDLINE and Embase databases for comparison.
- The data collection method involves using these databases to identify eligible records for a living map of COVID-19 research.
- A binary machine learning classifier is used to score records for eligibility, trained on a supervised dataset from MAG records.
- The AutoUpdate model is another key tool, trained on graph and text features from MAG records to infer relevance.
- The study does not explicitly mention training/validation/test data splits or specific data preprocessing and cleaning approaches.
- External knowledge bases or ontologies are not explicitly mentioned beyond the use of MAG and its network graph features.","  - ""This study sought to establish whether: (a) MAG was a sufficiently sensitive single source to maintain our living map of COVID-19 research; and (b) eligible records could be identified with an acceptably high level of specificity.""
  - ""We conducted an eight-arm cost-effectiveness analysis to assess the costs, recall and precision of semi-automated workflows, incorporating MAG with adjunctive machine learning, for continually updating our living map.""
  - ""The semi-automated MAG-enabled workflow dominated conventional workflows in both the base case and sensitivity analyses.""
  - ""We were able to increase recall and coverage of a large living map, whilst reducing its production costs.""
  - ""This finding is likely to be transferrable to OpenAlex, MAG's successor database platform.""
  - ""Improving the efficiency of evidence synthesis production workflows is an important catalyst to enabling better health decisions and outcomes.""
  - ""Globally, the research sector spends billions of dollars each year on identifying newly published empirical research articles for indexing in living maps, specialised registers, and tertiary databases on various topics -resources which facilitate the discovery, use and reuse of research, including prioritisation, specification, production and (continuous) updating of (living) systematic reviews.""
  - ""We have therefore been actively developing a suite of tools to enable automated searching of a local copy of the MAG dataset in EPPI-Reviewer, systematic review software that is hosted by UCL and runs in a web browser""
  - ""The AutoUpdate model is trained to infer the relevance of newly published MAG records to existing living maps, registers, systematic reviews and databases of research articles that subscribe to it.""
  - ""Selecting eligible articles has conventionally relied on manual screening of all unique bibliographic records, retrieved from updated searches, against pre-specified criteria.""
  - ""The explosion, during early 2020, in the volume and rate of publication of new primary and secondary research on COVID-19 prompted various efforts to filter and organise this evidence into living maps, specialised registers, or tertiary databases.""
  - ""Up to the end of April 2021, the living map included 52,355 bibliographic records of research articles reporting empirical primary research, modelling studies or systematic reviews on COVID-19, organised into 11 topic codes (based on the main focus of the study).""
  - ""Pre-prints, records of articles not reporting primary data, not relevant to/ not focused on humans, and/or not on the topic of COVID-19, are excluded.""
  - ""The tools for searching MAG and using the AutoUpdate models are available in EPPI-Reviewer and can be accessed by other teams wanting to explore the potential value of automated searching.""","  - ""(Page 5, Table 1) Below is a rendering of the page up to the first error.""
  - ""(Page 8, Table 1) | Study arm | Comparator/Intervention | Recall* | Precision** | Incremental effectiveness*** |\n|-----------|-------------------------|---------|-------------|------------------------------|\n| 1         | Comparator A            | 0.83    | 0.40        | -                            |""
  - ""(Page 9, Table 1) | Study arm | Comparator/Intervention | Resource use (hours) | Total cost  | Incremental cost |\n|-----------|--------------------------|----------------------|-------------|------------------|\n| **Manual**        |                          |                      |             |                  |""","- The paper focuses on the application of Microsoft Academic Graph (MAG) with machine learning for automated study identification in a living map of COVID-19 research. This indicates a specific use case in the field of health research, particularly in managing and updating systematic reviews and maps related to COVID-19.
- The target research discipline is health research, specifically COVID-19 studies, as evidenced by the focus on maintaining a living map of COVID-19 research.
- The specific literature review tasks addressed include identifying new, eligible studies for integration into living systematic reviews and maps, which involves searching and selection tasks.
- The types of academic documents processed include empirical primary research articles, modelling studies, and systematic reviews related to COVID-19.
- The user types and requirements include information specialists and researchers involved in map production, who need efficient workflows for updating living maps.
- The integration with research workflows is demonstrated through the use of EPPI-Reviewer software, which is adapted to incorporate MAG and associated machine learning workflows.
- The application is primarily academic, as it is focused on improving the efficiency of evidence synthesis production workflows for better health decisions and outcomes."
Total Recall via Keyqueries: A Case Study for Systematic Reviews,Paul Alexander,-,-,-,0,2021,"- Main AI/ML frameworks used: Logistic regression, decision table, random forest
- System architecture components: Not mentioned
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Not mentioned
- Overall system design approach: Keyquery-based approach for ranking relevant documents","- Algorithms and models used: Keyquery-based approach, logistic regression, decision table, random forest, naive Bayes
- Feature extraction techniques: Not mentioned
- Data processing pipelines: Not mentioned
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Not mentioned","- Relevance ranking approaches: Keyqueries to rank known relevant documents
- Recommendation system approaches: User feedback to learn and present more potential relevant documents
- Content-based matching techniques: Keyqueries to identify new, potentially relevant documents
- Machine-learning approaches: Logistic regression, decision table, random forest","- Algorithm performance issues: The keyquery-based approach is outperformed by the state-of-the-art and naive Bayes approaches.
- Scalability challenges: Creating a systematic review is time-consuming and can take several years.
- Integration difficulties: Systems incorporate user feedback to learn and present relevant documents.
- Evaluation methodology limitations: The evaluation shows mixed performance of the keyquery approach compared to other methods.","- New algorithmic contributions: Automatic creation of keyqueries to rank relevant documents.
- Creative problem-solving methods: Adapting related work search research to improve systematic reviews.
- Novel feature engineering techniques: Constructing keyqueries for relevant documents to find new relevant documents.
- Innovative evaluation metrics: Comparing keyquery-based approach with other machine-learning methods.","- Comparison with baseline methods: The keyquery-based approach outperforms logistic regression and decision table, is comparable to a random forest approach, but is outperformed by the state-of-the-art and naive Bayes approaches.
- Qualitative performance outcomes: The keyquery-based approach is effective in ranking known relevant documents in the top results of a reference search engine.","Not mentioned (the abstract does not provide specific information on data sources, dataset sizes, data collection methods, or external knowledge bases)","- Target research disciplines or fields: Biological research topics
- Specific literature review tasks addressed: Systematic reviews
- Types of academic documents processed: Research articles and scholarly publications relevant to systematic reviews
- User types and requirements: Researchers conducting systematic reviews
- Integration with research workflows: Technology-assisted systematic reviews
- Commercial vs. academic applications: Likely academic","  - ""The Evaluation shows that our keyquery-based approach outperforms our implementation of logistic regression and decision table, and is comparable to a random forest approach.""
  - ""We compare our keyquery-based approach with four classical machine-learning approaches and the state-of-the-art approach on three simulated systematic reviews with biological research topics.""
  - ""This keyquery approach is motivated by research on related work search, where keyqueries retrieve additional related work for a given set of documents.""
  - ""We propose a system that automatically creates so-called keyqueries which rank the known relevant documents in the top results of a reference search engine.""",,"- The abstract mentions the use of a ""keyquery-based approach,"" which suggests a technical architecture centered around generating and utilizing keyqueries to rank relevant documents.
- The mention of ""classical machine-learning approaches"" such as logistic regression, decision table, and random forest indicates that these are the main AI/ML frameworks used in the system.
- The abstract does not provide specific details about system architecture components like databases, APIs, or interfaces, nor does it mention technical infrastructure such as cloud platforms or computational resources.
- There is no mention of integration with existing academic databases or platforms, nor is there a description of the overall system design approach beyond the use of keyqueries and machine learning algorithms.
- The focus is on the methodological approach rather than the technical architecture or infrastructure.","  - ""The state-of-the-art and our naive Bayes approach both outperform our keyquery-based approach.""
  - ""The Evaluation shows that our keyquery-based approach outperforms our implementation of logistic regression and decision table, and is comparable to a random forest approach.""
  - ""We compare our keyquery-based approach with four classical machine-learning approaches and the state-of-the-art approach on three simulated systematic reviews with biological research topics.""
  - ""We propose a system that automatically creates so-called keyqueries which rank the known relevant documents in the top results of a reference search engine.""",,"- The abstract mentions the use of a ""keyquery-based approach,"" which is a method for automatically creating queries to rank relevant documents.
- It compares this approach with ""four classical machine-learning approaches,"" which are specified as logistic regression, decision table, random forest, and naive Bayes.
- These are the algorithms and models used in the study.
- The abstract does not provide specific details on feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or technical workflow.
- The focus is on the comparison of different machine learning approaches rather than detailed technical implementation methods.","  - ""Creating a systematic review is time-consuming and can take several years.""
  - ""Systems for technology-assisted systematic reviews incorpo-rate user feedback, whether a document was relevant or not, to learn presenting more yet unknown potential relevant documents.""
  - ""We propose a system that automatically creates so-called keyqueries which rank the known relevant documents in the top results of a reference search engine.""
  - ""This keyquery approach is motivated by research on related work search, where keyqueries retrieve additional related work for a given set of documents.""
  - ""Therefore, we construct keyqueries for the documents labeled as relevant in the systematic review to identify new, potentially relevant documents.""
  - ""We compare our keyquery-based approach with four classical machine-learning approaches and the state-of-the-art approach on three simulated systematic reviews with biological research topics.""
  - ""The Evaluation shows that our keyquery-based approach outperforms our implementation of logistic regression and decision table, and is comparable to a random forest approach.""",,"- The abstract discusses the use of ""keyqueries"" as a method for ranking relevant documents in search results, which is a relevance ranking approach.
- The system uses user feedback to learn and present more potential relevant documents, which is a recommendation system approach.
- The abstract mentions the comparison with machine-learning approaches such as logistic regression, decision table, and random forest, indicating the use of these algorithms for relevance ranking.
- The keyquery approach is used to identify new, potentially relevant documents, which aligns with content-based matching techniques.
- The abstract does not explicitly mention search and retrieval algorithms, citation analysis methods, metadata extraction and utilization, or expert/authority identification methods.","  - ""The state-of-the-art and our naive Bayes approach both outperform our keyquery-based approach.""
  - ""The Evaluation shows that our keyquery-based approach outperforms our implementation of logistic regression and decision table, and is comparable to a random forest approach.""
  - ""We propose a system that automatically creates so-called keyqueries which rank the known relevant documents in the top results of a reference search engine.""
  - ""Creating a systematic review is time-consuming and can take several years.""
  - ""Systems for technology-assisted systematic reviews incorpo-rate user feedback, whether a document was relevant or not, to learn presenting more yet unknown potential relevant documents.""",,"- The abstract mentions that creating a systematic review is time-consuming, which could imply a scalability challenge or computational resource constraint, as it takes several years to complete.
- The integration of user feedback into systems for technology-assisted systematic reviews suggests potential integration difficulties, as it requires effective incorporation of feedback to improve document relevance.
- The comparison of the keyquery-based approach with other machine learning methods indicates algorithm performance issues, as the keyquery approach is outperformed by some methods.
- The evaluation methodology limitations are implied by the comparison with other approaches, suggesting that the evaluation process may not fully capture the effectiveness of the keyquery approach.
- The abstract does not explicitly mention data quality or availability problems, scalability challenges beyond the time-consuming nature of systematic reviews, or technical bottlenecks or failure points.","  - ""We propose a system that automatically creates so-called keyqueries which rank the known relevant documents in the top results of a reference search engine.""
  - ""Therefore, we construct keyqueries for the documents labeled as relevant in the systematic review to identify new, potentially relevant documents.""
  - ""This keyquery approach is motivated by research on related work search, where keyqueries retrieve additional related work for a given set of documents.""
  - ""The Evaluation shows that our keyquery-based approach outperforms our implementation of logistic regression and decision table, and is comparable to a random forest approach.""",,"- The abstract introduces a novel technical approach by proposing a system that automatically creates ""keyqueries."" These keyqueries are designed to rank known relevant documents at the top of search engine results, which is a new algorithmic contribution.
- The keyquery approach is motivated by research on related work search, indicating a creative problem-solving method by adapting existing research to improve systematic reviews.
- The system constructs keyqueries for relevant documents to identify new, potentially relevant documents, which is a novel feature engineering technique.
- The evaluation compares the keyquery-based approach with other machine-learning approaches, suggesting an innovative evaluation metric by assessing its performance against established methods.
- The abstract does not mention any hybrid or ensemble approaches, nor does it discuss technical workarounds or original system design elements beyond the keyquery system.","  - ""The Evaluation shows that our keyquery-based approach outperforms our implementation of logistic regression and decision table, and is comparable to a random forest approach.""
  - ""The state-of-the-art and our naive Bayes approach both outperform our keyquery-based approach.""
  - ""We compare our keyquery-based approach with four classical machine-learning approaches and the state-of-the-art approach on three simulated systematic reviews with biological research topics.""
  - ""We propose a system that automatically creates so-called keyqueries which rank the known relevant documents in the top results of a reference search engine.""",,"- The abstract mentions the development of a system that uses keyqueries to rank relevant documents, which is a performance outcome related to the efficiency of document retrieval.
- The comparison with other machine-learning approaches (logistic regression, decision table, random forest, and naive Bayes) provides qualitative performance outcomes in terms of how the keyquery-based approach performs relative to these methods.
- The abstract states that the keyquery-based approach outperforms logistic regression and decision table, and is comparable to a random forest approach, indicating its effectiveness in certain contexts.
- The state-of-the-art and naive Bayes approaches are noted to outperform the keyquery-based approach, providing a baseline for comparison.
- There is no mention of specific metrics like accuracy, precision, recall, F1-scores, processing speed, user satisfaction, system reliability, or scalability test results in the abstract.","  - ""We compare our keyquery-based approach with four classical machine-learning approaches and the state-of-the-art approach on three simulated systematic reviews with biological research topics.""
  - ""We propose a system that automatically creates so-called keyqueries which rank the known relevant documents in the top results of a reference search engine.""",,"- The abstract mentions the use of a ""reference search engine"" but does not specify which academic databases were used (e.g., Scopus, Web of Science).
- There is no mention of dataset sizes or characteristics, nor are there details about data collection methods or training/validation/test data splits.
- The abstract does not provide information on data preprocessing and cleaning approaches.
- There is no mention of external knowledge bases or ontologies used in the study.
- The focus of the abstract is on the methodological approach using keyqueries and its comparison with other machine-learning methods, rather than on the specific data sources or datasets used.","  - ""We compare our keyquery-based approach with four classical machine-learning approaches and the state-of-the-art approach on three simulated systematic reviews with biological research topics.""
  - ""During the screening of documents, researchers aim for total recall to ensure that all relevant documents are covered in their systematic review.""
  - ""We propose a system that automatically creates so-called keyqueries which rank the known relevant documents in the top results of a reference search engine.""
  - ""Systems for technology-assisted systematic reviews incorpo-rate user feedback, whether a document was relevant or not, to learn presenting more yet unknown potential relevant documents.""
  - ""Creating a systematic review is time-consuming and can take several years.""",,"- The abstract discusses the context of systematic reviews, which is a specific literature review task addressed in the application context.
- The target research discipline or field mentioned is ""biological research topics,"" indicating the domain of application.
- The types of academic documents processed are those relevant to systematic reviews, which typically include research articles and other scholarly publications.
- The user types and requirements are implied to be researchers conducting systematic reviews, who need efficient methods to identify relevant documents.
- The integration with research workflows is suggested by the mention of technology-assisted systematic reviews, which implies integration into existing research processes.
- The abstract does not specify whether the application is commercial or academic, but given the context of systematic reviews and research, it is likely academic."
Recent Developments in Deep Learning-based Author Name Disambiguation,"Francesca Cappelli, Giovanni Colavizza, Silvio Peroni",10.48550/arXiv.2503.13448,https://doi.org/10.48550/arXiv.2503.13448,Italian Research Conference on Digital Library Management Systems,4,2024,"- Main AI/ML frameworks used: Deep Learning
- System architecture components: Not mentioned
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Not mentioned
- Overall system design approach: Integration of structured and unstructured data using hybrid approaches balancing supervised and unsupervised learning","- Algorithms and models used: Deep Learning algorithms
- Feature extraction techniques: Not mentioned
- Data processing pipelines: Integration of structured and unstructured data
- Training methodologies: Hybrid approaches balancing supervised and unsupervised learning
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Not mentioned","- Integration of structured and unstructured data
- Hybrid approaches balancing supervised and unsupervised learning","- Data quality or availability problems: Lack of persistent identifiers used by researchers
- Algorithm performance issues: Intrinsic linguistic challenges, such as homonymy","- Integration of structured and unstructured data
- Hybrid approaches balancing supervised and unsupervised learning",Not mentioned (the abstract does not provide specific performance results or metrics),"Not mentioned (the abstract does not provide information on data sources, datasets, or data processing methods)","- Target research disciplines or fields: Digital libraries
- Specific literature review tasks addressed: Author Name Disambiguation (AND)
- Types of academic documents processed: Publications and author records
- User types and requirements: Researchers and librarians
- Integration with research workflows: Integration of structured and unstructured data
- Commercial vs. academic applications: Academic","  - ""the development of Deep Learning algorithms to address this issue has become widespread.""
  - ""Many AND deep learning methods have been developed, and surveys exist comparing the approaches in terms of techniques, complexity, performance.""
  - ""In this paper, we provide a systematic review of state-of-the-art AND techniques based on deep learning, highlighting recent improvements, challenges, and open issues in the field.""
  - ""We find that DL methods have significantly impacted AND by enabling the integration of structured and unstructured data, and hybrid approaches effectively balance supervised and unsupervised learning.""
  - ""Author Name Disambiguation (AND) is a critical task for digital libraries aiming to link existing authors with their respective publications.""",,"- The abstract mentions the use of ""Deep Learning algorithms"" for Author Name Disambiguation (AND), indicating that deep learning is a main AI/ML framework used.
- The abstract does not specify any particular system architecture components such as databases, APIs, or interfaces.
- There is no mention of technical infrastructure like cloud platforms or computational resources.
- The abstract does not provide details on integration with existing academic databases or platforms.
- The overall system design approach is not explicitly described, but it is implied that the focus is on integrating structured and unstructured data using hybrid approaches that balance supervised and unsupervised learning.","  - ""the development of Deep Learning algorithms to address this issue has become widespread.""
  - ""Many AND deep learning methods have been developed, and surveys exist comparing the approaches in terms of techniques, complexity, performance.""
  - ""In this paper, we provide a systematic review of state-of-the-art AND techniques based on deep learning, highlighting recent improvements, challenges, and open issues in the field.""
  - ""We find that DL methods have significantly impacted AND by enabling the integration of structured and unstructured data, and hybrid approaches effectively balance supervised and unsupervised learning.""
  - ""Author Name Disambiguation (AND) is a critical task for digital libraries aiming to link existing authors with their respective publications.""",,"- The abstract mentions the use of ""Deep Learning algorithms"" for Author Name Disambiguation (AND), indicating that these are the primary implementation methods.
- It refers to ""state-of-the-art AND techniques based on deep learning,"" suggesting that the paper reviews various deep learning techniques used for AND.
- The abstract highlights the integration of ""structured and unstructured data"" and the use of ""hybrid approaches"" that balance ""supervised and unsupervised learning,"" which are technical implementation details.
- However, the abstract does not provide specific technical implementation details such as algorithms, models, feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or technical workflow.","  - ""We find that DL methods have significantly impacted AND by enabling the integration of structured and unstructured data, and hybrid approaches effectively balance supervised and unsupervised learning.""
  - ""In this paper, we provide a systematic review of state-of-the-art AND techniques based on deep learning, highlighting recent improvements, challenges, and open issues in the field.""
  - ""Many AND deep learning methods have been developed, and surveys exist comparing the approaches in terms of techniques, complexity, performance.""
  - ""the development of Deep Learning algorithms to address this issue has become widespread.""
  - ""Author Name Disambiguation (AND) is a critical task for digital libraries aiming to link existing authors with their respective publications.""",,"- The abstract discusses Author Name Disambiguation (AND) and its importance in linking authors with their publications, which is a part of the broader context of paper discovery techniques.
- It mentions the development and use of Deep Learning algorithms for AND, which could include techniques like content-based matching and metadata extraction.
- The abstract highlights the integration of structured and unstructured data, which is relevant to metadata extraction and utilization.
- The mention of hybrid approaches balancing supervised and unsupervised learning suggests relevance to search and retrieval algorithms and relevance ranking approaches.
- However, the abstract does not explicitly mention specific techniques like search and retrieval algorithms, citation analysis methods, or recommendation system approaches.","  - ""Due to the lack of persistent identifiers used by researchers and the presence of intrinsic linguistic challenges, such as homonymy, the development of Deep Learning algorithms to address this issue has become widespread.""
  - ""We find that DL methods have significantly impacted AND by enabling the integration of structured and unstructured data, and hybrid approaches effectively balance supervised and unsupervised learning.""",,"- The abstract mentions ""the lack of persistent identifiers used by researchers"" as a challenge. This can be interpreted as a data quality or availability problem, as it implies that there is a lack of consistent identifiers for authors, which can complicate the disambiguation process.
- The mention of ""intrinsic linguistic challenges, such as homonymy"" suggests algorithm performance issues, as homonymy can lead to confusion in identifying authors with similar names.
- The abstract does not explicitly mention scalability challenges, integration difficulties, evaluation methodology limitations, computational resource constraints, or technical bottlenecks or failure points. However, the integration of structured and unstructured data and the balancing of supervised and unsupervised learning suggest that there are challenges related to data integration and learning approach optimization.","  - ""We find that DL methods have significantly impacted AND by enabling the integration of structured and unstructured data, and hybrid approaches effectively balance supervised and unsupervised learning.""
  - ""In this paper, we provide a systematic review of state-of-the-art AND techniques based on deep learning, highlighting recent improvements, challenges, and open issues in the field.""
  - ""Many AND deep learning methods have been developed, and surveys exist comparing the approaches in terms of techniques, complexity, performance.""
  - ""the development of Deep Learning algorithms to address this issue has become widespread.""
  - ""Author Name Disambiguation (AND) is a critical task for digital libraries aiming to link existing authors with their respective publications.""",,"- The abstract discusses the use of Deep Learning algorithms for Author Name Disambiguation (AND), which is a critical task in digital libraries. This indicates a focus on technical solutions using deep learning.
- The mention of ""many AND deep learning methods have been developed"" suggests that there are various algorithmic contributions and approaches being explored in this field.
- The abstract highlights ""recent improvements, challenges, and open issues in the field,"" which implies that there are ongoing innovations and challenges being addressed.
- The integration of ""structured and unstructured data"" and the use of ""hybrid approaches"" that balance ""supervised and unsupervised learning"" are innovative solutions that address the challenges of AND.
- These points suggest that the paper discusses novel technical approaches, such as hybrid or ensemble methods, which are innovative solutions in the context of AND.","  - ""Many AND deep learning methods have been developed, and surveys exist comparing the approaches in terms of techniques, complexity, performance.""
  - ""We find that DL methods have significantly impacted AND by enabling the integration of structured and unstructured data, and hybrid approaches effectively balance supervised and unsupervised learning.""",,"- The abstract mentions that surveys exist comparing AND methods in terms of performance, but it does not provide specific performance results such as accuracy, precision, recall, F1-scores, or other metrics.
- The abstract highlights the impact of deep learning methods on AND, particularly in integrating structured and unstructured data and balancing supervised and unsupervised learning, but it does not provide quantitative or qualitative performance outcomes.
- There is no mention of processing speed and efficiency metrics, comparison with baseline methods, user satisfaction or usability results, system reliability and robustness measures, or scalability test results in the abstract.","  - ""We find that DL methods have significantly impacted AND by enabling the integration of structured and unstructured data, and hybrid approaches effectively balance supervised and unsupervised learning.""
  - ""In this paper, we provide a systematic review of state-of-the-art AND techniques based on deep learning, highlighting recent improvements, challenges, and open issues in the field.""",,"- The abstract mentions a ""systematic review"" of deep learning-based author name disambiguation techniques, which typically involves analyzing existing literature and studies.
- The abstract does not specify any particular academic databases (e.g., Scopus, Web of Science) used for the review.
- There is no mention of specific dataset sizes, characteristics, or data collection methods.
- The abstract does not provide details on training/validation/test data splits or data preprocessing and cleaning approaches.
- There is no mention of external knowledge bases or ontologies used in the study.","  - ""Author Name Disambiguation (AND) is a critical task for digital libraries aiming to link existing authors with their respective publications.""
  - ""DL methods have significantly impacted AND by enabling the integration of structured and unstructured data, and hybrid approaches effectively balance supervised and unsupervised learning.""
  - ""we provide a systematic review of state-of-the-art AND techniques based on deep learning, highlighting recent improvements, challenges, and open issues in the field.""
  - ""the development of Deep Learning algorithms to address this issue has become widespread.""",,"- The application context is primarily related to ""digital libraries,"" which suggests that the target research disciplines or fields are those involved in managing and organizing digital academic content.
- The specific literature review task addressed is ""Author Name Disambiguation (AND),"" which is a critical task for linking authors with their publications.
- The types of academic documents processed are likely publications and author records within digital libraries.
- The user types and requirements are not explicitly mentioned, but it can be inferred that the users are researchers and librarians who need accurate author identification.
- Integration with research workflows is implied by the focus on digital libraries and the integration of structured and unstructured data.
- The application is academic rather than commercial, as it is focused on improving digital library functions and author disambiguation."
An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature,"Abhiyan Dhakal, Kausik Paudel, Sanjog Sigdel",10.48550/arXiv.2509.15292,https://doi.org/10.48550/arXiv.2509.15292,arXiv.org,0,2025,"- Main AI/ML frameworks used: Transformer-based models (all-MiniLM-L6-v2, Specter2), traditional TF-IDF
- System architecture components: arXiv API for paper retrieval, BibTeX entry generation
- Technical infrastructure: Computational resources for processing and filtering papers
- Integration with existing academic databases: arXiv, planned integration with Semantic Scholar
- Overall system design approach: Minimal input, high relevance, quick results","- Algorithms and models used: Transformer-based embeddings (all-MiniLM-L6-v2, Specter2), TF-IDF, cosine similarity.
- Feature extraction techniques: Keyword generation from titles and abstracts.
- Data processing pipelines: Retrieval of papers from open access repositories like arXiv, filtering based on semantic similarity, summarization using LLM.
- Training methodologies: Not explicitly mentioned; models are pre-trained and used for embedding generation.
- Preprocessing steps: Use of regular expressions to structure textual content.
- Technical workflow or methodology: Automated pipeline involving keyword generation, paper retrieval, semantic similarity assessment, filtering, and summarization.","- Search and retrieval algorithms: Keyword generation from paper title and abstract, retrieval from open-access repositories like arXiv.
- Relevance ranking approaches: Cosine similarity, statistical thresholding based on IQR.
- Citation analysis methods: Evaluation of embedding models (TF-IDF, all-MiniLM-L6-v2, Specter2).
- Content-based matching techniques: Transformer-based embeddings (all-MiniLM-L6-v2, Specter2).
- Metadata extraction and utilization: Use of arXiv API for metadata retrieval, generation of BibTeX entries.
- Recommendation system approaches: Not mentioned.
- Expert/authority identification methods: Not mentioned.","- Lack of human-annotated or expert-verified relevance labels
- Reliance on quantitative metrics without qualitative assessment
- General-purpose transformers may not capture domain-specific semantics
- Thresholding method does not guarantee semantic relevance
- Need for adaptive thresholding and human-in-the-loop evaluation","- Automated pipeline for literature reviews using semantic similarity
- Use of transformer-based embeddings and cosine similarity for ranking papers
- Statistical thresholding approach based on IQR for filtering papers
- Evaluation of different embedding models (TF-IDF, all-MiniLM-L6-v2, Specter2)
- Future plans: adaptive thresholding, human-in-the-loop evaluation, re-ranking techniques
- Integration with citation network data sources and optimizing query formulation techniques","- Scalability: The system is designed to reduce manual overhead and is scalable for preliminary research and exploratory analysis.
- Efficiency: The system is efficient in terms of reducing manual intervention.
- Comparison with baseline methods: TF-IDF lacks semantic meaning, all-MiniLM-L6-v2 provides balanced performance but lacks domain-specific tuning, Specter2 is best aligned with scientific language but suffers from similarity saturation.
- Future improvements: Adaptive thresholding and human-in-the-loop evaluation are planned for future work.","- Academic databases used: arXiv
- Dataset size: 178 papers
- Data collection method: Keyword-based search using arXiv API
- External knowledge bases or ontologies: Future integration with Semantic Scholar planned","- Target research disciplines or fields: General research projects across various disciplines
- Specific literature review tasks addressed: Early-stage research and exploratory analysis
- Types of academic documents processed: Papers from open-access repositories like arXiv
- User types and requirements: Researchers needing efficient literature reviews
- Integration with research workflows: Automation of keyword generation, paper retrieval, and summarization
- Commercial vs. academic applications: Focus on academic applications","  - ""We propose an automated pipeline for performing literature reviews using semantic similarity.""
  - ""By providing a paper title and abstract, it generates relevant keywords, fetches relevant papers from open access repository, and ranks them based on their semantic closeness to the input.""
  - ""Three embedding models were evaluated.""
  - ""The initial stage involved generating a set of relevant keywords to guide the literature search.""
  - ""For each keyword generated, a corpus of a maximum of 20 relevant papers was fetched from arXiv via keyword-based search using the arXiv API.""
  - ""A statistical thresholding method based on the interquartile range (IQR) of the similarity scores distribution was applied to filter the papers.""
  - ""The metadata of the relevant papers, retrieved from the arXiv API in earlier stages, was utilized to generate BibTeX entries for each paper.""
  - ""The Gemini model was instructed to synthesize this information into a comprehensive overview of the existing literature related to the research topic.""
  - ""The thresholding method employed, using the third quartile plus half the interquartile range (Q3+0.5•IQR) is a statistical heuristic designed to filter out low-similarity papers.""
  - ""The current system operates without any human-annotated or expert-verified relevance labels.""
  - ""In future work, we plan to focus on implementing adaptive thresholding to dynamically adjust based on score distribution characteristics.""
  - ""optimizing query formulation techniques will be considered to increase recall while maintaining high relevance.""","  - ""(Page 5, Table 1) | Embedding Model      | Threshold | Skewness | Value Range     | # Retrieved Papers |\n|----------------------|-----------|----------|-----------------|--------------------|\n| TF-IDF               | 0.204     | 0.622    | [0.010, 0.294]  | 19                 |""","- The paper describes an automated pipeline for literature reviews using semantic similarity, which is the core technical architecture.
- The system uses transformer-based embeddings and cosine similarity to rank papers based on their semantic closeness to the input.
- The pipeline generates keywords from the input, retrieves papers from arXiv using the arXiv API, and filters them based on semantic similarity.
- The system architecture includes the use of the arXiv API for paper retrieval and the generation of BibTeX entries for proper citation formatting.
- The main AI/ML frameworks used include transformer-based models like all-MiniLM-L6-v2 and Specter2, as well as traditional TF-IDF.
- The system integrates with existing academic databases like arXiv and plans to integrate with citation network data sources like Semantic Scholar.
- The technical infrastructure involves the use of computational resources for processing and filtering papers, although specific cloud platforms or resources are not mentioned.
- The overall system design approach emphasizes minimal input and high relevance, focusing on quick and relevant results rather than complete coverage.","  - ""Unlike traditional systematic review systems or optimization based methods, this work emphasizes minimal overhead and high relevance by using transformer based embeddings and cosine similarity.""
  - ""By providing a paper title and abstract, it generates relevant keywords, fetches relevant papers from open access repository, and ranks them based on their semantic closeness to the input.""
  - ""Three embedding models were evaluated.""
  - ""A statistical thresholding approach is then applied to filter relevant papers, enabling an effective literature review pipeline.""
  - ""A statistical thresholding method based on the interquartile range (IQR) of the similarity scores distribution was applied to filter the papers.""
  - ""For each filtered paper, either the abstract or the full PDF was processed to extract structured textual content.""
  - ""To structure the content of each paper, regular expression (regex) patterns were defined to identify and delineate different sections: Introduction, Methodology, Results, and Conclusion.""
  - ""The extracted text from each section of the relevant papers was then summarized using the gemini-2.0-flash LLM.""
  - ""To further analyze the nature and contributions of each selected paper, the Gemini LLM was employed to perform two key tasks: This approach follows prior research on citation function classification and contribution categorization in scientific literature""
  - ""The overall pipeline for generating the literature review is shown in Figure 4""
  - ""The thresholding method employed, using the third quartile plus half the interquartile range (Q3+0.5•IQR) is a statistical heuristic designed to filter out low-similarity papers.""
  - ""Evaluation of the retrieved papers depends entirely on quantitative metrics such as cosine similarity distributions, mean scores, and standard deviations.""","  - ""(Page 5, Table 1) | Embedding Model      | Threshold | Skewness | Value Range     | # Retrieved Papers |\n|----------------------|-----------|----------|-----------------|--------------------|\n| TF-IDF               | 0.204     | 0.622    | [0.010, 0.294]  | 19                 |""","- The paper describes an automated pipeline for literature reviews using semantic similarity, which involves generating keywords from input titles and abstracts.
- The pipeline uses transformer-based embeddings and cosine similarity to rank papers based on their semantic closeness to the input.
- Three embedding models are evaluated: TF-IDF, all-MiniLM-L6-v2, and Specter2, each with its own characteristics and performance metrics.
- A statistical thresholding method based on the interquartile range (IQR) is used to filter relevant papers.
- The pipeline involves extracting structured textual content from papers using regular expressions and summarizing it using a large language model (LLM).
- The system uses quantitative metrics like cosine similarity distributions to evaluate the effectiveness of the embedding models.
- The paper outlines a detailed technical workflow involving keyword generation, paper retrieval, semantic similarity assessment, and summarization.","  - ""Unlike traditional systematic review systems or optimization based methods, this work emphasizes minimal overhead and high relevance by using transformer based embeddings and cosine similarity.""
  - ""By providing a paper title and abstract, it generates relevant keywords, fetches relevant papers from open access repository, and ranks them based on their semantic closeness to the input.""
  - ""The cosine similarities between the embeddings of the fetched papers and that of the input query were then calculated.""
  - ""A statistical thresholding method based on the interquartile range (IQR) of the similarity scores distribution was applied to filter the papers.""
  - ""The metadata of the relevant papers, retrieved from the arXiv API in earlier stages, was utilized to generate BibTeX entries for each paper.""
  - ""The Gemini model was instructed to synthesize this information into a comprehensive overview of the existing literature related to the research topic.""
  - ""TF-IDF filters documents based on exact text matches, resulting in loss of semantic meaning.""
  - ""all-MiniLM-L6-v2 provides a balanced performance but lacks domain-specific tuning.""
  - ""Specter2 is best aligned with scientific language, however suffered from similarity saturation, due to rigid threshold calibration.""","  - ""(Page 5, Table 1) | Embedding Model      | Threshold | Skewness | Value Range     | # Retrieved Papers |\n|----------------------|-----------|----------|-----------------|--------------------|\n| TF-IDF               | 0.204     | 0.622    | [0.010, 0.294]  | 19                 |""","- The paper discusses the use of transformer-based embeddings and cosine similarity for semantic search, which is a content-based matching technique.
- The pipeline generates keywords from the input paper title and abstract, which is a method for search and retrieval.
- Papers are retrieved from open-access repositories like arXiv, which involves metadata extraction and utilization.
- The use of cosine similarity and statistical thresholding based on IQR is a relevance ranking approach.
- The paper evaluates different embedding models (TF-IDF, all-MiniLM-L6-v2, Specter2) for their performance in filtering relevant papers, which includes citation analysis methods.
- The system does not explicitly mention citation analysis methods or expert/authority identification methods, but it does use metadata from arXiv to generate BibTeX entries.
- The paper does not mention a recommendation system approach or expert/authority identification methods.","  - ""Despite the absence of heuristic feedback or ground truth relevance labels, the proposed system shows promise as a scalable and practical tool for preliminary research and exploratory analysis.""
  - ""The current system operates without any human-annotated or expert-verified relevance labels. This makes it challenging to assess whether the papers retrieved by the semantic similarity filter are truly relevant to the input query.""
  - ""While models like Specter2 are trained on scientific text, general-purpose transformers such as all-MiniLM-L6-v2 are optimized for broad coverage across diverse domains rather than the scholarly literature specifically.""
  - ""This mismatch can lead to embeddings that overlook subtle but important conceptual relationships, reducing retrieval precision in fields with specialized jargon or structured discourse, such as medicine, chemistry, or legal research.""
  - ""The thresholding method employed, using the third quartile plus half the interquartile range (Q3+0.5•IQR) is a statistical heuristic designed to filter out low-similarity papers. However, this approach does not guarantee semantic relevance.""
  - ""Evaluation of the retrieved papers depends entirely on quantitative metrics such as cosine similarity distributions, mean scores, and standard deviations. There is no qualitative assessment of the relevance of the chosen papers in terms of their content or research methods.""
  - ""Without involvement of human reviewers or case studies, it remains uncertain how well the system supports real-world tasks like literature reviews or knowledge discovery.""
  - ""In future work, we plan to focus on implementing adaptive thresholding to dynamically adjust based on score distribution characteristics. Incorporating human-in-the-loop evaluation will provide qualitative insights and a ground-truth basis for refining automated decisions.""
  - ""Beyond thresholding and ranking improvements, domain adaptation and model fine-tuning (e.g., on specialized corpora) will be investigated to better capture field-specific semantics.""",,"- The paper highlights several technical challenges related to the proposed automated literature review system. One major issue is the lack of human-annotated or expert-verified relevance labels, which makes it difficult to assess the true relevance of retrieved papers.
- The system's reliance on quantitative metrics without qualitative assessment is another challenge, as it may not accurately reflect the relevance of papers in terms of content or research methods.
- The use of general-purpose transformers like all-MiniLM-L6-v2 may not capture domain-specific semantics as effectively as models trained on scientific text, leading to reduced retrieval precision in specialized fields.
- The thresholding method used does not guarantee semantic relevance, and the system lacks adaptive thresholding, which could improve its performance.
- The paper also mentions the need for future work to address these challenges, including implementing adaptive thresholding and incorporating human-in-the-loop evaluation to refine automated decisions.","  - ""We propose an automated pipeline for performing literature reviews using semantic similarity.""
  - ""Unlike traditional systematic review systems or optimization based methods, this work emphasizes minimal overhead and high relevance by using transformer based embeddings and cosine similarity.""
  - ""By providing a paper title and abstract, it generates relevant keywords, fetches relevant papers from open access repository, and ranks them based on their semantic closeness to the input.""
  - ""A statistical thresholding approach is then applied to filter relevant papers, enabling an effective literature review pipeline.""
  - ""This approach is useful for early-stage research and exploratory analysis, focusing on quick and relevant results rather than complete coverage.""
  - ""The pipeline consisted of seven key stages, each designed to efficiently and systematically process scholarly articles relevant to the research topic.""
  - ""The initial stage involved generating a set of relevant keywords to guide the literature search.""
  - ""A statistical thresholding method based on the interquartile range (IQR) of the similarity scores distribution was applied to filter the papers.""
  - ""In future work, we plan to focus on implementing adaptive thresholding to dynamically adjust based on score distribution characteristics.""
  - ""Incorporating human-in-the-loop evaluation will provide qualitative insights and a ground-truth basis for refining automated decisions.""
  - ""Re-ranking techniques will be explored to help improve the precision and contextual relevance of the retrieved papers.""
  - ""domain adaptation and model fine-tuning (e.g., on specialized corpora) will be investigated to better capture field-specific semantics.""
  - ""optimizing query formulation techniques will be considered to increase recall while maintaining high relevance.""",,"- The paper introduces an innovative automated pipeline for literature reviews using semantic similarity, which is a novel approach compared to traditional systematic review systems.
- The use of transformer-based embeddings and cosine similarity for ranking papers is a new algorithmic contribution, as it emphasizes minimal overhead and high relevance.
- The pipeline's ability to generate keywords and fetch papers from open-access repositories like arXiv is a creative problem-solving method for efficient literature discovery.
- The statistical thresholding approach based on IQR is a technical workaround for filtering relevant papers without relying on heuristic feedback or ground truth relevance labels.
- The evaluation of different embedding models (TF-IDF, all-MiniLM-L6-v2, Specter2) is an innovative evaluation metric, as it compares their performance in semantic retrieval.
- Future plans include implementing adaptive thresholding, incorporating human-in-the-loop evaluation, and exploring re-ranking techniques, which are novel feature engineering techniques and technical workarounds.
- The integration with citation network data sources and optimizing query formulation techniques are original system design elements aimed at enhancing retrieval accuracy and literature synthesis.","  - ""the proposed system shows promise as a scalable and practical tool for preliminary research and exploratory analysis.""
  - ""This approach is useful for early-stage research and exploratory analysis, focusing on quick and relevant results rather than complete coverage.""
  - ""A statistical thresholding approach is then applied to filter relevant papers, enabling an effective literature review pipeline.""
  - ""The IQR method is a robust, distribution-free approach for outlier detection that does not assume normality of the data""
  - ""The cosine similarities between the embeddings of the fetched papers and that of the input query were then calculated.""
  - ""The current system operates without any human-annotated or expert-verified relevance labels.""
  - ""The thresholding method employed, using the third quartile plus half the interquartile range (Q3+0.5•IQR) is a statistical heuristic designed to filter out low-similarity papers.""
  - ""Evaluation of the retrieved papers depends entirely on quantitative metrics such as cosine similarity distributions, mean scores, and standard deviations.""
  - ""TF-IDF filters documents based on exact text matches, resulting in loss of semantic meaning.""
  - ""all-MiniLM-L6-v2 provides a balanced performance but lacks domain-specific tuning.""
  - ""Specter2 is best aligned with scientific language, however suffered from similarity saturation, due to rigid threshold calibration.""
  - ""In future work, we plan to focus on implementing adaptive thresholding to dynamically adjust based on score distribution characteristics.""
  - ""Incorporating human-in-the-loop evaluation will provide qualitative insights and a ground-truth basis for refining automated decisions.""
  - ""Re-ranking techniques will be explored to help improve the precision and contextual relevance of the retrieved papers.""
  - ""This work has shown the potential to reduce manual overhead in writing literature reviews, while maintaining relevance on topics, proving it is an scalable and accessible alternative to traditional SLR systems.""","  - ""(Page 5, Table 1) | Embedding Model      | Threshold | Skewness | Value Range     | # Retrieved Papers |\n|----------------------|-----------|----------|-----------------|--------------------|\n| TF-IDF               | 0.204     | 0.622    | [0.010, 0.294]  | 19                 |""","- The paper discusses the performance of the proposed system in terms of its ability to reduce manual overhead and maintain relevance, indicating a focus on efficiency and scalability.
- The use of semantic similarity and cosine similarity suggests a quantitative approach to evaluating the relevance of papers, but no specific accuracy, precision, recall, or F1-scores are mentioned.
- The system's performance is evaluated using quantitative metrics such as cosine similarity distributions, but there is no mention of processing speed or efficiency metrics.
- The comparison with baseline methods is implicit in the discussion of TF-IDF, all-MiniLM-L6-v2, and Specter2, but no direct comparison metrics are provided.
- There is no mention of user satisfaction or usability results, nor are there any system reliability and robustness measures discussed.
- The scalability test results are implied by the system's ability to handle a large number of papers, but no specific metrics are provided.
- The table at the end of the paper likely contains performance metrics for the embedding models, but without specific values, it's difficult to extract detailed performance results.","  - ""For each keyword generated, a corpus of a maximum of 20 relevant papers was fetched from arXiv via keyword-based search using the arXiv API.""
  - ""The metadata of the relevant papers, retrieved from the arXiv API in earlier stages, was utilized to generate BibTeX entries for each paper.""
  - ""A total of 178 papers were fetched from arXiv API.""
  - ""The current system operates without any human-annotated or expert-verified relevance labels.""
  - ""In future work, we plan to focus on implementing adaptive thresholding to dynamically adjust based on score distribution characteristics. Incorporating human-in-the-loop evaluation will provide qualitative insights and a ground-truth basis for refining automated decisions.""
  - ""Integration with citation network data sources such as Semantic Scholar [23] can enable network-based relevance scoring (e.g., PageRank-style importance measures, co-citation patterns), which may enhance both retrieval accuracy and literature synthesis.""","  - ""(Page 5, Table 1) | Embedding Model      | Threshold | Skewness | Value Range     | # Retrieved Papers |\n|----------------------|-----------|----------|-----------------|--------------------|\n| TF-IDF               | 0.204     | 0.622    | [0.010, 0.294]  | 19                 |""","- The paper mentions that the data source used is arXiv, from which a corpus of up to 20 relevant papers is fetched for each keyword using the arXiv API. This indicates that arXiv is the primary academic database used.
- The paper does not specify any other academic databases like Scopus or Web of Science.
- The dataset size is mentioned as 178 papers fetched from arXiv.
- The data collection method involves keyword-based search using the arXiv API.
- There is no mention of training/validation/test data splits or specific data preprocessing and cleaning approaches.
- The paper does not use any external knowledge bases or ontologies, but it suggests future integration with Semantic Scholar for network-based relevance scoring.
- The table on page 5 provides a summary of embedding models and their performance but does not contain information about data sources or dataset characteristics.","  - ""Conducting a literature review is an important step in any research project. By examining existing studies, researchers understand the progress that has been made in their field.""
  - ""This approach is useful for early-stage research and exploratory analysis, focusing on quick and relevant results rather than complete coverage.""
  - ""The overall pipeline for generating the literature review is shown in Figure 4 . It begins with metadata retrieval from the arXiv API, followed by BibTeX entry generation, paper summarization and tagging using the Gemini LLM, and finally synthesis of the literature review paragraph based on these structured inputs.""
  - ""To evaluate the effectiveness of embedding models in filtering relevant scientific papers for automated literature review, we experimented with three distinct approaches for research paper similarity:""
  - ""This work presents an automated pipeline for conducting targeted literature reviews using semantic similarity.""
  - ""In future work, we plan to focus on implementing adaptive thresholding to dynamically adjust based on score distribution characteristics. Incorporating human-in-the-loop evaluation will provide qualitative insights and a ground-truth basis for refining automated decisions.""
  - ""This work has shown the potential to reduce manual overhead in writing literature reviews, while maintaining relevance on topics, proving it is an scalable and accessible alternative to traditional SLR systems.""","  - ""(Page 5, Table 1) | Embedding Model      | Threshold | Skewness | Value Range     | # Retrieved Papers |\n|----------------------|-----------|----------|-----------------|--------------------|\n| TF-IDF               | 0.204     | 0.622    | [0.010, 0.294]  | 19                 |""","- The paper discusses the application of an automated pipeline for conducting literature reviews, which is a key task in research projects across various disciplines.
- The system is designed for early-stage research and exploratory analysis, indicating its use in initial phases of research projects.
- The pipeline processes academic documents from open-access repositories like arXiv, suggesting its application in fields where such repositories are commonly used.
- The system is intended for researchers who need to conduct literature reviews efficiently, indicating its user type.
- The integration with research workflows is implied by the automation of tasks such as keyword generation, paper retrieval, and summarization.
- The paper does not explicitly mention commercial applications, suggesting its focus is on academic use.
- The table provides a summary of embedding models and their performance, which is relevant to understanding the technical implementation and challenges in matching and finding core papers."
Fast clinical trial identification using fuzzy-search elastic searches: retrospective validation with high-quality Cochrane benchmark,"Willem M Otte, D. G. V. IJzendoorn, P. Habets, C. Vinkers",10.1101/2023.09.06.23295135,https://doi.org/10.1101/2023.09.06.23295135,medRxiv,0,2023,"- Main AI/ML frameworks used: NLP, PubMedBERT Named-Entity Recognition model
- System architecture components: Fuzzy-enabled elastic search database, online interface for flexible text input, query building blocks
- Technical infrastructure: Not explicitly detailed, but likely relies on advanced computational resources for NLP and large-language models
- Integration with existing academic databases or platforms: Daily synchronization with PubMed
- Overall system design approach: Focus on NLP for extracting relevant subtext and user-friendly interface for searching clinical trials","- Algorithms and models used: PubMedBERT Named-Entity Recognition model
- Feature extraction techniques: Inside-Outside-Beginning (IOB) tagging scheme
- Data processing pipelines: Division into training, validation, and test sets
- Training methodologies: Finetuning with specific hyperparameters (7 epochs, learning rate of 0.0001, 400 warm-up steps, batch size of 16)
- Preprocessing steps: Conversion of annotated abstract text into IOB tagging scheme
- Technical workflow or methodology: Development of a fuzzy-enabled elastic search database with reliable filters for study identification and subtext extraction","- Search and retrieval algorithms: Fuzzy-enabled elastic search database, PubMedBERT for named-entity recognition
- Relevance ranking approaches: Flexible text input and query building
- Metadata extraction and utilization: Filters for identifying study types and extracting Population and Intervention subtext
- Content-based matching techniques: NLP techniques for identifying relevant studies","- Algorithm performance issues: Need for carefully composed keyword sets and Boolean operators in databases like PubMed.
- Data quality or availability problems: Lack of standardized format in PubMed abstracts; reliance on abstract text for trial identification; exclusion of non-free databases like Embase, Web of Science, and Scopus.
- Scalability challenges: Complexity and manual effort required for searching multiple databases.
- Integration difficulties: Focus on interventional reviews with PICO structures limits applicability to other types of reviews.
- Evaluation methodology limitations: Reliance on abstract text for validation may not accurately reflect full-text content.","- New algorithmic contributions: Fuzzy search capabilities, reliable filters for study identification, and NLP-based meta-information generation.
- Creative problem-solving methods: Use of NLP for Population and Intervention subtext extraction.
- Hybrid or ensemble approaches: Extension of PICO text labeling with NER models, integration of large-language models.
- Novel feature engineering techniques: Development of reliable filters for study identification.
- Innovative evaluation metrics: Precision, recall, and F1-score for subtext detection.
- Technical workarounds for known problems: Use of open-source language models like MedAlpaca.
- Original system design elements: Online interface with flexible text input and query building blocks.","- Accuracy, precision, recall, F1-scores: Precision of 0.74, recall of 0.81, F1-score of 0.77 for Population; precision of 0.70, recall of 0.71, F1-score of 0.70 for Intervention.
- Processing speed and efficiency metrics: Simplifies and speeds up identifying relevant clinical trials; median hits below 788.
- Comparison with baseline methods: Misses no more than two trials in 90% of reviews; fewer hits compared to PubMed keyword search.
- User satisfaction or usability results: Described as user-friendly and fast.
- System reliability and robustness measures: Handles large volume of data; consistent results.
- Scalability test results: Processes large number of PubMed entries efficiently.","- Academic databases used: PubMed
- Dataset sizes and characteristics: 36 million PubMed-indexed entries; training set of 50,000 PubMed abstracts; test dataset of 5,000 abstracts
- Data collection methods: Active learning strategy; multiple raters; daily updates from PubMed
- Training/validation/test data splits: Train (1020), validation (170), test (170)
- Data preprocessing and cleaning approaches: Use of PubMedBERT for text classification; annotation with Doccano
- External knowledge bases or ontologies used: PubMedBERT; Doccano","- Target research disciplines or fields: Medical and health sciences
- Specific literature review tasks addressed: Identifying relevant studies for systematic reviews, extracting population and intervention information from abstracts
- Types of academic documents processed: PubMed-indexed entries (abstracts and potentially full-text articles)
- User types and requirements: Researchers and clinicians conducting systematic reviews
- Integration with research workflows: Simplifies and speeds up identification of relevant clinical trials
- Commercial vs. academic applications: Academic, freely available tool","  - ""We built a stand-alone, freely available, fuzzy-enabled elastic search database containing all 36 million PubMed-indexed entries.""
  - ""The database is daily synchronized with PubMed.""
  - ""We developed and validated reliable filters to identify randomized clinical trials and other clinical intervention studies and extract Population and Intervention-relevant subtext.""
  - ""We combined the study identification, Population, and Intervention subtext extraction with an online interface, allowing flexible text input.""
  - ""The interface is available at: https://evidencehunt.com/browse/, and searching within 'randomized trial' and 'clinical intervention' entries is facilitated in the interface's search fields by distinguishing three types of query building blocks suitable for different combinations of words: 'all the words', 'at least one of the words', and 'without the words'.""
  - ""we finetuned a PubMedBERT Named-Entity Recognition word classification model on the training set and searched for the optimal hyperparameters using the validation set.""
  - ""The independent test set characterized the performance in terms of precision, recall, and F1-Score evaluation metrics.""
  - ""our platform is user-friendly, fast, allows automatic notification, and is more up-to-date with the latest text-mining algorithms and database handling.""
  - ""More rigorous search and database transformations are within reach with the latest development of large-language models (Singhal et al., 2023).""
  - ""Open-source language models like MedAlpaca are worth investigating if effectively coupled with the PubMed database and a user-friendly 'chat' platform (Han et al., 2023)""",,"- The paper describes the development of a fuzzy-enabled elastic search database that contains all PubMed-indexed entries, which is a key component of the technical architecture.
- The database is synchronized daily with PubMed, indicating integration with existing academic databases.
- The system uses natural language processing (NLP) techniques, specifically a PubMedBERT Named-Entity Recognition model, for extracting relevant subtext.
- The system architecture includes an online interface for flexible text input and query building blocks, which are part of the system components.
- The paper mentions the use of large-language models and open-source language models like MedAlpaca, indicating potential future directions in AI/ML frameworks.
- The technical infrastructure is not explicitly detailed in terms of cloud platforms or computational resources, but the focus on NLP and large-language models suggests a reliance on advanced computational capabilities.","  - ""We built a stand-alone, freely available, fuzzy-enabled elastic search database containing all 36 million PubMed-indexed entries.""
  - ""We developed and validated reliable filters to identify randomized clinical trials and other clinical intervention studies and extract Population and Intervention-relevant subtext.""
  - ""We developed a text classification model to identify the words and sentences associated with Population and Intervention in every PubMed abstract.""
  - ""we finetuned a PubMedBERT Named-Entity Recognition word classification model on the training set and searched for the optimal hyperparameters using the validation set.""
  - ""We finetuned the model for seven epochs with a learning rate of 0.0001, 400 warm-up steps, and a batch size of sixteen with the pre-trained weights of the PubMedBERT 'base' (abstract-only) network architecture as a starting point.""
  - ""The interface is available at: https://evidencehunt.com/browse/, and searching within 'randomized trial' and 'clinical intervention' entries is facilitated in the interface's search fields by distinguishing three types of query building blocks suitable for different combinations of words: 'all the words', 'at least one of the words', and 'without the words'.""
  - ""The building blocks are available for subtext identified as belonging to descriptive information of clinical 'Population' (P) or 'Intervention' (I).""
  - ""We combined the study identification, Population, and Intervention subtext extraction with an online interface, allowing flexible text input.""
  - ""The annotated abstract text was converted into an Inside-Outside-Beginning (IOB) tagging scheme and randomly divided into a train (sample size: 1020), validation (170), and test (170) dataset.""
  - ""The independent test set characterized the performance in terms of precision, recall, and F1-Score evaluation metrics.""",,"- The paper describes the development of a fuzzy-enabled elastic search database, which is a key technical implementation detail.
- The use of reliable filters to identify specific study types and extract relevant subtext indicates a classification model.
- The text classification model is specifically mentioned as being used to identify Population and Intervention-related words and sentences.
- The paper details the use of a PubMedBERT Named-Entity Recognition model, which is a specific algorithm used for text classification.
- The training methodology includes finetuning the model with specific hyperparameters, indicating a detailed training process.
- The use of an Inside-Outside-Beginning (IOB) tagging scheme for text annotation is a feature extraction technique.
- The division of data into training, validation, and test sets is a standard data processing pipeline.
- The evaluation metrics of precision, recall, and F1-score are used to assess the model's performance.","  - ""New database architectures and natural language processing (NLP) techniques have recently emerged that may streamline the systematic review process.""
  - ""We built a stand-alone, freely available, fuzzy-enabled elastic search database containing all 36 million PubMed-indexed entries.""
  - ""We developed and validated reliable filters to identify randomized clinical trials and other clinical intervention studies and extract Population and Intervention-relevant subtext.""
  - ""We developed a text classification model to identify the words and sentences associated with Population and Intervention in every PubMed abstract.""
  - ""we finetuned a PubMedBERT Named-Entity Recognition word classification model on the training set and searched for the optimal hyperparameters using the validation set.""
  - ""The interface is available at: https://evidencehunt.com/browse/, and searching within 'randomized trial' and 'clinical intervention' entries is facilitated in the interface's search fields by distinguishing three types of query building blocks suitable for different combinations of words: 'all the words', 'at least one of the words', and 'without the words'.""
  - ""To validate the usefulness of our approach, which integrates a newly structured NLP-based database with validated study types and approximate search of Population and Intervention into an online platform, we sampled reviews from the systematic reviews in the CDSR published in their first 2022 Issue (April 4).""
  - ""Queries were short, with a maximum length of 21 words (median: four).""
  - ""In 90% of systematic reviews (27/30), the new search strategy missed no more than two of all included trials by Cochrane,""
  - ""The average number of hits returned from PubMed was significantly higher (median 952;

Table 2 ): Compared to the new approach, twenty-six of the thirty reviews returned higher volumes on PubMed (proportion: 0.87, 95% confidence interval: 0.69-0.96, p < 0.0001; exact binomial test).""
  - ""This study shows that an additional software layer between the user and the PubMed database can simplify and speed up identifying relevant clinical trials.""
  - ""With a straightforward set of keywords related to patient population and type of intervention, combined with PICO-text identification and a specific and sensitive study filter, key trials can be retrospectively identified as used in Cochrane systematic reviews.""
  - ""Our study has limitations. 1) We validated our keyword combinations on a retrospective set of included trials in published Cochrane reviews.""
  - ""Other future directions include an extension of the PICO text labeling with NER models suitable for diagnostic and prognostic queries.""","  - ""(Page 13, Table 1) | Reference | URL                                      |\n|-----------|------------------------------------------|\n| 18        | https://classic.clinicaltrials.gov       |""
  - ""(Page 19, Table 1) | Cochrane review   | Title                                                                                                                                                                                                 | Included |\n|-------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|\n| 1, CD013739.pub0  | Prophylactic anticoagulants for people hospitalised with COVID-19                                                                                                                                     | yes      |""
  - ""(Page 21, Table 1) Below is a rendering of the page up to the first error.```markdown\n| Cochrane review       | Systemic review title                                                                 | Population text                                                                                      | Intervention text                                                                                                                                                                                                 | PM hits (N) | EH hits (N) | N trials avail. | N trials identif. | % trials identif. |\n|-----------------------|---------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|-------------|-----------------|-------------------|-------------------|""
  - ""(Page 22, Table 1) Below is a rendering of the page up to the first error.Table 1: Study Details and Outcomes\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| ID, Reference       | Intervention                                                                 | Condition                                                                 | Treatment                                                                 | N    | Events | Studies | Comparisons | Certainty |""
  - ""(Page 23, Table 1) | Number, ID          | Title                                                                 | Condition                        | Intervention                                                                 | Total | Randomized | Studies | Results | Percentage |\n|---------------------|-----------------------------------------------------------------------|----------------------------------|------------------------------------------------------------------------------|-------|------------|---------|---------|------------|\n| 44, CD015017.pub2   | Ivermectin for preventing and treating COVID-19                       | covid corona sars-cov-2          | ivermectin mectizan stromectol antiparasitic                                 | 194   | 153        | 7       | 6       | 86%        |""","- The paper discusses the use of natural language processing (NLP) techniques and new database architectures to streamline the systematic review process, which is relevant to content-based matching techniques.
- The development of a fuzzy-enabled elastic search database and the use of PubMedBERT for named-entity recognition are examples of search and retrieval algorithms.
- The paper mentions the use of filters to identify specific study types and extract relevant subtext, which relates to metadata extraction and utilization.
- The interface described allows for flexible text input and query building, which can be seen as a relevance ranking approach.
- The validation process involved comparing the new search strategy with traditional PubMed searches, indicating a focus on improving search efficiency and accuracy.
- The paper does not explicitly mention citation analysis methods, recommendation system approaches, or expert/authority identification methods.","  - ""The reason for the duration and efforts to arrive at systematic reviews and meta-analyses is that these searches are complex and require extensive manual work that spans several databases.""
  - ""Moreover, databases such as MEDLINE and its interface PubMed require carefully composed keyword sets and Boolean operators.""
  - ""Unfortunately, PubMed abstracts summarizing trials do not follow a standardized format.""
  - ""Our validation was limited due to trial identification based on abstract text only.""
  - ""Manual raters investigate the full text to decide whether or not to include a clinical trial report in the aggregated systematic review analysis.""
  - ""On the other hand, we relied on Patient and Intervention sentences available in the abstract-the non-structured nature of trial summaries results in missing information in some cases.""
  - ""Our study has limitations. 1) We validated our keyword combinations on a retrospective set of included trials in published Cochrane reviews.""
  - ""2) We focused on systematic reviews with an interventional, and hence PICOstructured, research question.""
  - ""3) PubMed contains most of the international biomedical literature. Still, our software and validation do not include different non-free databases, including available Embase, Web of Science, and Scopus.""
  - ""4) Our validation was limited due to trial identification based on abstract text only.""",,"- The paper discusses several technical challenges related to conducting systematic reviews and meta-analyses. One major challenge is the complexity and manual effort required for searching multiple databases, which is a scalability challenge.
- The need for carefully composed keyword sets and Boolean operators in databases like PubMed is an algorithm performance issue, as it requires precise and time-consuming query construction.
- The lack of standardized format in PubMed abstracts is a data quality problem, as it complicates the extraction of relevant information.
- The limitation of relying on abstract text for trial identification is a data availability problem, as full-text analysis is not always possible due to licensing restrictions.
- The focus on interventional reviews with PICO structures is an integration difficulty, as it limits the applicability to other types of reviews.
- The exclusion of non-free databases like Embase, Web of Science, and Scopus is a data availability problem, as it restricts the scope of the search.
- The reliance on abstract text for validation is an evaluation methodology limitation, as it may not accurately reflect the full-text content.","  - ""New database architectures and natural language processing (NLP) techniques have recently emerged that may streamline the systematic review process.""
  - ""We built a stand-alone, freely available, fuzzy-enabled elastic search database containing all 36 million PubMed-indexed entries.""
  - ""We developed and validated reliable filters to identify randomized clinical trials and other clinical intervention studies and extract Population and Intervention-relevant subtext.""
  - ""Relevant subtexts were detected with a precision of 0.74, recall of 0.81, and F1-score of 0.77 for the Population subtext, and a precision of 0.70, recall of 0.71, and an F1-score of 0.70 for the Intervention subtext.""
  - ""We combined the study identification, Population, and Intervention subtext extraction with an online interface, allowing flexible text input.""
  - ""The interface is available at: https://evidencehunt.com/browse/, and searching within 'randomized trial' and 'clinical intervention' entries is facilitated in the interface's search fields by distinguishing three types of query building blocks suitable for different combinations of words: 'all the words', 'at least one of the words', and 'without the words'.""
  - ""Our study has limitations. 1) We validated our keyword combinations on a retrospective set of included trials in published Cochrane reviews.""
  - ""Other future directions include an extension of the PICO text labeling with NER models suitable for diagnostic and prognostic queries.""
  - ""More rigorous search and database transformations are within reach with the latest development of large-language models (Singhal et al., 2023).""
  - ""Open-source language models like MedAlpaca are worth investigating if effectively coupled with the PubMed database and a user-friendly 'chat' platform (Han et al., 2023)""",,"- The paper introduces a new database architecture that uses fuzzy search capabilities, which is an innovative approach to streamline the systematic review process.
- The development of reliable filters to identify specific study types and extract relevant subtext is a novel feature engineering technique.
- The use of NLP to generate meta-information and extract Population and Intervention subtext is a creative problem-solving method.
- The online interface with flexible text input and query building blocks is an original system design element.
- The paper discusses future directions involving the extension of PICO text labeling with NER models and the integration of large-language models, which are potential hybrid or ensemble approaches.
- The mention of open-source language models like MedAlpaca suggests a technical workaround for known problems in literature search.","  - ""Relevant subtexts were detected with a precision of 0.74, recall of 0.81, and F1-score of 0.77 for the Population subtext, and a precision of 0.70, recall of 0.71, and an F1-score of 0.70 for the Intervention subtext.""
  - ""In 90% of systematic reviews (27/30), the new search strategy missed no more than two of all included trials by Cochrane, yet keeping the total hits lower compared to a comparable PubMed keyword search (87%; 26/30).""
  - ""The number of hits returned from the search varied considerably, with a minimum of five and a maximum of 9775 (Table 2 ). In most searches (i.e., 75%), the number was below 788 (median: 371).""
  - ""The average number of hits returned from PubMed was significantly higher (median 952;

Table 2 ): Compared to the new approach, twenty-six of the thirty reviews returned higher volumes on PubMed (proportion: 0.87, 95% confidence interval: 0.69-0.96, p < 0.0001; exact binomial test).""
  - ""This study shows that an additional software layer between the user and the PubMed database can simplify and speed up identifying relevant clinical trials.""
  - ""our platform is user-friendly, fast, allows automatic notification, and is more up-to-date with the latest text-mining algorithms and database handling.""
  - ""The acquisition of relevant evidence in making scientific progress and managing health-related decisions is becoming increasingly difficult due to the large volume of published trials.""",,"- The paper provides quantitative performance metrics such as precision, recall, and F1-scores for the detection of Population and Intervention subtexts, which are key components of the system's accuracy.
- The comparison with baseline methods is highlighted by the performance of the new search strategy against Cochrane reviews and PubMed keyword searches, showing that it misses fewer trials and returns fewer hits.
- The paper mentions the efficiency and speed of the system, indicating that it simplifies and speeds up the identification of relevant clinical trials.
- User satisfaction and usability are implied by the description of the platform as ""user-friendly"" and ""fast,"" although no specific user satisfaction metrics are provided.
- System reliability and robustness are suggested by the ability to handle a large volume of data and provide consistent results.
- Scalability is implied by the ability to process a large number of PubMed entries and provide results efficiently.","  - ""We built a stand-alone, freely available, fuzzy-enabled elastic search database containing all 36 million PubMed-indexed entries.""
  - ""The database is daily synchronized with PubMed.""
  - ""We developed and validated reliable filters to identify randomized clinical trials and other clinical intervention studies and extract Population and Intervention-relevant subtext.""
  - ""we used an active learning strategy and multiple raters to create a large training set of 50,000 PubMed abstracts.""
  - ""The model achieved an average sensitivity and specificity of 0.94 and 0.96, respectively, on an external dataset of 5,000 abstracts.""
  - ""We developed a text classification model to identify the words and sentences associated with Population and Intervention in every PubMed abstract.""
  - ""An independent physician identified all words related to the patient Population and Intervention in 1,360 random sampled 'randomized trial' and 'clinical intervention' entry abstracts within the open-source text annotation tool Doccano.""
  - ""The annotated abstract text was converted into an Inside-Outside-Beginning (IOB) tagging scheme and randomly divided into a train (sample size: 1020), validation (170), and test (170) dataset.""
  - ""we finetuned a PubMedBERT Named-Entity Recognition word classification model on the training set and searched for the optimal hyperparameters using the validation set.""
  - ""The independent test set characterized the performance in terms of precision, recall, and F1-Score evaluation metrics.""
  - ""We combined the study identification, Population, and Intervention subtext extraction with an online interface, allowing flexible text input.""
  - ""Our validation was limited due to trial identification based on abstract text only.""
  - ""Fortunately, the free PubMed Central (PMC) archive of biomedical and life sciences journal literature is growing exponentially, with clinical reports and preprints made available under license terms that allow reuse.""",,"- The primary data source used in this study is PubMed, as indicated by the construction of a database containing all 36 million PubMed-indexed entries.
- The database is updated daily, ensuring that it remains current with new publications.
- The study uses a large training set of 50,000 PubMed abstracts for developing and validating filters to identify clinical trials and extract relevant subtext.
- The training, validation, and test datasets were split from the annotated abstracts, with specific numbers provided for each set.
- The study employs an active learning strategy and multiple raters for annotation, indicating a robust data collection method.
- The use of PubMedBERT for text classification suggests the integration of external knowledge bases or ontologies specific to biomedical literature.
- The study mentions the use of the Doccano tool for annotation, which is an external resource for text annotation.
- The focus on PubMed and its daily updates indicates that the study relies heavily on this academic database for data collection.","  - ""Evidence-based medicine relies on systematic reviews of randomized controlled trials and other clinical intervention studies to synthesize the latest and most valid estimates of treatment effects.""
  - ""Our study compares the effectiveness of NLP-based literature searches within a new database structure to the yield of the Cochrane Database of Systematic Reviews study sets - currently the gold standard.""
  - ""We built a stand-alone, freely available, fuzzy-enabled elastic search database containing all 36 million PubMed-indexed entries.""
  - ""We developed and validated reliable filters to identify randomized clinical trials and other clinical intervention studies and extract Population and Intervention-relevant subtext.""
  - ""In 90% of systematic reviews (27/30), the new search strategy missed no more than two of all included trials by Cochrane,""
  - ""This study shows that an additional software layer between the user and the PubMed database can simplify and speed up identifying relevant clinical trials.""
  - ""our platform is user-friendly, fast, allows automatic notification, and is more up-to-date with the latest text-mining algorithms and database handling.""
  - ""We restricted our study to the (single) freely available PubMed database.""
  - ""Other future directions include an extension of the PICO text labeling with NER models suitable for diagnostic and prognostic queries.""","  - ""(Page 13, Table 1) | Reference | URL                                      |\n|-----------|------------------------------------------|\n| 18        | https://classic.clinicaltrials.gov       |""
  - ""(Page 19, Table 1) | Cochrane review   | Title                                                                                                                                                                                                 | Included |\n|-------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|\n| 1, CD013739.pub0  | Prophylactic anticoagulants for people hospitalised with COVID-19                                                                                                                                     | yes      |""
  - ""<table_quotation page_num=21 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=22 table_on_page=1></table_quotation>""
  - ""(Page 23, Table 1) | Number, ID          | Title                                                                 | Condition                        | Intervention                                                                 | Total | Randomized | Studies | Results | Percentage |\n|---------------------|-----------------------------------------------------------------------|----------------------------------|------------------------------------------------------------------------------|-------|------------|---------|---------|------------|\n| 44, CD015017.pub2   | Ivermectin for preventing and treating COVID-19                       | covid corona sars-cov-2          | ivermectin mectizan stromectol antiparasitic                                 | 194   | 153        | 7       | 6       | 86%        |""","- The paper focuses on improving the efficiency of systematic reviews in evidence-based medicine, particularly in identifying randomized controlled trials and other clinical intervention studies.
- The target research disciplines or fields are primarily medical and health sciences, as indicated by the use of PubMed and the focus on clinical trials.
- The specific literature review tasks addressed include identifying relevant studies for systematic reviews and extracting population and intervention information from abstracts.
- The types of academic documents processed are primarily PubMed-indexed entries, which include abstracts and potentially full-text articles.
- The user types and requirements suggest that the platform is designed for researchers and clinicians who need to conduct systematic reviews efficiently.
- Integration with research workflows is implied by the ability to simplify and speed up the identification of relevant clinical trials.
- The application is academic rather than commercial, as it is described as a freely available tool.
- Future directions include integrating with clinical trial registration registries and preprint databases, which suggests potential for broader application in research workflows."
Transformer-based Architecture for Assisting Title and Abstract Screening of a Systematic Review,"M. Canaparo, S.C. Todeschini, E. Ronchieri",10.1109/NSSMICRTSD49126.2023.10338654,https://doi.org/10.1109/NSSMICRTSD49126.2023.10338654,"2023 IEEE Nuclear Science Symposium, Medical Imaging Conference and International Symposium on Room-Temperature Semiconductor Detectors (NSS MIC RTSD)",0,2023,"- Main AI/ML frameworks used: Pre-trained transformer models (BERT, LITMC-BERT, PubMedBERT)
- System architecture components: Not mentioned
- Technical infrastructure: Not mentioned
- Integration with existing academic databases or platforms: Not mentioned
- Overall system design approach: Not mentioned","- Algorithms and models used: Pre-trained transformer models (BERT, LITMC-BERT, PubMedBERT)
- Feature extraction techniques: Not mentioned
- Data processing pipelines: Not mentioned
- Training methodologies: Not mentioned
- Preprocessing steps: Not mentioned
- Technical workflow or methodology: Not mentioned","- Search and retrieval algorithms: Keyword searches
- Relevance ranking approaches: Pre-trained transformer models (BERT, LITMC-BERT, PubMedBERT)
- Content-based matching techniques: Text analysis using transformer models","- Efficiency and scalability challenges due to laborious manual methods
- Data quality or availability problems due to rapid growth of research on COVID-19","- New algorithmic contributions: Use of pre-trained transformer models (BERT, LITMC-BERT, PubMedBERT) for document screening.
- Creative problem-solving methods: Categorization of documents based on specific COVID-19 related information.
- Technical workarounds: Evaluation using a test set from independent reviewers.
- Original system design elements: Generalizability of the approach.",Not mentioned (the abstract does not provide specific quantitative or qualitative performance outcomes),"Not mentioned (the abstract does not provide specific information on data sources, dataset sizes, data collection methods, training/validation/test data splits, data preprocessing, or external knowledge bases)","- Target research disciplines or fields: Artificial intelligence research on COVID-19
- Specific literature review tasks addressed: Screening of papers
- Types of academic documents processed: Papers related to artificial intelligence research on COVID-19
- User types and requirements: Researchers
- Integration with research workflows: Facilitates screening process using pre-trained transformer models
- Commercial vs. academic applications: Academic","  - ""Our approach employs pre-trained transformer models, such as Bidirectional Encoder Representations from Transformers (BERT), LITMC-BERT and PubMedBERT, for papers about artificial intelligence research on COVID-19 for the purposes of detection, diagnosis, and prediction.""
  - ""The proposed models have been evaluated for screening performance using a test set that comprises articles retrieved from the searches of three independent reviewers, who have assessed the eligibility of each paper.""
  - ""Our models find relevant documents and categorize them according to their information on COVID-19, such as diseases, treatments, and vaccinations.""",,"- The abstract mentions the use of ""pre-trained transformer models"" such as BERT, LITMC-BERT, and PubMedBERT, which are deep learning models used for natural language processing (NLP). This indicates the main AI/ML framework used in the technical architecture.
- The models are used for categorizing documents based on their content related to COVID-19, which suggests a system component focused on document classification.
- The abstract does not mention specific system architecture components like databases, APIs, or interfaces, nor does it discuss technical infrastructure such as cloud platforms or computational resources.
- There is no mention of integration with existing academic databases or platforms, nor is there a detailed description of the overall system design approach beyond the use of pre-trained transformer models.
- The focus is on the application of these models for screening and categorization purposes, but the abstract lacks detailed technical architecture information.","  - ""Our approach employs pre-trained transformer models, such as Bidirectional Encoder Representations from Transformers (BERT), LITMC-BERT and PubMedBERT, for papers about artificial intelligence research on COVID-19 for the purposes of detection, diagnosis, and prediction.""
  - ""Our models find relevant documents and categorize them according to their information on COVID-19, such as diseases, treatments, and vaccinations.""
  - ""The proposed models have been evaluated for screening performance using a test set that comprises articles retrieved from the searches of three independent reviewers, who have assessed the eligibility of each paper.""",,"- The abstract mentions the use of ""pre-trained transformer models"" such as BERT, LITMC-BERT, and PubMedBERT. This indicates that these models are the primary algorithms used for the implementation.
- The models are used for ""detection, diagnosis, and prediction"" related to COVID-19 research, suggesting a classification or categorization task.
- The abstract states that the models ""find relevant documents and categorize them according to their information on COVID-19,"" which implies a classification or clustering task based on the content of the documents.
- The evaluation of the models involved using a test set of articles assessed by independent reviewers, which suggests a data processing pipeline involving human evaluation for validation.
- The abstract does not provide specific details on feature extraction techniques, data processing pipelines, training methodologies, preprocessing steps, or technical workflow beyond the use of pre-trained transformer models.","  - ""Typically, these reviews are carried out using laborious manual methods that rely on, e.g. keyword searches, title, and text analysis.""
  - ""The proposed models have been evaluated for screening performance using a test set that comprises articles retrieved from the searches of three independent reviewers, who have assessed the eligibility of each paper.""
  - ""Our models find relevant documents and categorize them according to their information on COVID-19, such as diseases, treatments, and vaccinations.""
  - ""Our approach employs pre-trained transformer models, such as Bidirectional Encoder Representations from Transformers (BERT), LITMC-BERT and PubMedBERT, for papers about artificial intelligence research on COVID-19 for the purposes of detection, diagnosis, and prediction.""",,"- The abstract mentions the use of ""keyword searches, title, and text analysis"" as traditional methods for paper discovery, which aligns with content-based matching techniques.
- The employment of ""pre-trained transformer models"" such as BERT, LITMC-BERT, and PubMedBERT indicates the use of content-based matching techniques, as these models are used for text analysis and relevance ranking.
- The models are used to ""find relevant documents and categorize them"" based on specific topics related to COVID-19, which suggests a content-based matching technique.
- The evaluation process involves using a test set from independent reviewers, which implies a relevance ranking approach to assess the eligibility of papers.","  - ""Typically, these reviews are carried out using laborious manual methods that rely on, e.g. keyword searches, title, and text analysis.""
  - ""Our models find relevant documents and categorize them according to their information on COVID-19, such as diseases, treatments, and vaccinations.""
  - ""This subject has been growing rapidly over the past three years, thereby making it challenging to be up-to-date with the latest information pertaining to the virus.""
  - ""To alleviate the burden on scientists, several natural language processing-based approaches have been developed to facilitate the screening of papers.Our""
  - ""The proposed models have been evaluated for screening performance using a test set that comprises articles retrieved from the searches of three independent reviewers, who have assessed the eligibility of each paper.""",,"- The abstract mentions that traditional methods of systematic literature reviews are ""laborious,"" which implies a technical challenge related to efficiency and scalability.
- The rapid growth of research on COVID-19 makes it challenging to keep up with the latest information, suggesting a data quality or availability problem due to the sheer volume of new research.
- The use of pre-trained transformer models indicates an attempt to address these challenges, but the abstract does not explicitly mention any technical challenges or limitations with these models.
- The evaluation methodology involves a test set from independent reviewers, which could imply a challenge in ensuring consistent evaluation criteria, but this is not explicitly stated as a technical challenge.","  - ""Our approach employs pre-trained transformer models, such as Bidirectional Encoder Representations from Transformers (BERT), LITMC-BERT and PubMedBERT, for papers about artificial intelligence research on COVID-19 for the purposes of detection, diagnosis, and prediction.""
  - ""The proposed models have been evaluated for screening performance using a test set that comprises articles retrieved from the searches of three independent reviewers, who have assessed the eligibility of each paper.""
  - ""Our models find relevant documents and categorize them according to their information on COVID-19, such as diseases, treatments, and vaccinations.""
  - ""Our approach has achieved promising results for researchers and demonstrated its ability to be generalized.""",,"- The abstract mentions the use of pre-trained transformer models like BERT, LITMC-BERT, and PubMedBERT. This is a novel technical approach as it leverages advanced natural language processing (NLP) techniques to assist in title and abstract screening.
- The application of these models to categorize documents based on specific COVID-19 related information (diseases, treatments, vaccinations) is a creative problem-solving method. It addresses the challenge of manually screening and categorizing large volumes of literature.
- The evaluation method using a test set from independent reviewers is a technical workaround for ensuring the accuracy and reliability of the model's performance.
- The ability of the approach to generalize suggests an original system design element, as it implies that the model can be applied to various contexts beyond the specific COVID-19 research.","  - ""The proposed models have been evaluated for screening performance using a test set that comprises articles retrieved from the searches of three independent reviewers, who have assessed the eligibility of each paper.""
  - ""Our approach employs pre-trained transformer models, such as Bidirectional Encoder Representations from Transformers (BERT), LITMC-BERT and PubMedBERT, for papers about artificial intelligence research on COVID-19 for the purposes of detection, diagnosis, and prediction.""
  - ""Our approach has achieved promising results for researchers and demonstrated its ability to be generalized.""",,"- The abstract mentions the use of pre-trained transformer models like BERT, LITMC-BERT, and PubMedBERT, which are known for their high performance in natural language processing tasks. This suggests that the models are likely to have good accuracy and precision in screening papers.
- The evaluation of the models was conducted using a test set of articles assessed by independent reviewers, which implies a structured evaluation process. However, the abstract does not provide specific quantitative metrics such as accuracy, precision, recall, or F1-scores.
- The abstract states that the approach has achieved ""promising results"" and demonstrated ""its ability to be generalized,"" which suggests qualitative success in terms of usability and reliability. However, no specific metrics or comparisons with baseline methods are provided.
- There is no mention of processing speed and efficiency metrics, user satisfaction or usability results, system reliability and robustness measures, or scalability test results in the abstract.","  - ""Our models find relevant documents and categorize them according to their information on COVID-19, such as diseases, treatments, and vaccinations.""
  - ""The proposed models have been evaluated for screening performance using a test set that comprises articles retrieved from the searches of three independent reviewers, who have assessed the eligibility of each paper.""
  - ""Our approach employs pre-trained transformer models, such as Bidirectional Encoder Representations from Transformers (BERT), LITMC-BERT and PubMedBERT, for papers about artificial intelligence research on COVID-19 for the purposes of detection, diagnosis, and prediction.""",,"- The abstract mentions the use of pre-trained transformer models like BERT, LITMC-BERT, and PubMedBERT, which are typically trained on large datasets. However, it does not specify the exact datasets or academic databases used for training these models.
- The abstract does not provide information on the dataset sizes or characteristics, nor does it mention any specific data collection methods or training/validation/test data splits.
- There is no mention of data preprocessing or cleaning approaches in the abstract.
- The abstract does not specify any external knowledge bases or ontologies used in the study.
- The evaluation of the models was done using a test set from articles retrieved by three independent reviewers, but again, no specific details about the data sources or databases are provided.","  - ""Systematic literature reviews are crucial for decision-making across various fields.""
  - ""Typically, these reviews are carried out using laborious manual methods that rely on, e.g. keyword searches, title, and text analysis.""
  - ""Our approach employs pre-trained transformer models, such as Bidirectional Encoder Representations from Transformers (BERT), LITMC-BERT and PubMedBERT, for papers about artificial intelligence research on COVID-19 for the purposes of detection, diagnosis, and prediction.""
  - ""This subject has been growing rapidly over the past three years, thereby making it challenging to be up-to-date with the latest information pertaining to the virus.""
  - ""Our models find relevant documents and categorize them according to their information on COVID-19, such as diseases, treatments, and vaccinations.""
  - ""The proposed models have been evaluated for screening performance using a test set that comprises articles retrieved from the searches of three independent reviewers, who have assessed the eligibility of each paper.""
  - ""Our approach has achieved promising results for researchers and demonstrated its ability to be generalized.""",,"- The target research discipline or field is artificial intelligence research on COVID-19, specifically focusing on detection, diagnosis, and prediction.
- The specific literature review task addressed is the screening of papers, which involves finding relevant documents and categorizing them based on their content related to COVID-19.
- The types of academic documents processed are papers related to artificial intelligence research on COVID-19.
- The user types and requirements are implied to be researchers who need assistance in screening and categorizing large volumes of literature.
- The integration with research workflows is suggested by the use of pre-trained transformer models to facilitate the screening process, which is typically a laborious manual task.
- The application is academic, as it is designed to assist researchers in systematic literature reviews."
"A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges","Andrew Brown, Muhammad Roman, Barry Devereux",10.48550/arXiv.2508.06401,https://doi.org/10.48550/arXiv.2508.06401,arXiv.org,1,2025,"- Main AI/ML frameworks used: Deep learning, NLP
- System architecture components: Neural retriever, generative language model, integration with databases (ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, DBLP)
- Technical infrastructure: Google Sheets, EndNote
- Integration with existing academic databases or platforms: ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, DBLP
- Overall system design approach: PRISMA framework for systematic review","- Algorithms and models used: Neural retriever and generative language model.
- Feature extraction techniques: Not explicitly mentioned.
- Data processing pipelines: Use of specific databases (ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, DBLP) and a bibliographic index.
- Training methodologies: Not explicitly mentioned.
- Preprocessing steps: Initial screening and full-text screening with quality assurance protocols.
- Technical workflow or methodology: PRISMA 2020 framework for systematic review.","- Search and retrieval algorithms: Utilized four digital databases and the DBLP bibliographic index.
- Relevance ranking approaches: Followed PRISMA guidelines for structured review.
- Citation analysis methods: Used citation statistics to filter papers.
- Content-based matching techniques: Based search terms on the core concept of the RAG framework.
- Metadata extraction and utilization: Used a Python script to convert and organize data; utilized Google Sheets and EndNote.
- Recommendation system approaches: Used LLM-generated suggestions and manual checks for accuracy.
- Expert/authority identification methods: Involved human reviewers in final selection decisions.","- Algorithm performance issues: Risk of hallucination, potential absence of key data in retrieved passages.
- Data quality or availability problems: Potential absence of key data in retrieved passages.
- Scalability challenges: Fragmented results.
- Integration difficulties: Need for transparent, protocol-driven synthesis.
- Evaluation methodology limitations: Evolving evaluation protocols.
- Computational resource constraints: Need for careful cross-checking of extracted data.
- Technical bottlenecks or failure points: Risk of hallucination, potential absence of key data.","- Hybrid retrievers
- Iterative retrieval loops
- Graph-based retrieval
- Domain-specific pipelines
- Citation-weighted, PRISMA-compliant systematic synthesis
- Use of multiple databases and a bibliographic index for comprehensive coverage
- Lower citation-count threshold for capturing emerging breakthroughs",Not mentioned (the paper does not provide specific performance results or metrics),"- Academic databases used: ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, DBLP
- Dataset sizes and characteristics: Various datasets listed in tables with content descriptions and intended uses
- Data collection methods: Exported in BibTeX, CSV, or Excel formats; Python script for conversion
- Training/validation/test data splits: Not explicitly mentioned
- Data preprocessing and cleaning approaches: Duplicate removal and manual verification
- External knowledge bases or ontologies used: ConceptNet, Wikipedia","- Target research disciplines or fields: Natural Language Processing (NLP)
- Specific literature review tasks addressed: Identifying gaps and promising directions in RAG research
- Types of academic documents processed: Research articles and papers related to RAG
- User types and requirements: NLP researchers and engineers
- Integration with research workflows: Providing guidance for future research and practical applications
- Commercial vs. academic applications: Primarily academic context","  - ""RAG couples a neural retriever with a generative language model, grounding output in up-to-date, non-parametric memory while retaining the semantic generalisation stored in model weights.""
  - ""The present study addresses these gaps by offering a citation-weighted, PRISMA-compliant systematic synthesis of 128 influential RAG studies that maps datasets, architectures, evaluation metrics, and open research challenges, thus advancing the field toward more aligned, robust, and scalable retrieval-augmented systems.""
  - ""We used four digital databases and the DBLP bibliographic index to improve coverage and deduplication.""
  - ""Data extraction and management were handled using Google Sheets for organising data and EndNote for managing references.""
  - ""The PRISMA 2020 guidelines provide an extensive framework for systematic reviews, especially suitable for multidisciplinary fields such as RAG.""
  - ""We used the standard PRISMA flow diagram, as shown in Figure 1 , which encompasses searches exclusively in specific databases and registers, although these are not detailed here.""
  - ""The review process was structured into three main phases: Identification, Screening, and Inclusion.""
  - ""The data extracted from the articles were compiled into a structured database designed for easy access during subsequent analysis, synthesis, and reporting.""
  - ""Each entry was verified against the original articles to identify and correct any discrepancies, such as mismatched values or missing information.""
  - ""The synthesis used methods suited to the nature of the data and the review objectives, primarily through a descriptive approach that summarised and explained the data patterns by identifying trends, differences and similarities between studies.""","  - ""(Page 6, Table 1) | Datasets                                                                                           | Chunking Mechanism                                                                                      | Retrieval Mechanism                                                                                                                                                                                                 | Vector Space Encoder                                                                                       | Generation Model                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Domain: Knowledge-Intensive Tasks**                                                              |                                                                                                         |                                                                                                                                                                                                                     |                                                                                                            |                                                                                                                                                                                                                  |""
  - ""(Page 39, Table 1) | Dataset Name                                                                 | Content Description                                                                 | Intended Use                                                                                   | Citation Frequency |\n|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|--------------------|\n| Colossal Clean Crawled Corpus (C4) [220]                                     | Billions of English tokens from web.                                                | Unsupervised pre-training for NLP models.                                                      | 2                  |""","- The paper discusses the technical architecture of Retrieval-Augmented Generation (RAG) by highlighting its core components: a neural retriever and a generative language model. This indicates a deep learning framework with NLP capabilities.
- The use of databases such as ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, and DBLP suggests integration with existing academic databases.
- The mention of Google Sheets and EndNote for data management indicates the use of cloud-based tools for data organization and reference management.
- The PRISMA framework is used for systematic review, which is a structured approach to literature reviews, but it does not specify technical infrastructure like cloud platforms or computational resources.
- The paper does not explicitly mention specific technical infrastructure such as cloud platforms or computational resources, but it does emphasize the use of digital databases and bibliographic indices.
- The tables referenced provide detailed information on datasets, chunking mechanisms, retrieval mechanisms, vector space encoders, and generation models, which are crucial components of the RAG technical architecture.","  - ""RAG couples a neural retriever with a generative language model, grounding output in up-to-date, non-parametric memory while retaining the semantic generalisation stored in model weights.""
  - ""The present study addresses these gaps by offering a citation-weighted, PRISMA-compliant systematic synthesis of 128 influential RAG studies that maps datasets, architectures, evaluation metrics, and open research challenges, thus advancing the field toward more aligned, robust, and scalable retrieval-augmented systems.""
  - ""We used four digital databases and the DBLP bibliographic index to improve coverage and deduplication.""
  - ""Articles were screened against a set inclusion and exclusion criteria linked to our research questions.""
  - ""Following PRISMA guidelines, a reviewer handled initial screening, full text review, and data extraction, while a second reviewer independently checked the results to reduce bias.""
  - ""Data extraction and management were handled using Google Sheets for organising data and EndNote for managing references.""
  - ""The data extracted from the articles were compiled into a structured database designed for easy access during subsequent analysis, synthesis, and reporting.""
  - ""The synthesis used methods suited to the nature of the data and the review objectives, primarily through a descriptive approach that summarised and explained the data patterns by identifying trends, differences and similarities between studies.""
  - ""The PRISMA 2020 guidelines provide an extensive framework for systematic reviews, especially suitable for multidisciplinary fields such as RAG.""
  - ""We used the standard PRISMA flow diagram, as shown in Figure 1 , which encompasses searches exclusively in specific databases and registers, although these are not detailed here.""
  - ""The review process was structured into three main phases: Identification, Screening, and Inclusion.""
  - ""We based our search terms on the core concept of the RAG framework by breaking down ""retrieval augmented generation"" into three parts: ""retrieval"", ""augmented"", and ""generation"".""
  - ""Our systematic approach, combining the main keywords with related phrases such as ""retrieval augmented text generation"", gathered a wide range of relevant literature on RAG.""
  - ""A Python script converted BibTeX files into Excel format, gathering key details such as titles, abstracts, publication years, authors, author counts, and journal names into one data table.""
  - ""Duplicate entries were first automatically removed by the script, followed by a manual check to verify accuracy.""
  - ""The process, illustrated in Figure 1 , consisted of an initial screening and a review of the full text.""
  - ""1) Initial Screening: After removing duplicates and applying date and citation filters, two of the present authors (R 1 , R 2 ) independently screened all titles and abstracts (n = 202).""
  - ""2) Full Text Screening: Full texts were retrieved from the original sources indexed by our selected databases and the DBLP bibliographic index.""
  - ""During full-text screening, we applied a quality assurance protocol assessing soundness, validity, reliability, and statistical rigour to ensure the inclusion of only high-quality studies.""
  - ""The data extraction process followed our research question and eligibility criteria, focusing on topics, methods, and evaluation metrics.""
  - ""A single reviewer, using a RAG framework, independently extracted the data to confirm accuracy and reliability.""
  - ""Using this framework confirmed that the data collection was complete and consistent with the research criteria and objectives.""
  - ""However, the RAG framework poses two major challenges. The first is the risk of hallucination, where the system may generate information that does not exist. The second is that key data might be absent from the retrieved passages.""
  - ""Addressing these challenges is essential to preserve the integrity of the data extraction process.""","  - ""(Page 6, Table 1) | Datasets                                                                                           | Chunking Mechanism                                                                                      | Retrieval Mechanism                                                                                                                                                                                                 | Vector Space Encoder                                                                                       | Generation Model                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Domain: Knowledge-Intensive Tasks**                                                              |                                                                                                         |                                                                                                                                                                                                                     |                                                                                                            |                                                                                                                                                                                                                  |""
  - ""<table_quotation page_num=7 table_on_page=1></table_quotation>""
  - ""(Page 8, Table 1) | Datasets                                                                                           | Chunking Mechanism                                                                                     | Retrieval Mechanism                                                                                     | Vector Space Encoder                                                                                   | Generation Model                                                                                       |\n|----------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\n| **Domain: Knowledge-Intensive Tasks Continued...**                                                 |                                                                                                        |                                                                                                         |                                                                                                        |                                                                                                        |""","- The paper describes the use of a neural retriever and a generative language model in RAG, indicating a key technical implementation.
- The study follows the PRISMA 2020 framework, which provides a structured methodology for systematic reviews.
- The authors used specific databases and a bibliographic index for data collection, which is part of the data processing pipeline.
- The paper mentions the use of Google Sheets and EndNote for data management and organization.
- The synthesis of data was done using a descriptive approach to identify trends and patterns.
- The PRISMA framework is highlighted as a robust approach for literature reviews.
- The review process involved initial screening and full-text screening with quality assurance protocols.
- Data extraction was done using a structured database and a single reviewer to ensure accuracy and reliability.
- The paper discusses challenges such as hallucination and missing data, which are technical challenges in RAG implementation.","  - ""We used four digital databases and the DBLP bibliographic index to improve coverage and deduplication.""
  - ""We targeted five key electronic resources, chosen for their extensive repositories and relevance to our research topics:

1) ACM Digital Library: https://dl.acm.org 2) IEEE Xplore: https://ieeexplore.ieee.org/ 3) Scopus: https://www.scopus.com/ 4) ScienceDirect: https://www.sciencedirect.com/ 5) Digital Bibliography and Library Project (DBLP; bibliographic index): https://dblp.org/""
  - ""We based our search terms on the core concept of the RAG framework by breaking down ""retrieval augmented generation"" into three parts: ""retrieval"", ""augmented"", and ""generation"". These parts became the basis for our search terms used in titles, abstracts, and keywords.""
  - ""Articles were screened against a set inclusion and exclusion criteria linked to our research questions.""
  - ""Following PRISMA guidelines, a reviewer handled initial screening, full text review, and data extraction, while a second reviewer independently checked the results to reduce bias.""
  - ""The results were exported in BibTeX, CSV, or Excel formats as provided by the source. A Python script converted BibTeX files into Excel format, gathering key details such as titles, abstracts, publication years, authors, author counts, and journal names into one data table.""
  - ""Duplicate entries were first automatically removed by the script, followed by a manual check to verify accuracy.""
  - ""The process, illustrated in Figure 1 , consisted of an initial screening and a review of the full text.""
  - ""The LLM was prompted with our research questions and inclusion/exclusion criteria; its five binary recommendations were then collapsed into a single suggestion by majority vote.""
  - ""The final selection decisions remained exclusively with the human reviewers.""
  - ""Data extraction and management were handled using Google Sheets for organising data and EndNote for managing references.""
  - ""The data extracted from the articles were compiled into a structured database designed for easy access during subsequent analysis, synthesis, and reporting.""
  - ""Each entry was verified against the original articles to identify and correct any discrepancies, such as mismatched values or missing information.""",,"- The paper describes the use of multiple digital databases and a bibliographic index to search for relevant articles, which is a search and retrieval algorithm.
- The search terms were based on the core concept of the RAG framework, indicating a content-based matching technique.
- The use of PRISMA guidelines and a structured review process suggests a relevance ranking approach.
- The paper mentions the use of citation statistics to filter papers, which is a citation analysis method.
- The process of screening articles against inclusion and exclusion criteria, as well as the use of LLM-generated suggestions, indicates a metadata extraction and utilization technique.
- The use of a Python script to convert and organize data suggests a metadata extraction method.
- The manual check for accuracy and the use of Google Sheets and EndNote for data management indicate a recommendation system approach.
- The involvement of human reviewers in the final selection decisions suggests an expert/authority identification method.","  - ""The first is the risk of hallucination, where the system may generate information that does not exist.""
  - ""The second is that key data might be absent from the retrieved passages.""
  - ""Despite the framework's benefits in improving speed and precision, these issues call for careful cross-checking of the extracted data to maintain its authenticity and reliability.""
  - ""the results are fragmented and the evaluation protocols are still evolving.""
  - ""Therefore, a transparent, protocol-driven synthesis of RAG is required.""
  - ""the RAG framework poses two major challenges.""",,"- The paper identifies two major technical challenges with the RAG framework: the risk of hallucination and the potential absence of key data in retrieved passages. These are algorithm performance issues as they relate to the accuracy and reliability of the generated content.
- The paper mentions that the results are fragmented and evaluation protocols are evolving, indicating scalability challenges and evaluation methodology limitations. This suggests that there are difficulties in integrating and evaluating RAG systems consistently across different contexts.
- The need for a transparent, protocol-driven synthesis indicates integration difficulties and the necessity for standardized approaches to ensure reproducibility and reliability.
- The emphasis on careful cross-checking of extracted data highlights computational resource constraints and technical bottlenecks, as it requires additional resources to ensure data authenticity and reliability.","  - ""Since Meta AI introduced RAG in 2020 [1], the field has diversified rapidly, incorporating hybrid retrievers, iterative retrieval loops, graph-based retrieval, and domainspecific pipelines have been proposed.""
  - ""The present study addresses these gaps by offering a citation-weighted, PRISMA-compliant systematic synthesis of 128 influential RAG studies that maps datasets, architectures, evaluation metrics, and open research challenges, thus advancing the field toward more aligned, robust, and scalable retrieval-augmented systems.""
  - ""This review is aimed at both NLP researchers, who can use it to identify gaps and promising directions, and NLP engineers seeking practical guidance on applying RAG techniques.""
  - ""the RAG framework poses two major challenges. The first is the risk of hallucination, where the system may generate information that does not exist. The second is that key data might be absent from the retrieved passages.""
  - ""Addressing these challenges is essential to preserve the integrity of the data extraction process.""
  - ""the results are fragmented and the evaluation protocols are still evolving. Therefore, a transparent, protocol-driven synthesis of RAG is required.""
  - ""We used four digital databases and the DBLP bibliographic index to improve coverage and deduplication.""
  - ""The PRISMA 2020 guidelines provide an extensive framework for systematic reviews, especially suitable for multidisciplinary fields such as RAG.""
  - ""The synthesis used methods suited to the nature of the data and the review objectives, primarily through a descriptive approach that summarised and explained the data patterns by identifying trends, differences and similarities between studies.""
  - ""This review clarifies the current research landscape, highlights methodological gaps, and charts priority directions for future research.""
  - ""To mitigate citation-lag bias, we applied a lower citation-count threshold to papers published in 2025 so that emerging breakthroughs with naturally fewer citations were still captured.""","  - ""(Page 6, Table 1) | Datasets                                                                                           | Chunking Mechanism                                                                                      | Retrieval Mechanism                                                                                                                                                                                                 | Vector Space Encoder                                                                                       | Generation Model                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Domain: Knowledge-Intensive Tasks**                                                              |                                                                                                         |                                                                                                                                                                                                                     |                                                                                                            |                                                                                                                                                                                                                  |""
  - ""<table_quotation page_num=7 table_on_page=1></table_quotation>""
  - ""(Page 8, Table 1) | Datasets                                                                                           | Chunking Mechanism                                                                                     | Retrieval Mechanism                                                                                     | Vector Space Encoder                                                                                   | Generation Model                                                                                       |\n|----------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\n| **Domain: Knowledge-Intensive Tasks Continued...**                                                 |                                                                                                        |                                                                                                         |                                                                                                        |                                                                                                        |""","- The paper discusses the rapid diversification of RAG since its introduction in 2020, including the development of hybrid retrievers, iterative retrieval loops, and graph-based retrieval. These are innovative solutions that address the limitations of traditional retrieval systems.
- The paper highlights the challenges of hallucination and missing data in RAG systems, which are technical challenges that require innovative solutions.
- The use of a citation-weighted, PRISMA-compliant systematic synthesis is an innovative approach to reviewing RAG studies, ensuring transparency and reproducibility.
- The paper mentions the use of multiple databases and a bibliographic index to improve coverage and deduplication, which is a technical workaround for ensuring comprehensive coverage of relevant studies.
- The application of a lower citation-count threshold for 2025 publications is a methodological innovation to capture emerging breakthroughs.
- The tables included in the paper likely provide detailed information on datasets, architectures, evaluation metrics, and challenges, which are essential for identifying innovative solutions and technical workarounds.","  - ""The present study addresses these gaps by offering a citation-weighted, PRISMA-compliant systematic synthesis of 128 influential RAG studies that maps datasets, architectures, evaluation metrics, and open research challenges, thus advancing the field toward more aligned, robust, and scalable retrieval-augmented systems.""
  - ""This review clarifies the current research landscape, highlights methodological gaps, and charts priority directions for future research.""
  - ""To mitigate citation-lag bias, we applied a lower citation-count threshold to papers published in 2025 so that emerging breakthroughs with naturally fewer citations were still captured.""
  - ""The PRISMA 2020 guidelines provide an extensive framework for systematic reviews, especially suitable for multidisciplinary fields such as RAG.""
  - ""The synthesis used methods suited to the nature of the data and the review objectives, primarily through a descriptive approach that summarised and explained the data patterns by identifying trends, differences and similarities between studies.""
  - ""This review is aimed at both NLP researchers, who can use it to identify gaps and promising directions, and NLP engineers seeking practical guidance on applying RAG techniques.""
  - ""Our review catalogues datasets, novel methods, evaluation metrics, and RAG deployment challenges.""
  - ""The remainder of this paper is organised as follows. Section II details the methodology employed in this review, including search strategies and inclusion criteria.""
  - ""Section III presents the results, categorising the studies according to key themes and findings.""
  - ""Section IV discusses the implications of these findings, addressing both the strengths and challenges of RAG.""
  - ""Finally, Section VI concludes the paper, summarising the key insights with concrete recommendations for researchers and engineers building the next wave of knowledge-aware language models.""","  - ""(Page 2, Table 1) | Index | Research Question                                                                 | Goal                                                                                                                                                                                                 |\n|-------|-----------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| RQ1   | What thematic topics have already been addressed by highly cited RAG studies?     | Summarises the main topics in the field, outlining the current state of knowledge and identifying gaps in the literature.                                                                            |""
  - ""(Page 3, Table 1) | Database              | Query                                                                                                                                                      |\n|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ACM Digital Library   | Title:(retrieval AND augmented AND generation) OR Abstract:(retrieval AND augmented AND generation)                                                        |""
  - ""(Page 6, Table 1) | Datasets                                                                                           | Chunking Mechanism                                                                                      | Retrieval Mechanism                                                                                                                                                                                                 | Vector Space Encoder                                                                                       | Generation Model                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Domain: Knowledge-Intensive Tasks**                                                              |                                                                                                         |                                                                                                                                                                                                                     |                                                                                                            |                                                                                                                                                                                                                  |""
  - ""(Page 7, Table 1) | Datasets                                                                 | Chunking Mechanism                                                                 | Retrieval Mechanism                                                                                           | Vector Space Encoder                                                                 | Generation Model                                                                                      |\n|--------------------------------------------------------------------------|------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\n| **Domain: Knowledge-Intensive Tasks Continued...**                       |                                                                                    |                                                                                                               |                                                                                      |                                                                                                        |""
  - ""<table_quotation page_num=38 table_on_page=1></table_quotation>""
  - ""(Page 39, Table 1) | Dataset Name                                                                 | Content Description                                                                 | Intended Use                                                                                   | Citation Frequency |\n|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|--------------------|\n| Colossal Clean Crawled Corpus (C4) [220]                                     | Billions of English tokens from web.                                                | Unsupervised pre-training for NLP models.                                                      | 2                  |""
  - ""<table_quotation page_num=40 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=41 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=42 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=43 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=44 table_on_page=1></table_quotation>""
  - ""(Page 45, Table 1) | Dataset Name                                      | Content Description                                                                 | Intended Use                                           | Citation Frequency |\n|---------------------------------------------------|--------------------------------------------------------------------------------------|--------------------------------------------------------|--------------------|\n| UpToDate                                          | Clinical decision support content by Wolters Kluwer.                                 | Point-of-care medical reference.                       | 1                  |""","- The paper is a systematic review of retrieval-augmented generation (RAG) techniques, focusing on methodologies, datasets, and challenges rather than specific performance results.
- The paper does not provide quantitative performance results such as accuracy, precision, recall, F1-scores, or processing speed and efficiency metrics.
- There is no mention of comparison with baseline methods or user satisfaction/usability results.
- The paper does not discuss system reliability and robustness measures or scalability test results.
- The tables referenced in the paper are likely to contain information on datasets, architectures, and evaluation metrics, but the content of these tables is not provided in the quotes.
- The paper's focus is on synthesizing existing research and identifying gaps and future directions rather than presenting specific performance outcomes.","  - ""The records were retrieved from ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).""
  - ""We used four digital databases and the DBLP bibliographic index to improve coverage and deduplication.""
  - ""We targeted five key electronic resources, chosen for their extensive repositories and relevance to our research topics:

1) ACM Digital Library: https://dl.acm.org 2) IEEE Xplore: https://ieeexplore.ieee.org/ 3) Scopus: https://www.scopus.com/ 4) ScienceDirect: https://www.sciencedirect.com/ 5) Digital Bibliography and Library Project (DBLP; bibliographic index): https://dblp.org/""
  - ""The results were exported in BibTeX, CSV, or Excel formats as provided by the source.""
  - ""A Python script converted BibTeX files into Excel format, gathering key details such as titles, abstracts, publication years, authors, author counts, and journal names into one data table.""
  - ""Duplicate entries were first automatically removed by the script, followed by a manual check to verify accuracy.""
  - ""Articles were screened against a set inclusion and exclusion criteria linked to our research questions.""
  - ""Missing abstracts were retrieved from the original databases and manually added.""
  - ""Following PRISMA guidelines, a reviewer handled initial screening, full text review, and data extraction, while a second reviewer independently checked the results to reduce bias.""
  - ""The process, illustrated in Figure 1 , consisted of an initial screening and a review of the full text.""
  - ""We identified 4721 records; after removing duplicates (1494), out-of-range (158) and below-threshold items (2867), 202 were screened; 144 full texts were assessed; 128 studies were included (reasons in Fig. 1""
  - ""Following the screening of the title and abstract, 144 candidate records were recovered in full and assessed against the predefined inclusion criteria.""
  - ""Sixteen of these were excluded during the full text screening for the reasons summarised below.""
  - ""The reasons for exclusion were categorised as follows:

• Irrelevance of Primary Focus (n = 7): Papers whose primary contributions lay outside the augmented generation of retrieval, e.g. robustness of dense search, long-context benchmarks, general GenIR evaluation or system-level optimisations, where RAG appeared only as a peripheral baseline or illustrative example [4]- [10].""
  - ""Insufficient Emphasis or Ancillary Treatment (n = 7):

Studies that incorporated RAG merely as an auxiliary component within broader investigations-such as LLMhuman hybrids for marketing research, domain-specific LLM development, knowledge graph construction workflows, multimodal agent toolkits, healthcare task automation, cost-effective classification or materials modelling pipelines-without substantive and dedicated analysis of RAG itself [11]- [17].""
  - ""Methodological Distinction (n = 2): Works focused on conceptually distinct paradigms from RAG, specifically generative retrieval or generation-augmented retrieval, which invert the standard RAG pipeline by predicting document identifiers rather than conditioning the generation on the retrieved content [18], [19].""
  - ""All exclusion decisions were systematically documented to ensure methodological rigour, transparency, and reproducibility.""
  - ""Across 2020-2025, the number of identified articles increased year on year from 2020 to 2023, with a pronounced increase in 2024.""
  - ""These counts reflect the records that remained after deduplication and application of the eligibility criteria (Section II-C), including the citation thresholds (≥ 30 for publications up to 2024; ≥ 15 for 2025).""
  - ""Consequently, year-to-year comparisons should be interpreted in light of (i) the staged indexing of databases and (ii) the partial coverage of 2025 at the time of the last search.""
  - ""Studies were coded to a single primary domain for proportional reporting; secondary tags (e.g., multimodal, conversational) were retained for analysis but are not double-counted in the primary distribution.""
  - ""Coding rules and examples appear in Table III""
  - ""Proportions below refer to the included studies (Fig. 3""
  - ""Knowledge-intensive tasks accounted for 27.34%, followed by open-domain question answering (ODQA) at 15.62%, software engineering 10.16% and medical 8.59%.""
  - ""Evaluation comprised 7.03%.""
  - ""The ""Other"" category (7.03%) covers nine single-study niches: networking, counterfactual augmentation, content creation, personalisation, legal QA, recommender systems, chemistry, disaster response and personalised search.""
  - ""Multimodal and conversational AI each represented 4.69%; security/vulnerabilities and biomedical 3.91% each; education 3.12%; information extraction 2.34%; and finance 1.56%.""
  - ""These distributions indicate a concentration of work in knowledge-intensive and ODQA settings, with substantial activity in software engineering and medical applications and a long tail of niche areas.""
  - ""Full per-study domain labels and secondary tags are provided in • 100-token passages [25] • 100-word chunks [1] • 6-10 sentences [26] • A decompose-thenrecompose algorithm splits each retrieved document into smaller strips, filters out irrelevant portions, and reassembles the relevant parts. [27] • Align passage segmentation with paragraph boundaries.""","  - ""(Page 2, Table 1) | Index | Research Question                                                                 | Goal                                                                                                                                                                                                 |\n|-------|-----------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| RQ1   | What thematic topics have already been addressed by highly cited RAG studies?     | Summarises the main topics in the field, outlining the current state of knowledge and identifying gaps in the literature.                                                                            |""
  - ""(Page 3, Table 1) | Database              | Query                                                                                                                                                      |\n|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ACM Digital Library   | Title:(retrieval AND augmented AND generation) OR Abstract:(retrieval AND augmented AND generation)                                                        |""
  - ""(Page 6, Table 1) | Datasets                                                                                           | Chunking Mechanism                                                                                      | Retrieval Mechanism                                                                                                                                                                                                 | Vector Space Encoder                                                                                       | Generation Model                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Domain: Knowledge-Intensive Tasks**                                                              |                                                                                                         |                                                                                                                                                                                                                     |                                                                                                            |                                                                                                                                                                                                                  |""
  - ""<table_quotation page_num=7 table_on_page=1></table_quotation>""
  - ""(Page 8, Table 1) | Datasets                                                                                           | Chunking Mechanism                                                                                     | Retrieval Mechanism                                                                                     | Vector Space Encoder                                                                                   | Generation Model                                                                                       |\n|----------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\n| **Domain: Knowledge-Intensive Tasks Continued...**                                                 |                                                                                                        |                                                                                                         |                                                                                                        |                                                                                                        |""
  - ""<table_quotation page_num=38 table_on_page=1></table_quotation>""
  - ""(Page 39, Table 1) | Dataset Name                                                                 | Content Description                                                                 | Intended Use                                                                                   | Citation Frequency |\n|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|--------------------|\n| Colossal Clean Crawled Corpus (C4) [220]                                     | Billions of English tokens from web.                                                | Unsupervised pre-training for NLP models.                                                      | 2                  |""
  - ""<table_quotation page_num=40 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=41 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=42 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=43 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=44 table_on_page=1></table_quotation>""
  - ""(Page 45, Table 1) | Dataset Name                                      | Content Description                                                                 | Intended Use                                           | Citation Frequency |\n|---------------------------------------------------|--------------------------------------------------------------------------------------|--------------------------------------------------------|--------------------|\n| UpToDate                                          | Clinical decision support content by Wolters Kluwer.                                 | Point-of-care medical reference.                       | 1                  |""","- The paper uses several academic databases for data collection: ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, and DBLP.
- The data collection process involved exporting results in BibTeX, CSV, or Excel formats and using a Python script to convert BibTeX files into Excel format for easier data management.
- Duplicate entries were automatically removed and manually verified for accuracy.
- The paper mentions the use of a systematic review framework following PRISMA guidelines, which includes initial screening and full-text review to ensure the quality of included studies.
- The paper provides detailed information on the datasets used in the studies reviewed, including their sizes and characteristics, as well as the retrieval mechanisms and generation models employed.
- The tables at the end of the paper list various datasets and their characteristics, such as content description, intended use, and citation frequency.
- The paper discusses the use of external knowledge bases and ontologies, such as ConceptNet and Wikipedia, for knowledge-intensive tasks.","  - ""This systematic review of the research literature on retrieval-augmented generation (RAG) provides a focused analysis of the most highly cited studies published between 2020 and May 2025.""
  - ""RAG couples a neural retriever with a generative language model, grounding output in up-to-date, non-parametric memory while retaining the semantic generalisation stored in model weights.""
  - ""Guided by the PRISMA 2020 framework, we (i) specify explicit inclusion and exclusion criteria based on citation count and research questions, (ii) catalogue datasets, architectures, and evaluation practices, and (iii) synthesise empirical evidence on the effectiveness and limitations of RAG.""
  - ""This review clarifies the current research landscape, highlights methodological gaps, and charts priority directions for future research.""
  - ""The present study addresses these gaps by offering a citation-weighted, PRISMA-compliant systematic synthesis of 128 influential RAG studies that maps datasets, architectures, evaluation metrics, and open research challenges, thus advancing the field toward more aligned, robust, and scalable retrieval-augmented systems.""
  - ""This review is aimed at both NLP researchers, who can use it to identify gaps and promising directions, and NLP engineers seeking practical guidance on applying RAG techniques.""
  - ""Our review catalogues datasets, novel methods, evaluation metrics, and RAG deployment challenges.""
  - ""Knowledge-intensive tasks accounted for 27.34%, followed by open-domain question answering (ODQA) at 15.62%, software engineering 10.16% and medical 8.59%.""
  - ""Evaluation comprised 7.03%. The ""Other"" category (7.03%) covers nine single-study niches: networking, counterfactual augmentation, content creation, personalisation, legal QA, recommender systems, chemistry, disaster response and personalised search.""
  - ""Multimodal and conversational AI each represented 4.69%; security/vulnerabilities and biomedical 3.91% each; education 3.12%; information extraction 2.34%; and finance 1.56%.""","  - ""(Page 2, Table 1) | Index | Research Question                                                                 | Goal                                                                                                                                                                                                 |\n|-------|-----------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| RQ1   | What thematic topics have already been addressed by highly cited RAG studies?     | Summarises the main topics in the field, outlining the current state of knowledge and identifying gaps in the literature.                                                                            |""
  - ""(Page 3, Table 1) | Database              | Query                                                                                                                                                      |\n|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ACM Digital Library   | Title:(retrieval AND augmented AND generation) OR Abstract:(retrieval AND augmented AND generation)                                                        |""
  - ""(Page 6, Table 1) | Datasets                                                                                           | Chunking Mechanism                                                                                      | Retrieval Mechanism                                                                                                                                                                                                 | Vector Space Encoder                                                                                       | Generation Model                                                                                                                                                                                                 |\n|----------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Domain: Knowledge-Intensive Tasks**                                                              |                                                                                                         |                                                                                                                                                                                                                     |                                                                                                            |                                                                                                                                                                                                                  |""
  - ""<table_quotation page_num=7 table_on_page=1></table_quotation>""
  - ""(Page 8, Table 1) | Datasets                                                                                           | Chunking Mechanism                                                                                     | Retrieval Mechanism                                                                                     | Vector Space Encoder                                                                                   | Generation Model                                                                                       |\n|----------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\n| **Domain: Knowledge-Intensive Tasks Continued...**                                                 |                                                                                                        |                                                                                                         |                                                                                                        |                                                                                                        |""
  - ""<table_quotation page_num=38 table_on_page=1></table_quotation>""
  - ""(Page 39, Table 1) | Dataset Name                                                                 | Content Description                                                                 | Intended Use                                                                                   | Citation Frequency |\n|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|--------------------|\n| Colossal Clean Crawled Corpus (C4) [220]                                     | Billions of English tokens from web.                                                | Unsupervised pre-training for NLP models.                                                      | 2                  |""
  - ""<table_quotation page_num=40 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=41 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=42 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=43 table_on_page=1></table_quotation>""
  - ""<table_quotation page_num=44 table_on_page=1></table_quotation>""
  - ""(Page 45, Table 1) | Dataset Name                                      | Content Description                                                                 | Intended Use                                           | Citation Frequency |\n|---------------------------------------------------|--------------------------------------------------------------------------------------|--------------------------------------------------------|--------------------|\n| UpToDate                                          | Clinical decision support content by Wolters Kluwer.                                 | Point-of-care medical reference.                       | 1                  |""","- The paper provides a systematic review of retrieval-augmented generation (RAG) techniques, focusing on highly cited studies from 2020 to May 2025. This indicates that the application context is primarily within the field of natural language processing (NLP).
- The review is aimed at both NLP researchers and engineers, suggesting that the application context includes both academic and practical applications in NLP.
- The paper catalogues datasets, architectures, evaluation metrics, and challenges, which are relevant to various domains such as knowledge-intensive tasks, open-domain question answering, software engineering, medical applications, and more. These domains are identified as key areas where RAG is applied.
- The paper mentions specific literature review tasks addressed, such as identifying gaps and promising directions in RAG research.
- The types of academic documents processed include research articles and papers related to RAG.
- User types include NLP researchers and engineers, indicating that the application context involves both academic research and practical engineering applications.
- Integration with research workflows is implied as the review aims to provide guidance for future research and practical applications.
- The paper does not explicitly distinguish between commercial and academic applications, but the focus on NLP research suggests a strong academic context."