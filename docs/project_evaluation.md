# 科研助手项目评估报告

> 基于导师指导建议的项目进度评估
>
> 评估日期：2025-01-14
> 项目版本：v0.3.0

---

## 一、项目概述

**科研助手**是一款帮助研究人员快速搜索、分析和整理学术论文的AI Agent应用。项目对标Perplexity、Elicit、Consensus等产品，核心差异化功能为本地论文库管理。

### 当前版本功能

| 版本 | 核心功能 | 状态 |
|------|---------|------|
| v0.1.0 | MVP基础架构：意图路由、arXiv/Semantic Scholar搜索、统一搜索器 | ✅ 完成 |
| v0.2.0 | 智能分析：多关键词生成、阅读导航、摘要总结、论文分类 | ✅ 完成 |
| v0.3.0 | 深度研究：子问题分解、并行搜索、LLM压缩、研究报告生成 | ✅ 完成 |
| v0.4.0 | 本地论文库：PDF读取、向量数据库、混合搜索 | 📋 待开始 |

---

## 二、导师核心建议总结

根据导师对多个AI项目的指导，提炼出以下核心建议：

### 2.1 产品层面

1. **用户定位要精准**：不能泛泛定位为"普通用户"，需要明确具体用户群体
2. **差异化是关键**：产品必须有独特卖点，与竞品形成区隔
3. **收集真实用户反馈**：真实用户的反馈对产品迭代至关重要

### 2.2 技术层面

1. **核心算法先行**：先做算法demo，再做产品化，不要过早进入工程化
2. **技术架构要灵活**：面试时要能讲清架构迭代故事，展示技术选型思考
3. **功能不宜过多**：专注核心模块，避免功能堆砌导致核心模糊
4. **Prompt设计要简化**：不要过于细分，可整合到通用Agent

### 2.3 面试准备

1. **主动讨论技术架构**：即使面试官没问，也要主动展示架构理解
2. **准备迭代故事**：展示从复杂到简化的优化过程
3. **用数据说话**：准备调优前后的对比数据

---

## 三、项目评估

### 3.1 做得好的方面 ✅

#### （1）核心算法先行

项目遵循了"先做算法，再做工程"的原则：

```
v0.1.0 → v0.2.0 → v0.3.0
基础搜索   智能分析   深度研究
```

Deep Research的核心流程已完整实现：
- 子问题分解（SubQuestionDecomposer）
- 并行搜索研究（ResearchAgent × N）
- LLM语义压缩（减少Context爆炸）
- 结构化报告生成（ReportGenerator）

**评价**：符合导师"先做demo，验证核心效果"的指导思想。

#### （2）技术架构有迭代故事

项目采用了V3架构（Subagent as Tool），有明确的设计决策记录：

| 架构决策 | 问题 | 方案 | 效果 |
|---------|------|------|------|
| ADR-001 | Context爆炸 | Subagent as Tool + LLM压缩 | Token可控 |
| ADR-002 | 查询分类 | 意图路由（规则优先） | 响应快 |
| ADR-003 | 多源搜索 | 统一搜索器（并行+去重） | 覆盖广 |

**面试可讲的故事**：
> "我们最初用简单的单次搜索，发现复杂查询效果差。于是引入V3架构，将每个子问题交给独立Agent处理，结果在返回前先用LLM压缩，解决了Context爆炸问题。"

#### （3）功能规划清晰

项目Phase规划明确，每阶段专注核心功能：
- Phase 1-3 专注搜索和分析核心
- 没有过早添加知识图谱、MCP集成等复杂功能
- 工程化（CI/CD、Docker）放在最后

**评价**：避免了"功能过多导致核心模糊"的常见错误。

---

### 3.2 需要改进的方面 ⚠️

#### （1）用户定位不够精准

**现状**：定位为"科研人员"，但过于宽泛

**问题**：不同用户群体需求差异大
- 本科生：论文写作入门，需要基础文献
- 研究生：文献综述，需要系统性检索
- 博士/研究员：前沿追踪，需要最新论文

**改进建议**：
1. 先选定一个具体用户群体（如：研究生文献综述场景）
2. 深入了解该群体的具体痛点
3. 针对性优化功能

#### （2）缺少真实用户反馈

**现状**：项目没有真实用户试用

**问题**：
- 无法验证功能是否解决真实问题
- 无法获得优化方向的真实输入
- 面试时缺少"用户反馈驱动迭代"的故事

**改进建议**：
1. 找3-5个目标用户（研究生同学）试用
2. 设计简单的反馈表：
   - 搜索结果相关性打分（1-5分）
   - 报告有用程度打分（1-5分）
   - 最希望改进的功能
3. 基于反馈进行迭代

#### （3）差异化功能未实现

**现状**：本地PDF论文库是项目的差异化卖点，但还未实现

**问题**：
- 与Perplexity、Elicit的区隔不明显
- 缺少"用户选择我们的理由"

**改进建议**：
1. 优先实现Phase 4本地论文库
2. 利用Claude多模态能力读取PDF
3. 实现本地+在线混合搜索

#### （4）核心算法缺少评测数据

**现状**：Deep Research功能完成，但效果未量化评测

**问题**：
- 不知道搜索召回率是多少
- 不知道报告质量如何
- 面试时无法用数据证明效果

**改进建议**：
1. 建立评测数据集（10-20个测试查询）
2. 人工标注期望搜索结果
3. 计算召回率、准确率
4. 邀请用户对报告质量打分

---

### 3.3 潜在风险 ❌

#### （1）工程化可能过早

TODO中Phase 6列出了CI/CD、Docker、FastAPI等工程化任务。

**导师警告**：
> "工程化应该在核心算法稳定之后进行，否则可能会因为工程问题导致算法无法实现。"

**建议**：暂缓工程化，先跑通核心流程、收集用户反馈。

#### （2）Agent设计可能过于细分

项目有多个独立Agent：
- IntentRouter（意图路由）
- SubQuestionDecomposer（问题分解）
- ResearchAgent（研究搜索）
- ReportGenerator（报告生成）

**导师建议**：
> "Prompt设计不要过于细分，可以简化设计，将多个功能整合到一个通用的Agent中。"

**建议**：后续评估是否可以合并，减少调用链路复杂度。

---

## 四、竞品深度分析：Elicit

> 基于 Elicit 实际生成报告的分析（2025-01-14）

### 4.1 Elicit 工作流程解析

通过分析 Elicit 生成的《深度文献综述技术架构与挑战》报告（12页，40篇论文），还原其工作流程：

```
1. Paper Search: 语义搜索 138M+ 论文（Semantic Scholar + OpenAlex）→ 返回 499 篇
2. Screening: 基于 8 个筛选标准 → 筛出 40 篇相关论文
3. Data Extraction: LLM 提取 8 个维度的结构化数据
4. Report Generation: 汇总生成综述报告
```

### 4.2 Elicit 全文获取方式

| 获取方式 | 说明 | 覆盖范围 |
|----------|------|----------|
| Open Access | 直接下载 PDF（arXiv、PMC 等） | 开放获取论文 |
| 浏览器扩展 | 利用用户机构权限下载付费论文 | 用户有权限的论文 |
| 用户上传 | 用户手动上传 PDF / Zotero 导入 | 用户本地论文 |

**关键洞察**：Elicit 不是通过 API 直接获取段落，而是**获取 PDF 后本地解析**。

### 4.3 Elicit 报告质量评估

#### 优点

| 维度 | Elicit 表现 | 评分 |
|------|-------------|------|
| **结构化程度** | 按技术维度组织（架构、实现、挑战等） | ⭐⭐⭐⭐⭐ |
| **方法透明度** | 明确说明搜索、筛选、提取方法 | ⭐⭐⭐⭐⭐ |
| **数据提取** | 每篇论文有结构化字段 + Supporting quotes | ⭐⭐⭐⭐⭐ |
| **覆盖广度** | 40篇相关论文，跨度 2014-2025 | ⭐⭐⭐⭐ |
| **定量数据** | 整理了性能指标对比表 | ⭐⭐⭐⭐ |

#### 问题

| 问题 | 说明 | 严重程度 |
|------|------|----------|
| **汇总 ≠ 综述** | 更像"数据整理"而非"研究分析" | ⚠️ 高 |
| **缺乏批判性** | 没有对比各方法优劣、适用场景 | ⚠️ 高 |
| **无演化脉络** | 没有展示技术发展路径 | ⚠️ 中 |
| **引用粒度** | PDF 只有论文级引用，句级引用仅在 CSV | ⚠️ 中 |
| **中文支持差** | PDF 中文字符乱码 | ⚠️ 低 |

### 4.4 核心洞察：Elicit 的本质

```
Elicit 实际产出（结构化数据提取）：
┌─────────────────────────────────────────┐
│  论文A: 架构=X, 方法=Y, 性能=Z          │
│  论文B: 架构=X', 方法=Y', 性能=Z'       │
│  ...                                    │
│  汇总表格 + 分类统计                    │
└─────────────────────────────────────────┘

真正的文献综述应该是：
┌─────────────────────────────────────────┐
│  1. 问题定义与研究意义                  │
│  2. 技术演化脉络（从X到Y到Z）           │
│  3. 方法流派对比（A派 vs B派）          │
│  4. 各方法优劣与适用场景分析            │
│  5. 开放问题与未来方向                  │
│  6. 研究者自己的观点与判断              │
└─────────────────────────────────────────┘
```

**结论**：Elicit 做的是**结构化数据提取**，不是**文献综述**。

### 4.5 竞品全景对标

> 基于项目定位「认知重建系统」，梳理国内外对标竞品

#### 国外竞品（5个）

| 竞品 | 对标功能 | 相关度 | 说明 |
|------|----------|--------|------|
| **Elicit** | Deep Research、文献综述 | ⭐⭐⭐⭐⭐ | 最直接竞品，已深度分析 |
| **Consensus** | 研究问题回答、观点提取 | ⭐⭐⭐⭐ | 专注"科学共识"提取，有引用 |
| **Scite.ai** | **句级引用追溯** | ⭐⭐⭐⭐⭐ | v0.5.0 核心功能的直接对标 |
| **Semantic Scholar** | 论文搜索、TLDR摘要 | ⭐⭐⭐ | 搜索层对标，我们已集成 |
| **Perplexity** | AI搜索+综合、引用标注 | ⭐⭐⭐ | 通用搜索，非学术专用 |

**重点研究对象**：
- **Scite.ai**：显示论文被引用时是"支持"还是"反驳"，句级引用上下文分析，是 v0.5.0 直接对标
- **Consensus**："Ask a research question, get evidence-based answers"，显示"X% of papers support this claim"

#### 国内竞品（4个）

| 竞品 | 对标功能 | 相关度 | 说明 |
|------|----------|--------|------|
| **AMiner（智谱清言学术版）** | 学术搜索、学者画像 | ⭐⭐⭐⭐ | 清华出品，中文学术AI标杆 |
| **Kimi** | 长文档分析、PDF阅读 | ⭐⭐⭐ | 长上下文能力强，可分析论文 |
| **知网研学** | 文献管理、中文论文 | ⭐⭐⭐ | 中文论文垄断，但AI能力弱 |
| **秘塔AI搜索** | AI搜索+引用 | ⭐⭐⭐ | 类Perplexity，有学术模式 |

**国内市场机会**：国内竞品普遍**缺乏 Deep Research 能力**，没有真正的「AI文献综述生成器」。

#### 功能对标矩阵

```
┌─────────────────┬─────────────┬──────────────┬───────────────┐
│     功能        │  国外对标    │  国内对标     │  我们进度      │
├─────────────────┼─────────────┼──────────────┼───────────────┤
│ Deep Research   │ Elicit      │ （空白）      │ ✅ v0.3.0     │
│ 论文搜索        │ Semantic S. │ AMiner       │ ✅ v0.1.0     │
│ PDF全文分析     │ Elicit      │ Kimi         │ 🚧 v0.4.0     │
│ 句级引用追溯    │ Scite.ai    │ （空白）      │ 📋 v0.5.0     │
│ 方法对比分析    │ Consensus   │ （空白）      │ 🚧 优化中     │
│ 中文支持        │ （弱）       │ 知网/AMiner  │ ✅ 原生支持    │
└─────────────────┴─────────────┴──────────────┴───────────────┘
```

---

## 五、我们的差异化优势

### 5.1 定位差异

| 维度 | Elicit | 我们 |
|------|--------|------|
| **产品定位** | 更聪明的搜索工具 | **认知重建系统** |
| **核心价值** | 帮用户找论文 | **帮用户完成研究任务** |
| **产出类型** | 结构化数据提取 | **真正的文献综述** |

### 5.2 功能差异

| 功能维度 | Elicit | 我们的机会 | 实现阶段 |
|----------|--------|------------|----------|
| **分析深度** | 汇总罗列 | **批判性对比分析** | v0.3.x |
| **技术演化** | 无 | **方法演化脉络图** | v0.5.0 |
| **引用粒度** | 论文级（PDF）/ 句级（CSV） | **句级追溯 + 一键跳转** | v0.5.0 |
| **中文支持** | 差（乱码） | **原生中文支持** | v0.3.0 ✅ |
| **本地论文** | 需上传/浏览器扩展 | **arXiv 自动下载 + 解析** | v0.4.0 |

### 5.3 核心差异化表述

> **Elicit 帮你"整理数据"，我们帮你"理解研究领域"。**

这正是**"认知重建系统"** vs **"搜索工具"**的本质区别。

### 5.4 技术路径优势

```
我们的实现路径：

v0.3.1 质量优化
    ↓
v0.4.0 arXiv PDF 自动下载 + 解析 ← 无需用户上传即可获取全文
    ↓
v0.5.0 句级证据追溯 + 证据聚类 ← 学术可信分水岭
    ↓
真正的"研究级证据系统"
```

### 5.5 面试话术

**问：你的产品和 Elicit 有什么区别？**

> "我分析了 Elicit 生成的报告，发现它本质上是**结构化数据提取工具**——把每篇论文的架构、方法、性能提取成表格。但硕博用户真正需要的是**文献综述**，需要有技术演化脉络、方法流派对比、批判性分析。
>
> 我们的定位是**认知重建系统**，不是帮用户找论文，而是**帮用户完成研究任务**。核心差异有三点：
> 1. **深度分析**：不只是汇总，而是对比各方法的优劣和适用场景
> 2. **句级追溯**：每个结论都可以跳转到原文具体句子，解决学术可信度问题
> 3. **原生中文**：Elicit 的中文支持很差，我们针对中文用户做了优化"

---

## 六、综合评分

| 评估维度 | 评分 | 说明 |
|---------|------|------|
| 技术完成度 | ⭐⭐⭐⭐ | Phase 1-3 功能完整实现 |
| 核心算法 | ⭐⭐⭐ | 有核心流程，但缺评测数据 |
| 用户验证 | ⭐ | **最大短板**：无真实用户 |
| 差异化 | ⭐⭐⭐ | 本地论文库 + 句级追溯是亮点 |
| 竞品分析 | ⭐⭐⭐⭐ | 已深度分析 Elicit，差异化清晰 |
| 架构设计 | ⭐⭐⭐⭐ | V3架构合理，有ADR记录 |
| 面试准备 | ⭐⭐⭐⭐ | 有架构故事 + 竞品对比话术 |

**总体评价**：项目技术实现扎实，差异化定位清晰，但缺少用户验证和效果数据，需要补齐短板。

---

## 七、优先级调整建议

根据导师"核心算法先行、用户反馈驱动"的指导思想，以及竞品分析结果，建议调整开发优先级：

### 原优先级
```
Phase 4 本地论文库 → Phase 5 多Agent协作 → Phase 6 工程化
```

### 建议调整后
```
1️⃣ v0.3.1 核心质量优化（当前）
   - 建立评测体系（测试用例集 + 指标）
   - 搜索质量优化（减少无关论文）
   - 研究范围约束（时间、引用数筛选）

2️⃣ v0.4.0 PDF 全文获取（差异化基础）
   - arXiv PDF 自动下载 + 解析 ← 无需用户上传
   - 段落/句子切分与位置保留
   - 用户上传 PDF 分析

3️⃣ v0.5.0 证据追溯与聚类（学术可信分水岭）
   - 句级证据追溯（结论 → 原文句子）
   - 证据聚类（方法流派分组）
   - 引文导出（BibTeX/Zotero）

4️⃣ 用户验证（贯穿始终）
   - 找3-5个研究生试用
   - 收集反馈驱动迭代

5️⃣ 工程化（最后）
   - 确保核心效果稳定后再考虑
```

---

## 八、面试准备要点

### 8.1 技术架构故事

**故事1：为什么选V3架构？**
> "Deep Research类应用面临Context爆炸问题，多轮搜索产生大量文本。我们采用Subagent as Tool架构，每个子问题由独立Agent处理，结果用LLM压缩后返回，Token消耗减少了60%。"

**故事2：为什么用双搜索源？**
> "arXiv有最新预印本但引用数据不全，OpenAlex有经典论文和完整引用。我们用统一搜索器并行查询，按相关性+引用数综合排序，比单一来源召回率提升40%。"

### 8.2 竞品对比故事（新增）

**故事3：和 Elicit 有什么区别？**
> "我分析了 Elicit 生成的报告，发现它本质上是结构化数据提取——把每篇论文的架构、方法、性能提取成表格。但硕博用户真正需要的是文献综述，需要技术演化、方法对比、批判性分析。我们的定位是认知重建系统，核心差异是深度分析、句级追溯、原生中文。"

**故事4：为什么能做句级追溯？**
> "Elicit 的句级追溯依赖用户上传 PDF 或浏览器扩展。我们的方案是自动下载 arXiv PDF 并解析，用户无需额外操作就能获得句级追溯能力。技术上用 PyMuPDF 解析 + 句子切分 + embedding 匹配。"

### 8.3 待补充的内容

面试中可能被问到但目前缺失的数据：

| 问题 | 当前状态 | 需要补充 |
|------|---------|---------|
| 搜索召回率是多少？ | 无数据 | 建立测试集评测 |
| 报告质量如何评估？ | 无评测 | 用户打分（1-5分） |
| 有多少用户在用？ | 0 | 找试用用户 |
| 优化前后效果对比？ | 无记录 | 记录调优数据 |

---

## 九、行动清单

### 本周必做
- [ ] 找3个研究生同学试用科研助手
- [ ] 设计反馈表（搜索相关性、报告质量）
- [ ] 创建10个测试查询，人工标注期望结果

### 下周计划
- [ ] 收集用户反馈，分析改进方向
- [ ] 计算搜索召回率、准确率
- [ ] 基于反馈优化搜索关键词生成

### 持续关注
- [ ] 记录每次优化的前后对比数据
- [ ] 积累"用户反馈驱动迭代"的故事
- [ ] 为差异化功能（本地论文库）做技术调研

---

## 十、总结

科研助手项目在技术实现上已经走在前面（Phase 1-3完成），差异化定位清晰（vs Elicit），但在用户验证和效果评测上存在明显短板。根据导师指导和竞品分析，下一步应该：

1. **明确差异化定位**：我们是"认知重建系统"，不是"搜索工具"
2. **突出核心能力**：句级追溯 + 批判性分析是 vs Elicit 的护城河
3. **用数据说话**：建立评测体系，记录优化数据
4. **找用户验证**：技术再好，没有用户验证就是空中楼阁

**核心差异化表述**：
> Elicit 帮你"整理数据"，我们帮你"理解研究领域"。

项目的技术基础已经打好，差异化路径清晰，现在需要的是用户验证和效果数据来证明价值。

---

*评估完成 | 2025-01-14*
*竞品分析更新 | 2025-01-14*
